{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Module\n",
    "\n",
    "> `DataModule` for training parametric models, generating and benchmarking CF explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.utils import load_json, validate_configs, get_config, save_pytree, load_pytree, get_config\n",
    "from relax.base import *\n",
    "from relax.data_utils import *\n",
    "from relax.import_essentials import *\n",
    "import jax\n",
    "from jax import numpy as jnp, random as jrand, lax, Array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json, os, shutil\n",
    "from urllib.request import urlretrieve\n",
    "from pydantic.fields import ModelField, Field\n",
    "from typing import List, Dict, Union, Optional, Tuple, Callable, Any, Iterable\n",
    "import warnings\n",
    "from pandas.testing import assert_frame_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "from copy import deepcopy\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module Interfaces\n",
    "\n",
    "High-level interfaces for `DataModule`. Docs to be added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseDataModule(BaseModule):\n",
    "    \"\"\"DataModule Interface\"\"\"\n",
    "\n",
    "    def _prepare(self, *args, **kwargs):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def apply_constraints(self, x: Array, cf: Array, hard: bool = False, **kwargs) -> Array:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def compute_reg_loss(self, x: Array, cf: Array, hard: bool = False, **kwargs) -> float:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class DataModuleInfoMixin:\n",
    "    \"\"\"This base class exposes some attributes of DataModule\n",
    "    at the base level for easy access.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def data(self) -> pd.DataFrame:\n",
    "        return self._data\n",
    "    \n",
    "    @property\n",
    "    def xs(self) -> Array:\n",
    "        return self._features.transformed_data\n",
    "    \n",
    "    @property\n",
    "    def ys(self) -> Array:\n",
    "        return self._label.transformed_data\n",
    "    \n",
    "    @property\n",
    "    def features(self) -> FeaturesList:\n",
    "        return self._features\n",
    "    \n",
    "    @property\n",
    "    def label(self) -> FeaturesList:\n",
    "        return self._label\n",
    "\n",
    "    @property\n",
    "    def dataset(self) -> Tuple[Array, Array]:\n",
    "        return (self.xs, self.ys)\n",
    "    \n",
    "    @property\n",
    "    def train_indices(self) -> List[int]:\n",
    "        return self.config.train_indices\n",
    "    \n",
    "    @property\n",
    "    def test_indices(self) -> List[int]:\n",
    "        return self.config.test_indices\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module\n",
    "\n",
    "`DataModule` for processing data, training models, and benchmarking CF explanations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataModuleConfig(BaseConfig):\n",
    "    \"\"\"Configurator of `DataModule`.\"\"\"\n",
    "\n",
    "    data_dir: str = Field(None, description=\"The directory of dataset.\")\n",
    "    data_name: str = Field(None, description=\"The name of `DataModule`.\")\n",
    "    continous_cols: List[str] = Field([], description=\"Continuous features/columns in the data.\")\n",
    "    discret_cols: List[str] = Field([], description=\"Categorical features/columns in the data.\")\n",
    "    imutable_cols: List[str] = Field([], description=\"Immutable features/columns in the data.\")\n",
    "    continuous_transformation: Optional[str] = Field('minmax', description=\"Transformation for continuous features. `None` indicates unknown.\")\n",
    "    discret_transformation: Optional[str] = Field('ohe', description=\"Transformation for categorical features. `None` indicates unknown.\")\n",
    "    sample_frac: Optional[float] = Field(\n",
    "        None, description=\"Sample fraction of the data. Default to use the entire data.\", ge=0., le=1.0\n",
    "    )\n",
    "    train_indices: List[int] = Field([], description=\"Indices of training data.\")\n",
    "    test_indices: List[int] = Field([], description=\"Indices of testing data.\")\n",
    "    \n",
    "    def shuffle(self, data: Array, test_size: float, seed: int = None):\n",
    "        \"\"\"Shuffle data with a seed.\"\"\"\n",
    "        if seed is None:\n",
    "            seed = get_config().global_seed\n",
    "        key = jrand.PRNGKey(seed)\n",
    "        total_length = data.shape[0]\n",
    "        train_length = int((1 - test_size) * total_length)\n",
    "        if len(self.train_indices) == 0:\n",
    "            self.train_indices = jrand.permutation(key, total_length)[:train_length].tolist()\n",
    "        if len(self.test_indices) == 0:\n",
    "            self.test_indices = jrand.permutation(key, total_length)[train_length:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "config = DataModuleConfig(data_name=\"TabularDataModule\", data_dir=\"data\", continous_cols=[], discret_cols=[], imutable_cols=[])\n",
    "# Test shuffle\n",
    "assert len(config.train_indices) == 0\n",
    "assert len(config.test_indices) == 0\n",
    "config.shuffle(data=np.arange(100), test_size=0.2)\n",
    "assert len(config.train_indices) == 100 * 0.8\n",
    "assert len(config.test_indices) == 100 * 0.2\n",
    "assert isinstance(config.train_indices, list)\n",
    "assert isinstance(config.test_indices, list)\n",
    "assert (sorted(config.train_indices + config.test_indices) == list(range(100)))\n",
    "\n",
    "configs_dict = {\n",
    "    \"data_dir\": \"assets/adult/data/data.csv\",\n",
    "    \"data_name\": \"adult\",\n",
    "    \"continous_cols\": [\"age\", \"hours_per_week\"], \n",
    "    \"discret_cols\": [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"race\", \"gender\"], \n",
    "    \"imutable_cols\": [\"race\", \"gender\"],\n",
    "    \"sample_frac\": 0.1,\n",
    "}\n",
    "configs = DataModuleConfig(**configs_dict)\n",
    "assert len(configs.train_indices) == 0\n",
    "assert len(configs.test_indices) == 0\n",
    "assert config.continuous_transformation == 'minmax'\n",
    "assert config.discret_transformation == 'ohe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils\n",
    "\n",
    "> util functions for `DataModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def features2config(\n",
    "    features: FeaturesList, # FeaturesList to be converted\n",
    "    name: str, # Name of the data used for `DataModuleConfig`\n",
    "    return_dict: bool = False # Whether to return a dict or `DataModuleConfig`\n",
    ") -> Union[DataModuleConfig, Dict]: # Return configs\n",
    "    \"\"\"Get `DataModuleConfig` from `FeaturesList`.\"\"\"\n",
    "\n",
    "    cont, cats, immu = [], [], []\n",
    "    cont_transformation, cat_transformation = None, None\n",
    "    for f in features:\n",
    "        if f.is_categorical:\n",
    "            cats.append(f.name)\n",
    "        else:\n",
    "            cont.append(f.name)\n",
    "        if f.is_immutable:\n",
    "            immu.append(f.name)\n",
    "    \n",
    "    configs_dict = {\n",
    "        \"data_dir\": \".\",\n",
    "        \"data_name\": name,\n",
    "        \"continous_cols\": cont,\n",
    "        \"discret_cols\": cats,\n",
    "        \"imutable_cols\": immu,\n",
    "        \"continuous_transformation\": cont_transformation,\n",
    "        \"discret_transformation\": cat_transformation,\n",
    "    }\n",
    "    if return_dict:\n",
    "        return configs_dict\n",
    "    return DataModuleConfig(**configs_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "feats = FeaturesList([\n",
    "    Feature(\"age\", np.random.normal(0, 1, (10, 1)), transformation='minmax', is_immutable=True),\n",
    "    Feature(\"workclass\", np.random.randint(0, 2, (10, 1)), transformation='ohe'),\n",
    "    Feature(\"education\", np.random.randint(0, 2, (10, 1)), transformation='ordinal'),    \n",
    "])\n",
    "config = features2config(feats, \"test\")\n",
    "assert config.data_dir == \".\"\n",
    "assert config.data_name == \"test\"\n",
    "assert config.continous_cols == [\"age\"]\n",
    "assert config.discret_cols == [\"workclass\", \"education\"]\n",
    "assert config.imutable_cols == [\"age\"]\n",
    "assert config.continuous_transformation is None\n",
    "assert config.discret_transformation is None\n",
    "\n",
    "config_dict = features2config(feats, \"test\", return_dict=True)\n",
    "assert isinstance(config_dict, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def features2pandas(\n",
    "    features: FeaturesList, # FeaturesList to be converted\n",
    "    labels: FeaturesList # labels to be converted\n",
    ") -> pd.DataFrame: # Return pandas dataframe\n",
    "    \"\"\"Convert `FeaturesList` to pandas dataframe.\"\"\"\n",
    "    \n",
    "    feats_df = features.to_pandas()\n",
    "    labels_df = labels.to_pandas()\n",
    "    df = pd.concat([feats_df, labels_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = FeaturesList([\n",
    "    Feature(\"age\", np.random.normal(0, 1, (10, 1)), \n",
    "            transformation='minmax', is_immutable=True),\n",
    "    Feature(\"workclass\", np.random.randint(0, 2, (10, 1)), \n",
    "            transformation='ohe'),\n",
    "    Feature(\"education\", np.random.randint(0, 2, (10, 1)), \n",
    "            transformation='ordinal'),    \n",
    "])\n",
    "labels = FeaturesList([\n",
    "    Feature(\"income\", np.random.randint(0, 2, (10, 1)), \n",
    "            transformation='identity'),\n",
    "])\n",
    "df = features2pandas(feats, labels)\n",
    "assert isinstance(df, pd.DataFrame)\n",
    "assert df.shape == (10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def to_feature(col: str, data: pd.DataFrame, config: DataModuleConfig, transformation: str):\n",
    "    return Feature(\n",
    "        name=col, data=data[col].to_numpy().reshape(-1, 1),\n",
    "        transformation=transformation,\n",
    "        is_immutable=col in config.imutable_cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dataframe2features(\n",
    "    data: pd.DataFrame,\n",
    "    config: DataModuleConfig,\n",
    ") -> FeaturesList:\n",
    "    \"\"\"Convert pandas dataframe of features to `FeaturesList`.\"\"\"\n",
    "\n",
    "    cont_features = [to_feature(col, data, config, config.continuous_transformation) for col in config.continous_cols]\n",
    "    cat_features = [to_feature(col, data, config, config.discret_transformation) for col in config.discret_cols]\n",
    "    features = cont_features + cat_features\n",
    "    return FeaturesList(features)\n",
    "\n",
    "\n",
    "def dataframe2labels(\n",
    "    data: pd.DataFrame,\n",
    "    config: DataModuleConfig,\n",
    ") -> FeaturesList:\n",
    "    \"\"\"Convert pandas dataframe of labels to `FeaturesList`.\"\"\"\n",
    "    \n",
    "    label_cols = set(data.columns) - set(config.continous_cols) - set(config.discret_cols)\n",
    "    labels = [to_feature(col, data, config, 'identity') for col in label_cols]\n",
    "    return FeaturesList(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Data Module\n",
    "\n",
    "> Main module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataModule(BaseDataModule, DataModuleInfoMixin):\n",
    "    \"\"\"DataModule for tabular data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        features: FeaturesList,\n",
    "        label: FeaturesList,\n",
    "        config: DataModuleConfig = None,\n",
    "        data: pd.DataFrame = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self._prepare(features, label)\n",
    "        if config is None:\n",
    "            name = kwargs.pop('name', 'DataModule')\n",
    "            config = features2config(features, name)\n",
    "        config.shuffle(self.xs, test_size=0.25)\n",
    "        self._data = features2pandas(features, label) if data is None else data\n",
    "        super().__init__(config, name=config.data_name)\n",
    "\n",
    "    def _prepare(self, features, label):\n",
    "        if features is not None and label is not None:\n",
    "            self._features = FeaturesList(features)\n",
    "            self._label = FeaturesList(label)\n",
    "        elif features is None:\n",
    "            raise ValueError(\"Features cannot be None.\")\n",
    "        elif label is None:\n",
    "            raise ValueError(\"Label cannot be None.\")\n",
    "            \n",
    "    def save(\n",
    "        self, \n",
    "        path: str # Path to the directory to save `DataModule`\n",
    "    ):\n",
    "        \"\"\"Save `DataModule` to a directory.\"\"\"\n",
    "        path = Path(path)\n",
    "        if not path.exists():\n",
    "            path.mkdir(parents=True)\n",
    "        self._features.save(path / 'features')\n",
    "        self._label.save(path / 'label')\n",
    "        if self._data is not None:\n",
    "            self._data.to_csv(path / 'data.csv', index=False)\n",
    "        with open(path / \"config.json\", \"w\") as f:\n",
    "            json.dump(self.config.dict(), f)\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_path(\n",
    "        cls, \n",
    "        path: str,  # Path to the directory to load `DataModule`\n",
    "        config: Dict|DataModuleConfig = None # Configs of `DataModule`. This argument is ignored.\n",
    "    ) -> DataModule: # Initialized `DataModule` from path\n",
    "        \"\"\"Load `DataModule` from a directory.\"\"\"\n",
    "        if config is not None:\n",
    "            warnings.warn(\"Passing `config` will have no effect.\")\n",
    "        \n",
    "        path = Path(path)\n",
    "        config = DataModuleConfig.load_from_json(path / 'config.json')\n",
    "        # config = validate_configs(config, DataModuleConfig)\n",
    "        features = FeaturesList.load_from_path(path / 'features')\n",
    "        label = FeaturesList.load_from_path(path / 'label')\n",
    "        data = pd.read_csv(path / 'data.csv')\n",
    "        return cls(features=features, label=label, config=config, data=data)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_path(cls, path, config: DataModuleConfig = None):\n",
    "        \"\"\"Alias of `load_from_path`\"\"\"\n",
    "        return cls.load_from_path(path, config)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(\n",
    "        cls, \n",
    "        config: Dict|DataModuleConfig, # Configs of `DataModule`\n",
    "        data: pd.DataFrame=None # Passed in pd.Dataframe\n",
    "    ) -> DataModule: # Initialized `DataModule` from configs and data\n",
    "        config = validate_configs(config, DataModuleConfig)\n",
    "        if data is None:\n",
    "            data = pd.read_csv(config.data_dir)\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise ValueError(\"`data` should be a pandas DataFrame.\")\n",
    "        features = dataframe2features(data, config)\n",
    "        label = dataframe2labels(data, config)\n",
    "        return cls(features=features, label=label, config=config, data=data)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_numpy(\n",
    "        cls,\n",
    "        xs: np.ndarray, # Input data\n",
    "        ys: np.ndarray, # Labels\n",
    "        name: str = None, # Name of `DataModule`\n",
    "        transformation='minmax'\n",
    "    ) -> DataModule: # Initialized `DataModule` from numpy arrays\n",
    "        \"\"\"Create `DataModule` from numpy arrays. Note that the `xs` are treated as continuous features.\"\"\"\n",
    "        \n",
    "        features = FeaturesList([Feature(f\"feature_{i}\", xs[:, i].reshape(-1, 1), transformation=transformation) for i in range(xs.shape[1])])\n",
    "        labels = FeaturesList([Feature(f\"label\", ys.reshape(-1, 1), transformation='identity')])\n",
    "        return cls(features=features, label=labels, name=name)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_features(\n",
    "        cls, \n",
    "        features: FeaturesList, # Features of `DataModule`\n",
    "        label: FeaturesList, # Labels of `DataModule`\n",
    "        name: str = None # Name of `DataModule`\n",
    "    ) -> DataModule: # Initialized `DataModule` from features and labels\n",
    "        \"\"\"Create `DataModule` from `FeaturesList`.\"\"\"\n",
    "        return cls(features=features, label=label, name=name)\n",
    "        \n",
    "    def _get_data(self, indices):\n",
    "        if isinstance(indices, list):\n",
    "            indices = jnp.array(indices)\n",
    "        return (self.xs[indices], self.ys[indices])\n",
    "        \n",
    "    def __getitem__(self, name: str):\n",
    "        if name == 'train':\n",
    "            return self._get_data(self.config.train_indices)\n",
    "        elif name in ['valid', 'test']:\n",
    "            return self._get_data(self.config.test_indices)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown data name: {name}. Should be one of ['train', 'valid', 'test']\")\n",
    "    \n",
    "    def set_transformations(\n",
    "        self, \n",
    "        feature_names_to_transformation: Dict[str, Union[str, Dict, Transformation]], # Dict[feature_name, Transformation]\n",
    "    ) -> DataModule:\n",
    "        \"\"\"Reset transformations for features.\"\"\"\n",
    "\n",
    "        self._features = self._features.set_transformations(feature_names_to_transformation)\n",
    "        return self\n",
    "    \n",
    "    def sample(\n",
    "        self, \n",
    "        size: float | int, # Size of the sample. If float, should be 0<=size<=1.\n",
    "        stage: str = 'train', # Stage of data to sample from. Should be one of ['train', 'valid', 'test']\n",
    "        key: jrand.PRNGKey = None # Random key. \n",
    "    ) -> Tuple[Array, Array]: # Sampled data\n",
    "        \"\"\"Sample data from `DataModule`.\"\"\"\n",
    "\n",
    "        key = jrand.PRNGKey(get_config().global_seed) if key is None else key\n",
    "        xs, ys = self[stage]\n",
    "        indices = jnp.arange(xs.shape[0])\n",
    "        \n",
    "        if isinstance(size, float) and 0 <= size <= 1:\n",
    "            size = int(size * indices.shape[0])\n",
    "        elif isinstance(size, int):\n",
    "            size = min(size, indices.shape[0])\n",
    "        else:\n",
    "            raise ValueError(f\"`size` should be a floating number 0<=size<=1, or an integer,\"\n",
    "                             f\" but got size={size}.\")\n",
    "                \n",
    "        indices = jrand.permutation(key, indices)[:size]\n",
    "        return xs[indices], ys[indices]\n",
    "\n",
    "    def transform(\n",
    "        self, \n",
    "        data: pd.DataFrame | Dict[str, Array] # Data to be transformed\n",
    "    ) -> Array: # Transformed data\n",
    "        \"\"\"Transform data to `jax.Array`.\"\"\"\n",
    "        # TODO: test this function\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data_dict = {k: np.array(v).reshape(-1, 1) for k, v in data.iloc[:, :-1].to_dict(orient='list').items()}\n",
    "            return self._features.transform(data_dict)\n",
    "        elif isinstance(data, dict):\n",
    "            data = jax.tree_util.tree_map(lambda x: np.array(x).reshape(-1, 1), data)\n",
    "            return self._features.transform(data)\n",
    "        else:\n",
    "            raise ValueError(\"data should be a pandas DataFrame or `Dict[str, jax.Array]`.\")\n",
    "        \n",
    "    def inverse_transform(\n",
    "        self, \n",
    "        data: Array, # Data to be inverse transformed\n",
    "        return_type: str = 'pandas' # Type of the returned data. Should be one of ['pandas', 'dict']\n",
    "    ) -> pd.DataFrame: # Inverse transformed data\n",
    "        \"\"\"Inverse transform data to `pandas.DataFrame`.\"\"\"\n",
    "        # TODO: test this function\n",
    "        inversed = self._features.inverse_transform(data)\n",
    "        if return_type == 'pandas':\n",
    "            return inversed\n",
    "        elif return_type == 'dict':\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown return type: {return_type}. Should be one of ['pandas', 'dict']\")\n",
    "        \n",
    "    def apply_constraints(\n",
    "        self, \n",
    "        xs: Array, # Input data\n",
    "        cfs: Array, # Counterfactuals to be constrained\n",
    "        hard: bool = False # Whether to apply hard constraints or not\n",
    "    ) -> Array: # Constrained counterfactuals\n",
    "        \"\"\"Apply constraints to counterfactuals.\"\"\"\n",
    "        return self._features.apply_constraints(xs, cfs, hard)\n",
    "    \n",
    "    def compute_reg_loss(\n",
    "        self, \n",
    "        xs: Array, # Input data\n",
    "        cfs: Array, # Counterfactuals to be constrained\n",
    "        hard: bool = False # Whether to apply hard constraints or not\n",
    "    ) -> float:\n",
    "        \"\"\"Compute regularization loss.\"\"\"\n",
    "        return self._features.compute_reg_loss(xs, cfs, hard)\n",
    "    \n",
    "    __ALL__ = [\n",
    "        'load_from_path', \n",
    "        'from_config', \n",
    "        'from_features',\n",
    "        'from_numpy',\n",
    "        'save',\n",
    "        'transform',\n",
    "        'inverse_transform',\n",
    "        'apply_constraints',\n",
    "        'compute_reg_loss',\n",
    "        'set_transformations',\n",
    "        'sample'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def dm_equals(dm1: DataModule, dm2: DataModule):\n",
    "    # data_equals = np.allclose(dm1.data.to_numpy(), dm2.data.to_numpy())\n",
    "    assert_frame_equal(dm1.data, dm2.data)\n",
    "    xs_equals = np.allclose(dm1.xs, dm2.xs)\n",
    "    ys_equals = np.allclose(dm1.ys, dm2.ys)\n",
    "    train_indices_equals = np.array_equal(dm1.train_indices, dm2.train_indices)\n",
    "    test_indices_equals = np.array_equal(dm1.test_indices, dm2.test_indices)\n",
    "    # print(f\"data_equals: {data_equals}, xs_equals: {xs_equals}, ys_equals: {ys_equals}, train_indices_equals: {train_indices_equals}, test_indices_equals: {test_indices_equals}\")\n",
    "    return (\n",
    "        xs_equals and ys_equals and \n",
    "        train_indices_equals and test_indices_equals\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test initialization\n",
    "config = DataModuleConfig.load_from_json(\"assets/adult/data/config.json\")\n",
    "config_1 = config.dict()\n",
    "config_1.update({\"imutable_cols\": []})\n",
    "dm = DataModule.from_config(config)\n",
    "dm_1 = DataModule.from_config(config.dict())\n",
    "assert dm_equals(dm, dm_1)\n",
    "dm_2 = DataModule.from_path(\"assets/adult/data\")\n",
    "assert dm_equals(dm, dm_2)\n",
    "dm_3 = DataModule.from_config(config_1)\n",
    "assert dm_equals(dm, dm_3)\n",
    "assert dm_3.config.imutable_cols == []\n",
    "feats = FeaturesList.load_from_path(\"assets/adult/data/features\")\n",
    "label = FeaturesList.load_from_path(\"assets/adult/data/label\")\n",
    "dm_4 = DataModule.from_features(feats, label)\n",
    "assert dm_equals(dm, dm_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test from_numpy\n",
    "xs, ys = make_classification(n_samples=100, n_features=5, n_informative=3, random_state=0)\n",
    "dm_5 = DataModule.from_numpy(xs, ys, name=\"test\", transformation='identity')\n",
    "config_5 = dm_5.config\n",
    "assert dm_5.config.data_name == \"test\"\n",
    "assert dm_5.data.shape == (100, 6)\n",
    "assert np.allclose(dm_5.data.to_numpy(), np.concatenate([xs, ys.reshape(-1, 1)], axis=1))\n",
    "assert np.allclose(\n",
    "    xs[config_5.train_indices],\n",
    "    dm_5['train'][0]\n",
    ")\n",
    "assert np.allclose(\n",
    "    xs[config_5.test_indices],\n",
    "    dm_5['test'][0]\n",
    ")\n",
    "dm_5.save('tmp/test')\n",
    "dm_6 = DataModule.load_from_path('tmp/test')\n",
    "assert dm_equals(dm_5, dm_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test save and load\n",
    "dm.save(\"tmp/adult\")\n",
    "dm_5 = DataModule.load_from_path(\"tmp/adult\")\n",
    "assert dm_equals(dm, dm_5)\n",
    "shutil.rmtree(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set_transformations\n",
    "dm_6 = deepcopy(dm)\n",
    "dm_6.set_transformations({\"age\": 'identity'})\n",
    "assert dm_6.features['age'].transformation.name == 'identity'\n",
    "assert np.array_equal(dm_6.xs[:, :1], dm_6.data[['age']].to_numpy())\n",
    "dm_6.set_transformations({feat: 'ordinal' for feat in config.discret_cols})\n",
    "assert dm_6.xs.shape == (dm.data.shape[0], len(config.continous_cols) + len(config.discret_cols))\n",
    "\n",
    "assert np.array_equal(dm_6.xs[:, :1], dm_6.data[['age']].to_numpy())\n",
    "\n",
    "test_fail(lambda: dm_6.set_transformations({1: 'identity'}), contains=\"Invalid idx type\")\n",
    "test_fail(lambda: dm_6.set_transformations({\"❤\": 'identity'}), contains=\"Invalid feature name\")\n",
    "test_fail(lambda: dm_6.set_transformations({\"age\": '❤'}), contains=\"Unknown transformation\")\n",
    "test_fail(lambda: dm_6.set_transformations('❤'), contains=\"Invalid feature_names_to_transformation type\")\n",
    "\n",
    "dm_6.set_transformations({\"age\": MinMaxTransformation()})\n",
    "assert np.allclose(dm_6.xs[:, :1], dm.xs[:, :1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sample\n",
    "sampled_xs, sampled_ys = dm.sample(0.1)\n",
    "assert len(sampled_xs) == len(sampled_ys)\n",
    "assert sampled_xs.shape[0] == int(0.1 * dm['train'][0].shape[0])\n",
    "assert not jnp.all(sampled_xs == dm['train'][0][:sampled_xs.shape[0]])\n",
    "\n",
    "sampled_xs, sampled_ys = dm.sample(100)\n",
    "assert len(sampled_xs) == len(sampled_ys)\n",
    "assert sampled_xs.shape[0] == 100\n",
    "assert not jnp.all(sampled_xs == dm['train'][0][:100])\n",
    "\n",
    "test_fail(lambda: dm.sample(1.1), contains='should be a floating number 0<=size<=1,')\n",
    "test_fail(lambda: dm.sample('train'), contains='or an integer')\n",
    "\n",
    "xs = dm['train'][0]\n",
    "cfs = jrand.uniform(jrand.PRNGKey(0), shape=xs.shape, minval=0.01, maxval=0.99)\n",
    "cfs = dm.apply_constraints(xs, cfs, hard=False)\n",
    "assert cfs.shape == xs.shape\n",
    "\n",
    "cfs = dm.apply_constraints(xs, cfs, hard=True)\n",
    "assert cfs.shape == xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test transform\n",
    "data = dm.transform(dm.data)\n",
    "assert np.allclose(data, dm.xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legacy\n",
    "\n",
    "> Legacy code for DataModule\n",
    "\n",
    ":::{.callout-warning}\n",
    "\n",
    "Don't use these if you do not have to.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TabularDataModuleConfigs(DataModuleConfig):\n",
    "    \"\"\"!!!Deprecated!!! - Configurator of `TabularDataModule`.\"\"\"\n",
    "    def __ini__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        warnings.warn(\"TabularDataModuleConfigs is deprecated since v0.2, please use DataModuleConfig instead.\", \n",
    "                      DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TabularDataModule(DataModule):\n",
    "    \"\"\"!!!Deprecated!!! - DataModule for tabular data.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        warnings.warn(\"TabularDataModule is deprecated since v0.2, please use DataModule instead.\", \n",
    "                      DeprecationWarning)\n",
    "        \n",
    "    __ALL__ = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "DEFAULT_DATA = [\n",
    "    'adult',\n",
    "    'heloc',\n",
    "    'oulad',\n",
    "    'credit',\n",
    "    'cancer',\n",
    "    'student_performance',\n",
    "    'titanic',\n",
    "    'german',\n",
    "    'spam',\n",
    "    'ozone',\n",
    "    'qsar',\n",
    "    'bioresponse',\n",
    "    'churn',\n",
    "    'road',\n",
    "    'dummy'\n",
    " ]\n",
    "\n",
    "DEFAULT_DATA_CONFIGS = { \n",
    "    data: { \n",
    "        'data': f\"{data}/data\", 'model': f\"{data}/model\",\n",
    "    } for data in DEFAULT_DATA\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "# xs, ys = make_classification(n_samples=1000, n_features=10)\n",
    "# xs = pd.DataFrame(xs, columns=[f\"col_{i}\" for i in range(10)])\n",
    "# ys = pd.DataFrame(ys, columns=['label'])\n",
    "# data = pd.concat([xs, ys], axis=1)\n",
    "# os.makedirs('assets/dummy/data', exist_ok=True)\n",
    "# data.to_csv('assets/dummy/data/data.csv', index=False)\n",
    "# config = DataModuleConfig(\n",
    "#     data_name=\"dummy\", \n",
    "#     data_dir=\"assets/dummy/data/data.csv\", \n",
    "#     continous_cols=[f\"col_{i}\" for i in range(10)]\n",
    "# )\n",
    "# dm = DataModule(config)\n",
    "# dm.save('assets/dummy/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data_name in DEFAULT_DATA_CONFIGS.keys():\n",
    "#     print(f\"Loading {data_name}...\")\n",
    "#     shutil.rmtree(f'../relax-assets/{data_name}', ignore_errors=True)\n",
    "#     conf_path = DEFAULT_DATA_CONFIGS[data_name]['conf']\n",
    "#     config = load_json(conf_path)['data_configs']\n",
    "#     dm_config = DataModuleConfig(**config)\n",
    "#     dm = DataModule(dm_config)\n",
    "#     dm.save(f'../relax-assets/{data_name}/data')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data_name in DEFAULT_DATA_CONFIGS.keys():\n",
    "#     print(f\"Loading {data_name}...\")\n",
    "#     DataModule.load_from_path(f'../relax-assets/{data_name}/data')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = load_json('assets/adult/configs.json')['data_configs']\n",
    "# dm_config = DataModuleConfig(**config)\n",
    "# dm = DataModule(dm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _validate_dataname(data_name: str):\n",
    "    if data_name not in DEFAULT_DATA:\n",
    "        raise ValueError(f'`data_name` must be one of {DEFAULT_DATA}, '\n",
    "            f'but got data_name={data_name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_data_module_files(\n",
    "    data_name: str, # The name of data\n",
    "    data_parent_dir: Path, # The directory to save data.\n",
    "    download_original_data: bool = False, # Download original data or not\n",
    "):\n",
    "    files = [\n",
    "        \"features/data.npy\", \"features/treedef.json\",\n",
    "        \"label/data.npy\", \"label/treedef.json\",\n",
    "        \"config.json\"\n",
    "    ]\n",
    "    if download_original_data:\n",
    "        files.append(\"data.csv\")\n",
    "    for f in files:\n",
    "        url = f\"https://huggingface.co/datasets/birkhoffg/ReLax-Assets/resolve/main/{data_name}/data/{f}\"\n",
    "        f_path = data_parent_dir / f'{data_name}/data' / f\n",
    "        os.makedirs(f_path.parent, exist_ok=True)\n",
    "        if not f_path.is_file(): urlretrieve(url, f_path)\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    data_name: str, # The name of data\n",
    "    return_config: bool = False, # Deprecated\n",
    "    data_configs: dict = None, # Data configs to override default configuration\n",
    ") -> DataModule | Tuple[DataModule, DataModuleConfig]: # Return `DataModule` or (`DataModule`, `DataModuleConfig`)\n",
    "    \"\"\"High-level util function for loading `data` and `data_config`.\"\"\"\n",
    "    \n",
    "    _validate_dataname(data_name)\n",
    "\n",
    "    # create new dir\n",
    "    data_parent_dir = Path(os.getcwd()) / \"relax-assets\"\n",
    "    if not data_parent_dir.exists():\n",
    "        os.makedirs(data_parent_dir, exist_ok=True)\n",
    "    # download files\n",
    "    download_data_module_files(\n",
    "        data_name, data_parent_dir, \n",
    "        download_original_data=True\n",
    "    )\n",
    "\n",
    "    if return_config:\n",
    "        warnings.warn(\"`return_config` is deprecated since v0.2. \"\n",
    "                      \"Please access `config` from `DataModule`.\", DeprecationWarning)\n",
    "\n",
    "    # read and override config\n",
    "    # comment them for now since we cannot garantee the override configs are valid\n",
    "    # conf_path = data_parent_dir / f'{data_name}/data/config.json'\n",
    "    # config = load_json(conf_path)\n",
    "    # if not (data_configs is None):\n",
    "    #     config.update(data_configs)\n",
    "    # config = DataModuleConfig(**config)\n",
    "\n",
    "    data_dir = data_parent_dir / f'{data_name}/data'\n",
    "    data_module = DataModule.load_from_path(data_dir, config=data_configs)\n",
    "\n",
    "    return data_module\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_data` easily loads example datasets by passing the `data_name`. \n",
    "For example, you can load the [adult](https://archive.ics.uci.edu/ml/datasets/adult) as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = load_data(data_name = 'adult')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supported Datasets\n",
    "\n",
    "`load_data` currently supports following datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Cont Features</th>\n",
       "      <th># Cat Features</th>\n",
       "      <th># of Data Points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>32561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heloc</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>10459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oulad</th>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>32593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>credit</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancer</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student_performance</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>titanic</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>german</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>4601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ozone</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>2534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qsar</th>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>1055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bioresponse</th>\n",
       "      <td>1776</td>\n",
       "      <td>0</td>\n",
       "      <td>3751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>churn</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>7043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>road</th>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>111762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     # Cont Features  # Cat Features  # of Data Points\n",
       "adult                              2               6             32561\n",
       "heloc                             21               2             10459\n",
       "oulad                             23               8             32593\n",
       "credit                            20               3             30000\n",
       "cancer                            30               0               569\n",
       "student_performance                2              14               649\n",
       "titanic                            2              24               891\n",
       "german                             7              13              1000\n",
       "spam                              57               0              4601\n",
       "ozone                             72               0              2534\n",
       "qsar                              38               3              1055\n",
       "bioresponse                     1776               0              3751\n",
       "churn                              3              16              7043\n",
       "road                              29               3            111762\n",
       "dummy                             10               0              1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| eval: false\n",
    "def display_data_attrbutes(names: list):\n",
    "    attrs = {\n",
    "        '# Cont Features': { data_name: 0 for data_name in names}, \n",
    "        '# Cat Features': { data_name: 0 for data_name in names},\n",
    "        '# of Data Points': { data_name: 0 for data_name in names}, \n",
    "    }\n",
    "    for data_name in names:\n",
    "        dm= load_data(data_name)\n",
    "        config = dm.config\n",
    "        attrs['# Cont Features'][data_name] = len(config.continous_cols)\n",
    "        attrs['# Cat Features'][data_name] = len(config.discret_cols)\n",
    "        attrs['# of Data Points'][data_name] = len(dm.data)\n",
    "\n",
    "        # run tests\n",
    "        # check_datamodule(dm, config)\n",
    "    return pd.DataFrame.from_dict(attrs)\n",
    "\n",
    "display_data_attrbutes(DEFAULT_DATA_CONFIGS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# for data_name in DEFAULT_DATA_CONFIGS.keys():\n",
    "#     dm, config = load_data(\n",
    "#         data_name, return_config=True, data_configs=dict(sample_frac=0.1)\n",
    "#     )\n",
    "#     assert config.sample_frac == 0.1\n",
    "    # check_datamodule(dm, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
