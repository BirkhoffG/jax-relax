{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ml_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from relax.import_essentials import *\n",
    "from relax.data_module import DataModule, DEFAULT_DATA_CONFIGS\n",
    "from relax.utils import validate_configs\n",
    "from relax.base import *\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLPBlock(keras.layers.Layer):\n",
    "    \"\"\"MLP block with leaky relu activation and dropout.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        output_size: int, \n",
    "        dropout_rate: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.dense = keras.layers.Dense(\n",
    "            self.output_size, activation='leaky_relu')\n",
    "        self.dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return x\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class MLP(keras.Model):\n",
    "    \"\"\"MLP model with multiple MLP blocks and a dense layer at the end.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        sizes: list, \n",
    "        output_size: int = 2,\n",
    "        dropout_rate: float = 0.3,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.blocks = []\n",
    "        for size in sizes:\n",
    "            self.blocks.append(MLPBlock(size, dropout_rate))\n",
    "        self.dense = keras.layers.Dense(output_size, activation='softmax')\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training)\n",
    "        return self.dense(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'sizes': [block.output_size for block in self.blocks],\n",
    "            'output_size': self.dense.units,\n",
    "            'dropout_rate': self.blocks[0].dropout_rate,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLModuleConfig(BaseParser):\n",
    "    \"\"\"Configurator of `MLModule`.\"\"\"\n",
    "    \n",
    "    sizes: List[int] = Field(description=\"List of hidden layer sizes.\")\n",
    "    output_size: int = Field(2, description=\"The number of output classes.\")\n",
    "    dropout_rate: float = Field(0.3, description=\"Dropout rate.\")\n",
    "    lr: float = Field(1e-3, description=\"Learning rate.\")\n",
    "    opt_name: str = Field(\"adam\", description=\"Optimizer name.\")\n",
    "    loss: str = Field(\"sparse_categorical_crossentropy\", description=\"Loss function name.\")\n",
    "    metrics: List[str] = Field([\"accuracy\"], description=\"List of metrics names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLModule(BaseModule, TrainableMixedin, PredFnMixedin):\n",
    "    def __init__(self, config: MLModuleConfig, *, model: keras.Model = None, name: str = None):\n",
    "        config = validate_configs(config, MLModuleConfig)\n",
    "        self.model = self._init_model(config, model)\n",
    "        self._is_trained = False\n",
    "        super().__init__(config, name=name)\n",
    "\n",
    "    def _init_model(self, config: MLModuleConfig, model: keras.Model):\n",
    "        if model is None:\n",
    "            model = MLP(\n",
    "                sizes=config.sizes,\n",
    "                output_size=config.output_size,\n",
    "                dropout_rate=config.dropout_rate\n",
    "            )\n",
    "            model.compile(\n",
    "                optimizer=keras.optimizers.get({\n",
    "                    'class_name': config.opt_name, \n",
    "                    'config': {'learning_rate': config.lr}\n",
    "                }),\n",
    "                loss=config.loss,\n",
    "                metrics=config.metrics\n",
    "            )\n",
    "        return model\n",
    "            \n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        data: DataModule, \n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 10,\n",
    "        **fit_kwargs\n",
    "    ):\n",
    "        if isinstance(data, DataModule):\n",
    "            X_train, y_train = data['train']\n",
    "        else:\n",
    "            X_train, y_train = data\n",
    "        self.model.fit(\n",
    "            X_train, y_train, \n",
    "            batch_size=batch_size, \n",
    "            epochs=epochs,\n",
    "            **fit_kwargs\n",
    "        )\n",
    "        self._is_trained = True\n",
    "        return self.model\n",
    "    \n",
    "    @property\n",
    "    def is_trained(self) -> bool:\n",
    "        return self._is_trained\n",
    "    \n",
    "    def save(self, path):\n",
    "        path = Path(path)\n",
    "        if not path.exists():\n",
    "            path.mkdir(parents=True)\n",
    "        # self.model.save_weights(path / \"model.weights.h5\", overwrite=True)\n",
    "        self.model.save(path / \"model.keras\")\n",
    "        with open(path / \"config.json\", \"w\") as f:\n",
    "            json.dump(self.config.dict(), f)\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_path(cls, path):\n",
    "        path = Path(path)\n",
    "        config = MLModuleConfig(**json.load(open(path / \"config.json\")))\n",
    "        model = keras.models.load_model(path / \"model.keras\")\n",
    "        module = cls(config, model=model)\n",
    "        module._is_trained = True\n",
    "        return module\n",
    "    \n",
    "    def pred_fn(self, x):\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model is not trained.\")\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=5000, n_features=10, n_informative=5, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.5581 - loss: 0.7279    \n",
      "Epoch 2/5\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7460 - loss: 0.5062        \n",
      "Epoch 3/5\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7960 - loss: 0.4470        \n",
      "Epoch 4/5\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8195 - loss: 0.4112      \n",
      "Epoch 5/5\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8251 - loss: 0.3935        \n"
     ]
    }
   ],
   "source": [
    "model = MLModule(\n",
    "    MLModuleConfig(sizes=[64, 32, 16],)\n",
    ")\n",
    "model.train((X_train, y_train), epochs=5)\n",
    "assert model.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 18:18:25.386186: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "model.save('tmp/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = MLModule.load_from_path('tmp/model')\n",
    "assert model_1.is_trained\n",
    "assert np.allclose(model_1.pred_fn(X_test), model.pred_fn(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# remove tmp directory\n",
    "shutil.rmtree('tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ML Module\n",
    "\n",
    "TODO: Need test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_ml_module(name: str, path: str = None):\n",
    "    if path is None:\n",
    "        path = Path('relax_assets') / name / 'model'\n",
    "    else:\n",
    "        path = Path(path)\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)\n",
    "    _model_path = f\"assets/{name}/model\"\n",
    "    model_url = f\"https://github.com/BirkhoffG/ReLax/raw/master/{_model_path}/model.keras\"\n",
    "    config_url = f\"https://github.com/BirkhoffG/ReLax/raw/master/{_model_path}/config.json\"\n",
    "\n",
    "    if not (path / \"model.keras\").exists():\n",
    "        urlretrieve(model_url, filename=str(path / \"model.keras\"))\n",
    "    if not (path / \"config.json\").exists():\n",
    "        urlretrieve(config_url, filename=str(path / \"config.json\"))   \n",
    "    \n",
    "\n",
    "def load_ml_module(name: str) -> MLModule:\n",
    "    \"\"\"Load the ML module\"\"\"\n",
    "\n",
    "    if name not in DEFAULT_DATA_CONFIGS.keys():\n",
    "        raise ValueError(f'`data_name` must be one of {DEFAULT_DATA_CONFIGS.keys()}, '\n",
    "            f'but got data_name={name}.')\n",
    "\n",
    "    download_ml_module(name)\n",
    "    return MLModule.load_from_path(f\"relax_assets/{name}/model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
