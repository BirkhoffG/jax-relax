{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2C\n",
    "\n",
    "https://arxiv.org/abs/2209.13446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.l2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import ParametricCFModule\n",
    "from relax.base import BaseConfig\n",
    "from relax.utils import *\n",
    "from relax.data_utils import Feature, FeaturesList\n",
    "from relax.ml_model import MLP, MLPBlock\n",
    "from relax.data_module import DataModule\n",
    "from keras.random import SeedGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import torch\n",
    "import relax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end = jnp.array([[0, 1], [1, 2], [2, 3], [3, 5]])\n",
    "xs = jrand.normal(jrand.PRNGKey(0), (4, 5),)\n",
    "cfs = jrand.normal(jrand.PRNGKey(1), (4, 5),)\n",
    "prob = jrand.uniform(jrand.PRNGKey(2), (4, 4),)\n",
    "\n",
    "# split xs into 4 parts according to start_end\n",
    "xs_split = jnp.split(xs, start_end[:-1, 1], axis=1)\n",
    "cfs_split = jnp.split(cfs, start_end[:-1, 1], axis=1)\n",
    "prob_split = jnp.split(prob, start_end.shape[0], axis=1)\n",
    "\n",
    "def perturb(x, cf, prob):\n",
    "    return x * (1 - prob) + cf * prob\n",
    "\n",
    "perturbed = jax.tree_util.tree_map(\n",
    "    perturb, xs_split, cfs_split, prob_split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2C Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def gumbel_softmax(\n",
    "    key: jrand.PRNGKey, # Random key\n",
    "    logits: Array, # Logits for each class. Shape (batch_size, num_classes)\n",
    "    tau: float, # Temperature for the Gumbel softmax\n",
    "):\n",
    "    \"\"\"The Gumbel softmax function.\"\"\"\n",
    "\n",
    "    gumbel_noise = jrand.gumbel(key, shape=logits.shape)\n",
    "    y = logits + gumbel_noise\n",
    "    return jax.nn.softmax(y / tau, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_categorical(\n",
    "    key: jrand.PRNGKey, # Random key\n",
    "    logits: Array, # Logits for each class. Shape (batch_size, num_classes)\n",
    "    tau: float, # Temperature for the Gumbel softmax\n",
    "    training: bool = True, # Apply gumbel softmax if training\n",
    "):\n",
    "    \"\"\"Sample from a categorical distribution.\"\"\"\n",
    "\n",
    "    def sample_cat(key, logits):\n",
    "        cat = jrand.categorical(key, logits=logits, axis=-1)\n",
    "        return jax.nn.one_hot(cat, logits.shape[-1])\n",
    "\n",
    "    return lax.cond(\n",
    "        training,\n",
    "        lambda _: gumbel_softmax(key, logits, tau=tau),\n",
    "        lambda _: sample_cat(key, logits),\n",
    "        None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = jnp.array([[2.0, 1.0, 0.1], [1.0, 2.0, 3.0]])\n",
    "key = jrand.PRNGKey(0)\n",
    "output = sample_categorical(key, logits, tau=0.5, training=True)\n",
    "assert output.shape == logits.shape\n",
    "assert jnp.allclose(output.sum(axis=-1), 1.0)\n",
    "# low temperature -> one-hot\n",
    "output = sample_categorical(key, logits, tau=0.01, training=True)\n",
    "assert jnp.array_equal(\n",
    "    output.argmax(axis=-1), logits.argmax(axis=-1)\n",
    ")\n",
    "# high temperature -> uniform\n",
    "output = sample_categorical(key, logits, tau=100, training=True)\n",
    "assert jnp.max(output) - jnp.min(output) < 0.5\n",
    "\n",
    "output = sample_categorical(key, logits, tau=0.5, training=False)\n",
    "assert output.shape == logits.shape\n",
    "assert jnp.array_equal(\n",
    "    output.argmax(axis=-1), logits.argmax(axis=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_bernouli(\n",
    "    key: jrand.PRNGKey, # Random key\n",
    "    prob: Array, # Logits for each class. Shape (batch_size, 1)\n",
    "    tau: float, # Temperature for the Gumbel softmax\n",
    "    training: bool = True, # Apply gumbel softmax if training\n",
    ") -> Array:\n",
    "    \"\"\"\"Sample from a bernouli distribution.\"\"\"\n",
    "\n",
    "    def sample_ber(key, prob):\n",
    "        return jrand.bernoulli(key, p=prob).astype(prob.dtype)\n",
    "    \n",
    "    def gumbel_ber(key, prob, tau):\n",
    "        key_1, key_2 = jrand.split(key)\n",
    "        gumbel_1 = jrand.gumbel(key_1, shape=prob.shape)\n",
    "        gumbel_2 = jrand.gumbel(key_2, shape=prob.shape)\n",
    "        no_logits = (prob * jnp.exp(gumbel_1)) / tau\n",
    "        de_logits = no_logits + ((1. - prob) * jnp.exp(gumbel_2)) / tau\n",
    "        return no_logits / de_logits\n",
    "    \n",
    "    return lax.cond(\n",
    "        training,\n",
    "        lambda _: gumbel_ber(key, prob, tau),\n",
    "        lambda _: sample_ber(key, prob),\n",
    "        None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_fn(feature_indices: list[tuple[int, int]]):\n",
    "    feature_indices = tuple([x[1] for x in feature_indices[:-1]])\n",
    "\n",
    "    @ft.partial(jit, static_argnums=1)\n",
    "    def split_xs(xs, feature_indices):\n",
    "        return jnp.split(xs, list(feature_indices), axis=-1)\n",
    "    \n",
    "    @ft.partial(jit, static_argnums=1)\n",
    "    def split_prob(prob, feature_indices):\n",
    "        return jnp.split(prob, len(feature_indices) + 1, axis=-1)\n",
    "    \n",
    "    return ft.partial(split_xs, feature_indices=feature_indices), ft.partial(split_prob, feature_indices=feature_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end = [(0, 1), (1, 2), (2, 3), (3, 5)]\n",
    "split_xs, split_prob = split_fn(start_end)\n",
    "assert len(split_xs(xs)) == len(start_end)\n",
    "assert len(split_prob(prob)) == len(start_end) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2CModel(keras.Model):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_layers: list[int],\n",
    "        selector_layers: list[int],\n",
    "        feature_indices: list[tuple[int, int]] = None,\n",
    "        immutable_mask: Array = None,\n",
    "        pred_fn: Callable = None,\n",
    "        alpha: float = 1e-4, # Sparsity regularization\n",
    "        tau: float = 0.7,\n",
    "        seed: int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.generator_layers = generator_layers\n",
    "        self.selector_layers = selector_layers\n",
    "        self.pred_fn = pred_fn\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        seed = seed or get_config().global_seed\n",
    "        self.seed_generator = SeedGenerator(seed)\n",
    "        self.set_features_info(feature_indices)\n",
    "        self.set_immutable_mask(immutable_mask)\n",
    "        \n",
    "        # split functions\n",
    "        self.split_xs_fn, self.split_prob_fn = split_fn(self.feature_indices)\n",
    "\n",
    "    @property\n",
    "    def start_end_indices(self):\n",
    "        feature_indices = jnp.array(list(map(lambda x: list(x), self.feature_indices)))\n",
    "        return feature_indices[:-1, 1]\n",
    "\n",
    "    def set_features_info(self, feature_indices: list[tuple[int, int]]):\n",
    "        self.feature_indices = feature_indices\n",
    "        # self.feature_indices = jnp.array(\n",
    "        #     list(map(lambda x: list(x), feature_indices)))\n",
    "        # assert self.feature_indices.shape == (len(feature_indices), 2)\n",
    "        # TODO: check if the feature indices are valid\n",
    "\n",
    "    def set_immutable_mask(self, immutable_mask: Array):\n",
    "        self.immutable_mask = immutable_mask\n",
    "\n",
    "    def set_pred_fn(self, pred_fn: Callable):\n",
    "        self.pred_fn = pred_fn\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n_feats = len(self.feature_indices)\n",
    "        self.generator = MLP(\n",
    "            sizes=self.generator_layers,\n",
    "            output_size=input_shape[-1],\n",
    "            dropout_rate=0.0,\n",
    "            last_activation=\"linear\",\n",
    "        )\n",
    "        self.selector = MLP(\n",
    "            sizes=self.selector_layers,\n",
    "            output_size=n_feats,\n",
    "            dropout_rate=0.0,\n",
    "            last_activation=\"sigmoid\",\n",
    "        )\n",
    "\n",
    "    def compute_l2c_loss(self, inputs, cfs, probs):\n",
    "        # inputs = self.split_xs_fn(inputs)\n",
    "        # cfs = self.split_xs_fn(cfs)\n",
    "        y_target = self.pred_fn(inputs).argmin(axis=-1)\n",
    "        y_pred = self.pred_fn(cfs)\n",
    "        validity_loss = keras.losses.sparse_categorical_crossentropy(\n",
    "            y_target, y_pred\n",
    "        ).mean()\n",
    "        sparsity = jnp.linalg.norm(probs, ord=1) * self.alpha\n",
    "        return validity_loss, sparsity\n",
    "        \n",
    "    def forward(self, rng_key, inputs, training=False):\n",
    "        \n",
    "        def perturb(x, cf, prob, immutable):\n",
    "            cf = sample_categorical(\n",
    "                key_2, cf, tau=tau, training=training\n",
    "            )\n",
    "            prob = prob * (1 - immutable)\n",
    "            return x * (1 - prob) + cf * prob\n",
    "\n",
    "        key_1, key_2 = jrand.split(rng_key)\n",
    "        select_probs = self.selector(inputs, training=training)\n",
    "        probs = sample_bernouli(\n",
    "            key_1, select_probs, \n",
    "            tau=self.tau, training=training\n",
    "        )\n",
    "        cfs_logits = self.generator(inputs, training=training)\n",
    "                \n",
    "        # xs = jnp.split(inputs, start_end, axis=-1)\n",
    "        # cfs = jnp.split(cfs_logits, start_end, axis=-1)\n",
    "        # probs = jnp.split(prob, len(self.feature_indices), axis=-1)\n",
    "        xs = self.split_xs_fn(inputs)\n",
    "        cfs = self.split_xs_fn(cfs_logits)\n",
    "        probs = self.split_prob_fn(probs)\n",
    "        immutables = self.split_prob_fn(self.immutable_mask)\n",
    "        tau = self.tau\n",
    "        \n",
    "        cfs = jax.tree_util.tree_map(\n",
    "            perturb, xs, cfs, probs, immutables #self.tau, training\n",
    "        )\n",
    "        cfs = jnp.concatenate(cfs, axis=-1)\n",
    "        probs = jnp.concatenate(probs, axis=-1)\n",
    "        return cfs, probs\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        rng_key = self.seed_generator.next()\n",
    "        cfs, probs = self.forward(rng_key, inputs, training=training)\n",
    "        # loss = self.compute_l2c_loss(inputs, cfs, probs)\n",
    "\n",
    "        validity_loss, sparsity = self.compute_l2c_loss(inputs, cfs, probs)\n",
    "        self.add_loss(validity_loss)\n",
    "        self.add_loss(sparsity)\n",
    "        return cfs   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def qcut(\n",
    "    x: Array, # Input array\n",
    "    q: int, # Number of quantiles\n",
    "    axis: int = 0, # Axis to quantile\n",
    ") -> tuple[Array, Array]: # (digitized array, quantiles)\n",
    "    \"\"\"Quantile binning.\"\"\"\n",
    "    \n",
    "    # Handle edge cases: empty array or single element\n",
    "    if x.size <= 1:\n",
    "        return jnp.zeros_like(x), jnp.array([])\n",
    "    quantiles = jnp.quantile(x, jnp.linspace(0, 1, q + 1)[1:-1], axis=axis)\n",
    "    digitized = jnp.digitize(x, quantiles)\n",
    "    return digitized, quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitized, quantiles = qcut(jnp.arange(10), 4)\n",
    "assert digitized.shape == (10,)\n",
    "assert quantiles.shape == (3,)\n",
    "assert jnp.allclose(\n",
    "    digitized, jnp.array([0,0,0,1,1,2,2,3,3,3])\n",
    ")\n",
    "\n",
    "quantiles_true = jnp.array([0, 2.25, 4.5, 6.75, 9])\n",
    "assert jnp.allclose(\n",
    "    quantiles, quantiles_true[1:-1]\n",
    ")\n",
    "x_empty = jnp.array([])\n",
    "q = 2\n",
    "digitized_empty, quantiles_empty = qcut(x_empty, q)\n",
    "assert digitized_empty.size == 0 and quantiles_empty.size == 0\n",
    "# Test with single element array\n",
    "x_single = jnp.array([1])\n",
    "digitized_single, quantiles_single = qcut(x_single, q)\n",
    "assert digitized_single.size == 1 and quantiles_single.size == 0\n",
    "\n",
    "# Test with large q value\n",
    "xs = jnp.array([1, 2, 3, 4, 5, 6])\n",
    "q_large = 10\n",
    "_, quantiles_large = qcut(xs, q_large)\n",
    "assert len(quantiles_large) == q_large - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def qcut_inverse(\n",
    "    digitized: Array, # Digitized One-Hot Encoding Array\n",
    "    quantiles: Array, # Quantiles\n",
    ") -> Array:\n",
    "    \"\"\"Inverse of qcut.\"\"\"\n",
    "    \n",
    "    result = digitized @ quantiles\n",
    "    if result.ndim == 1:\n",
    "        result = result[..., None]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitized, quantiles = qcut(jnp.arange(10), 4)\n",
    "ohe_digitized = jax.nn.one_hot(digitized, 4)\n",
    "# continuous feats\n",
    "quantiles_inv = qcut_inverse(ohe_digitized, jnp.arange(4))\n",
    "assert quantiles_inv.shape == (10, 1)\n",
    "# discrete feats\n",
    "quantiles_inv = qcut_inverse(ohe_digitized, jnp.identity(4))\n",
    "assert jnp.array_equal(quantiles_inv, ohe_digitized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cut_quantiles(\n",
    "    quantiles: Array, # Quantiles\n",
    "    xs: Array, # Input array\n",
    "):\n",
    "    quantiles = jnp.concatenate([\n",
    "        xs.min(axis=0, keepdims=True), \n",
    "        quantiles, \n",
    "        xs.max(axis=0, keepdims=True)\n",
    "    ])\n",
    "    quantiles = (quantiles[1:] + quantiles[:-1]) / 2\n",
    "    return quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def discretize_xs(\n",
    "    xs: Array, # Input array\n",
    "    is_categorical_and_indices: list[tuple[bool, tuple[int, int]]], # Features list\n",
    "    q: int = 4, # Number of quantiles\n",
    ") -> tuple[list[Array], list[Array], list[Array], list[list[int, int]]]: # (discretized array, indices_and_quantiles_and_mid)\n",
    "    \"\"\"Discretize continuous features.\"\"\"\n",
    "    \n",
    "    discretized_xs = []\n",
    "    mid_quantiles = []\n",
    "    quantiles_feats = []\n",
    "    feature_indices = []\n",
    "    discretized_start, discretized_end = 0, 0\n",
    "\n",
    "    for is_categorical, (start, end) in is_categorical_and_indices:\n",
    "        if is_categorical:\n",
    "            discretized, quantiles, mid = xs[:, start:end], None, jnp.identity(end - start)\n",
    "            discretized_end += end - start\n",
    "        else:\n",
    "            discretized, quantiles = qcut(xs[:, start:end].reshape(-1), q=q)\n",
    "            mid = cut_quantiles(quantiles, xs[:, start])\n",
    "            discretized = jax.nn.one_hot(discretized, q)\n",
    "            discretized_end += discretized.shape[-1]\n",
    "        \n",
    "        discretized_xs.append(discretized)\n",
    "        quantiles_feats.append(quantiles)\n",
    "        mid_quantiles.append(mid)\n",
    "        feature_indices.append([discretized_start, discretized_end])\n",
    "        discretized_start = discretized_end\n",
    "    # discretized_xs = jnp.concatenate(discretized_xs, axis=-1)\n",
    "    return discretized_xs, quantiles_feats, mid_quantiles, feature_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = relax.load_data(\"dummy\")\n",
    "xs, ys = dm['train']\n",
    "is_categorical_and_indices = [\n",
    "    (feat.is_categorical, indices) for feat, indices in zip(dm.features, dm.features.feature_indices)\n",
    "]\n",
    "discretized_xs, quantiles_feats, mid_quantiles, feature_indices = discretize_xs(xs, is_categorical_and_indices)\n",
    "assert len(discretized_xs) == len(is_categorical_and_indices)\n",
    "assert all(discretized_xs[i].shape[1] == 4 for i in range(len(discretized_xs)))\n",
    "\n",
    "assert len(quantiles_feats) == len(is_categorical_and_indices)\n",
    "assert all(len(quantiles_feats[i]) == 3 for i in range(len(quantiles_feats)))\n",
    "assert len(mid_quantiles) == len(is_categorical_and_indices)\n",
    "assert all(len(mid_quantiles[i]) == 4 for i in range(len(mid_quantiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Discretizer:\n",
    "    \"\"\"Discretize continuous features.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        is_cat_and_indices: list[tuple[bool, tuple[int, int]]], # Features list\n",
    "        q: int = 4 # Number of quantiles\n",
    "    ):\n",
    "        self.is_cat_and_indices = is_cat_and_indices\n",
    "        self.q = q\n",
    "\n",
    "    @property\n",
    "    def transform_indices(self):\n",
    "        return [x[1][1] for x in self.is_cat_and_indices[:-1]]\n",
    "    \n",
    "    @property\n",
    "    def inverse_transform_indices(self):\n",
    "        return [x[1] for x in self.indices[:-1]]\n",
    "        \n",
    "    def fit(self, xs: Array):\n",
    "        _, self.quantiles, self.mid_quantiles, self.indices = discretize_xs(\n",
    "            xs, self.is_cat_and_indices, self.q\n",
    "        )\n",
    "        self.transform_indices, self.inverse_transform_indices\n",
    "        return self\n",
    "\n",
    "    # @ft.partial(jit, static_argnums=0)\n",
    "    def transform(self, xs: Array):\n",
    "        def digitize_fn(x, quantile):\n",
    "            if quantile is None: \n",
    "                return x\n",
    "            else: \n",
    "                digitized = jnp.digitize(x.reshape(-1), quantile)\n",
    "                return jax.nn.one_hot(digitized, self.q)\n",
    "\n",
    "        # indices = [x[1][1] for x in self.is_cat_and_indices[:-1]]\n",
    "        # print(indices)\n",
    "        digitized_xs = jnp.split(xs, self.transform_indices, axis=-1) # [feat_1, ..., feat_n]\n",
    "        digitized_xs = jax.tree_util.tree_map(\n",
    "            digitize_fn, digitized_xs, self.quantiles\n",
    "        )        \n",
    "        return jnp.concatenate(digitized_xs, axis=-1)\n",
    "\n",
    "    def fit_transform(self, xs: Array):\n",
    "        self.fit(xs)\n",
    "        return self.transform(xs)\n",
    "\n",
    "    # @ft.partial(jit, static_argnums=0)\n",
    "    def inverse_transform(self, xs: Array):\n",
    "        xs = jnp.split(xs, self.inverse_transform_indices, axis=-1)\n",
    "        xs = jax.tree_util.tree_map(\n",
    "            lambda x, q: qcut_inverse(x, q), xs, self.mid_quantiles\n",
    "        )\n",
    "        return jnp.concatenate(xs, axis=-1)\n",
    "    \n",
    "    def inversed_transform_pytree(self, xs: list[Array]):\n",
    "        xs = jax.tree_util.tree_map(\n",
    "            lambda x, q: qcut(x, q), xs, self.mid_quantiles)\n",
    "        return jnp.concatenate(xs, axis=-1)\n",
    "    \n",
    "    def get_pred_fn(self, pred_fn: Callable[[Array], Array]):\n",
    "        def _pred_fn(xs: Array):\n",
    "            return pred_fn(self.inverse_transform(xs))\n",
    "            # return pred_fn(self.inversed_transform_pytree(xs))\n",
    "        return _pred_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = relax.load_data(\"adult\")\n",
    "xs, ys = dm['train']\n",
    "is_categorical_and_indices = [\n",
    "    (feat.is_categorical, indices) for feat, indices in zip(dm.features, dm.features.feature_indices)\n",
    "]\n",
    "\n",
    "dis = Discretizer(is_categorical_and_indices)\n",
    "dis.fit(xs)\n",
    "digitized_xs_1 = dis.transform(xs)\n",
    "assert digitized_xs_1.shape == (xs.shape[0], 35)\n",
    "# assert jnp.array_equal(jnp.concatenate(discretized_xs, axis=-1), digitized_xs_1)\n",
    "inversed_xs = dis.inverse_transform(digitized_xs_1)\n",
    "assert xs.shape == inversed_xs.shape\n",
    "# assert jnp.unique(inversed_xs).size == xs.shape[1] * 4\n",
    "\n",
    "ml_module = relax.load_ml_module(\"adult\")\n",
    "pred_fn = dis.get_pred_fn(ml_module.pred_fn)\n",
    "# digitized_xs_1 = split_xs(xs)\n",
    "y = pred_fn(digitized_xs_1)\n",
    "assert y.shape == (xs.shape[0], 2)\n",
    "\n",
    "def f(x, y):\n",
    "    y_pred = pred_fn(x)\n",
    "    return jnp.mean((y_pred - y) ** 2)\n",
    "\n",
    "grad = jax.grad(f)(digitized_xs_1, ys)\n",
    "assert grad.shape == digitized_xs_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2C Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2CConfig(BaseConfig):\n",
    "    generator_layers: list[int] = Field(\n",
    "        [64, 64, 64], description=\"Generator MLP layers.\"\n",
    "    )\n",
    "    selector_layers: list[int] = Field(\n",
    "        [64], description=\"Selector MLP layers.\"\n",
    "    )\n",
    "    lr: float = Field(1e-3, description=\"Model learning rate.\")\n",
    "    opt_name: str = Field(\"adam\", description=\"Optimizer name of training L2C.\")\n",
    "    alpha: float = Field(1e-4, description=\"Sparsity regularization.\")\n",
    "    tau: float = Field(0.7, description=\"Temperature for the Gumbel softmax.\")\n",
    "    q: int = Field(4, description=\"Number of quantiles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2C(ParametricCFModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Dict | L2CConfig = None,\n",
    "        l2c_model: L2CModel = None,\n",
    "        name: str = \"l2c\",\n",
    "    ):\n",
    "        if config is None:\n",
    "            config = L2CConfig()\n",
    "        config = validate_configs(config, L2CConfig)\n",
    "        name = name or \"l2c\"\n",
    "        self.l2c_model = l2c_model\n",
    "        super().__init__(config=config, name=name)\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        data: DataModule, \n",
    "        pred_fn: Callable,\n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 10,\n",
    "        **fit_kwargs\n",
    "    ):\n",
    "        if not isinstance(data, DataModule):\n",
    "            raise ValueError(f\"Only support `data` to be `DataModule`, \"\n",
    "                             f\"got type=`{type(data).__name__}` instead.\")\n",
    "        \n",
    "        xs_train, ys_train = data['train']\n",
    "        self.discretizer = Discretizer(\n",
    "            [(feat.is_categorical, indices) for feat, indices in zip(data.features, data.features.feature_indices)],\n",
    "            q=self.config.q\n",
    "        )\n",
    "        discretized_xs_train = self.discretizer.fit_transform(xs_train)\n",
    "        pred_fn = self.discretizer.get_pred_fn(pred_fn)\n",
    "        features_indices = self.discretizer.indices\n",
    "\n",
    "        self.l2c_model = L2CModel(\n",
    "            generator_layers=self.config.generator_layers,\n",
    "            selector_layers=self.config.selector_layers,\n",
    "            feature_indices=features_indices,\n",
    "            immutable_mask=jnp.array([feat.is_immutable for feat in data.features], dtype=jnp.int32),\n",
    "            pred_fn=pred_fn,\n",
    "            alpha=self.config.alpha,\n",
    "            tau=self.config.tau,\n",
    "        )\n",
    "        self.l2c_model.compile(\n",
    "            optimizer=keras.optimizers.get({\n",
    "                'class_name': self.config.opt_name, \n",
    "                'config': {'learning_rate': self.config.lr}\n",
    "            }),\n",
    "            loss=None,\n",
    "        )\n",
    "        self.l2c_model.fit(\n",
    "            discretized_xs_train, ys_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            **fit_kwargs\n",
    "        )\n",
    "        self._is_trained = True\n",
    "        return self\n",
    "    \n",
    "    @auto_reshaping('x')\n",
    "    def generate_cf(\n",
    "        self,\n",
    "        x: Array,\n",
    "        pred_fn: Callable = None,\n",
    "        y_target: Array = None,\n",
    "        rng_key: jrand.PRNGKey = None,\n",
    "        **kwargs\n",
    "    ) -> Array:\n",
    "        \n",
    "        @jax.jit\n",
    "        def generate_cf(x: Array):\n",
    "            discretized_x = self.discretizer.transform(x)\n",
    "            cfs, probs = self.l2c_model.forward(rng_key, discretized_x, training=False)\n",
    "            return self.discretizer.inverse_transform(cfs)\n",
    "        return generate_cf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = relax.load_data('adult')\n",
    "ml_module = relax.load_ml_module('adult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.8767   \n",
      "Epoch 2/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 0.1725     \n",
      "Epoch 3/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 0.1539    \n",
      "Epoch 4/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 0.1462    \n",
      "Epoch 5/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 0.1434    \n",
      "Epoch 6/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 0.1389    \n",
      "Epoch 7/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 0.1383    \n",
      "Epoch 8/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 0.1372    \n",
      "Epoch 9/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 0.1360    \n",
      "Epoch 10/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 0.1345    \n"
     ]
    }
   ],
   "source": [
    "l2c = L2C()\n",
    "exp = relax.generate_cf_explanations(\n",
    "    l2c, dm, ml_module.pred_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>validity</th>\n",
       "      <th>proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <th>l2c</th>\n",
       "      <td>0.827124</td>\n",
       "      <td>0.98099</td>\n",
       "      <td>6.412683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                acc  validity  proximity\n",
       "adult l2c  0.827124   0.98099   6.412683"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relax.benchmark_cfs([exp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_gen = ft.partial(l2c.generate_cf, pred_fn=ml_module.pred_fn)\n",
    "cfs = jax.vmap(partial_gen)(dm.xs, rng_key=jrand.split(jrand.PRNGKey(0), dm.xs.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
