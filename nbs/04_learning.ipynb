{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "> Functions for training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from relax.utils import show_doc\n",
    "show_doc_parser = show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.data import TabularDataModule\n",
    "from relax.module import BaseTrainingModule\n",
    "from relax.logger import TensorboardLogger\n",
    "from relax.utils import validate_configs\n",
    "from relax._ckpt_manager import CheckpointManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainingConfigs(BaseParser):\n",
    "    \"\"\"Configurator of `train_model`.\"\"\"\n",
    "    \n",
    "    n_epochs: int = Field(\n",
    "        description=\"Number of epochs.\"\n",
    "    )\n",
    "    batch_size: int = Field(\n",
    "        description=\"Batch size.\"\n",
    "    )\n",
    "    monitor_metrics: Optional[str] = Field(\n",
    "        None, description=\"Monitor metrics used to evaluate the training result after each epoch.\"\n",
    "    )\n",
    "    seed: int = Field(\n",
    "        42, description=\"Seed for generating random number.\"\n",
    "    )\n",
    "    log_dir: str = Field(\n",
    "        \"log\", description=\"The name for the directory that holds logged data during training.\"\n",
    "    )\n",
    "    logger_name: str = Field(\n",
    "        \"debug\", description=\"The name for the directory that holds logged data during training under log directory.\"\n",
    "    )\n",
    "    log_on_step: bool = Field(\n",
    "        False, description=\"Log the evaluate metrics at the current step.\"\n",
    "    )\n",
    "    max_n_checkpoints: int = Field(\n",
    "        3, description=\"Maximum number of checkpoints stored.\"\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def PRNGSequence(self):\n",
    "        return hk.PRNGSequence(self.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_model_with_states(\n",
    "    training_module: BaseTrainingModule,\n",
    "    params: hk.Params,\n",
    "    opt_state: optax.OptState,\n",
    "    data_module: TabularDataModule,\n",
    "    t_configs: Dict[str, Any] | TrainingConfigs,\n",
    ") -> Tuple[hk.Params, optax.OptState]:\n",
    "    \"\"\"Train models with `params` and `opt_state`.\"\"\"\n",
    "\n",
    "    t_configs = validate_configs(t_configs, TrainingConfigs)\n",
    "    keys = t_configs.PRNGSequence\n",
    "    # define logger\n",
    "    logger = TensorboardLogger(\n",
    "        log_dir=t_configs.log_dir,\n",
    "        name=t_configs.logger_name,\n",
    "        on_step=t_configs.log_on_step,\n",
    "    )\n",
    "    logger.save_hyperparams(t_configs.dict())\n",
    "    if training_module.hparams:\n",
    "        logger.save_hyperparams(training_module.hparams)\n",
    "\n",
    "    training_module.init_logger(logger)\n",
    "    # define checkpoint manageer\n",
    "    if t_configs.monitor_metrics is None:\n",
    "        monitor_metrics = None\n",
    "    else:\n",
    "        monitor_metrics = f\"{t_configs.monitor_metrics}_epoch\"\n",
    "\n",
    "    ckpt_manager = CheckpointManager(\n",
    "        log_dir=Path(training_module.logger.log_dir) / \"checkpoints\",\n",
    "        monitor_metrics=monitor_metrics,\n",
    "        max_n_checkpoints=t_configs.max_n_checkpoints,\n",
    "    )\n",
    "    # dataloaders\n",
    "    train_loader = data_module.train_dataloader(t_configs.batch_size)\n",
    "    val_loader = data_module.val_dataloader(t_configs.batch_size)\n",
    "\n",
    "    # start training\n",
    "    for epoch in range(t_configs.n_epochs):\n",
    "        training_module.logger.on_epoch_started()\n",
    "        # training\n",
    "        with tqdm(\n",
    "            train_loader, unit=\"batch\", leave=epoch == t_configs.n_epochs - 1\n",
    "        ) as t_loader:\n",
    "            t_loader.set_description(f\"Epoch {epoch}\")\n",
    "            for batch in t_loader:\n",
    "                x, y = map(device_put, tuple(batch))\n",
    "                params, opt_state = training_module.training_step(\n",
    "                    params, opt_state, next(keys), (x, y)\n",
    "                )\n",
    "                # logs = training_module.training_step_logs(\n",
    "                #     params, next(keys), (x, y))\n",
    "                logs = training_module.logger.get_last_logs()\n",
    "                t_loader.set_postfix(**logs)\n",
    "                # logger.log(logs)\n",
    "\n",
    "        # validation\n",
    "        for batch in val_loader:\n",
    "            x, y = map(device_put, tuple(batch))\n",
    "            logs = training_module.validation_step(params, next(keys), (x, y))\n",
    "            # logger.log(logs)\n",
    "        epoch_logs = training_module.logger.on_epoch_finished()\n",
    "        ckpt_manager.update_checkpoints(params, opt_state, epoch_logs, epoch)\n",
    "\n",
    "    training_module.logger.close()\n",
    "    return params, opt_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_model(\n",
    "    training_module: BaseTrainingModule, # Training module\n",
    "    data_module: TabularDataModule, # Data module\n",
    "    t_configs: Dict[str, Any] | TrainingConfigs, # Training configurator\n",
    ") -> Tuple[hk.Params, optax.OptState]:\n",
    "    \"\"\"Train models.\"\"\"\n",
    "    \n",
    "    t_configs = validate_configs(t_configs, TrainingConfigs)\n",
    "    keys = t_configs.PRNGSequence \n",
    "    params, opt_state = training_module.init_net_opt(data_module, next(keys))\n",
    "    return train_model_with_states(\n",
    "        training_module=training_module,\n",
    "        params=params,\n",
    "        opt_state=opt_state,\n",
    "        data_module=data_module,\n",
    "        t_configs=t_configs,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "A siimple example to train a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.data import TabularDataModule, load_data\n",
    "from relax.module import PredictiveTrainingModule, PredictiveModelConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 96/96 [00:02<00:00, 47.02batch/s, train/train_loss_1=0.0614]\n"
     ]
    }
   ],
   "source": [
    "datamodule = load_data('adult')\n",
    "\n",
    "params, opt_state = train_model(\n",
    "    PredictiveTrainingModule({'sizes': [50, 10, 50], 'lr': 0.003}), \n",
    "    datamodule, t_configs={\n",
    "        'n_epochs': 10, 'batch_size': 256, 'monitor_metrics': 'val/val_loss'\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
