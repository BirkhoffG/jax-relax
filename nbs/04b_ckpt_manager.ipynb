{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint Manager\n",
    "\n",
    "> Manage the model and optimizer checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp _ckpt_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from relax.import_essentials import *\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# https://github.com/deepmind/dm-haiku/issues/18#issuecomment-981814403\n",
    "def save_checkpoint(state, ckpt_dir: Path):\n",
    "    with open(os.path.join(ckpt_dir, \"params.npy\"), \"wb\") as f:\n",
    "        for x in jax.tree_util.tree_leaves(state):\n",
    "            np.save(f, x, allow_pickle=False)\n",
    "\n",
    "    tree_struct = jax.tree_util.tree_map(lambda t: 0, state)\n",
    "    with open(os.path.join(ckpt_dir, \"tree.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(tree_struct, f)\n",
    "\n",
    "\n",
    "def load_checkpoint(ckpt_dir: Path):\n",
    "    with open(os.path.join(ckpt_dir, \"tree.pkl\"), \"rb\") as f:\n",
    "        tree_struct = pickle.load(f)\n",
    "\n",
    "    leaves, treedef = jax.tree_util.tree_flatten(tree_struct)\n",
    "    with open(os.path.join(ckpt_dir, \"params.npy\"), \"rb\") as f:\n",
    "        flat_state = [np.load(f) for _ in leaves]\n",
    "\n",
    "    return jax.tree_util.tree_unflatten(treedef, flat_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CheckpointManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        log_dir: Union[Path, str],\n",
    "        monitor_metrics: Optional[str],\n",
    "        max_n_checkpoints: int = 3,\n",
    "    ):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.monitor_metrics = monitor_metrics\n",
    "        self.max_n_checkpoints = max_n_checkpoints\n",
    "        self.checkpoints = OrderedDict()\n",
    "        self.n_checkpoints = 0\n",
    "        if self.monitor_metrics is None:\n",
    "            warnings.warn(\n",
    "                \"`monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\"\n",
    "            )\n",
    "\n",
    "    # update checkpoints based on monitor_metrics\n",
    "    def update_checkpoints(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        opt_state: optax.OptState,\n",
    "        epoch_logs: Dict[str, float],\n",
    "        epochs: int,\n",
    "        steps: Optional[int] = None,\n",
    "    ):\n",
    "        if self.monitor_metrics is None:\n",
    "            return\n",
    "        if self.monitor_metrics not in epoch_logs:\n",
    "            raise ValueError(\n",
    "                \"The monitor_metrics ({}) is not appropriately configured.\".format(\n",
    "                    self.monitor_metrics\n",
    "                )\n",
    "            )\n",
    "        metric = float(epoch_logs[self.monitor_metrics])\n",
    "        if steps:\n",
    "            ckpt_name = f\"epoch={epochs}_step={steps}\"\n",
    "        else:\n",
    "            ckpt_name = f\"epoch={epochs}\"\n",
    "\n",
    "        if self.n_checkpoints < self.max_n_checkpoints:\n",
    "            self.checkpoints[metric] = ckpt_name\n",
    "            self.save_net_opt(params, opt_state, ckpt_name)\n",
    "            self.n_checkpoints += 1\n",
    "        else:\n",
    "            old_metric, old_ckpt_name = self.checkpoints.popitem(last=True)\n",
    "            if metric < old_metric:\n",
    "                self.checkpoints[metric] = ckpt_name\n",
    "                self.save_net_opt(params, opt_state, ckpt_name)\n",
    "                self.delete_net_opt(old_ckpt_name)\n",
    "            else:\n",
    "                self.checkpoints[old_metric] = old_ckpt_name\n",
    "\n",
    "        self.checkpoints = OrderedDict(\n",
    "            sorted(self.checkpoints.items(), key=lambda x: x[0])\n",
    "        )\n",
    "\n",
    "    def save_net_opt(self, params, opt_state, ckpt_name: str):\n",
    "        ckpt_dir = self.log_dir / f\"{ckpt_name}\"\n",
    "        ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        model_ckpt_dir = ckpt_dir / \"model\"\n",
    "        opt_ckpt_dir = ckpt_dir / \"opt\"\n",
    "        # create dirs for storing states of model and optimizer\n",
    "        model_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        opt_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # save model and optimizer states\n",
    "        save_checkpoint(params, model_ckpt_dir)\n",
    "        save_checkpoint(opt_state, opt_ckpt_dir)\n",
    "\n",
    "    def delete_net_opt(self, ckpt_name: str):\n",
    "        ckpt_dir = self.log_dir / f\"{ckpt_name}\"\n",
    "        shutil.rmtree(ckpt_dir)\n",
    "\n",
    "    # # deprecated\n",
    "    # def load_net_opt(self, ckpt_name: str):\n",
    "    #     ckpt_dir = self.log_dir / f\"{ckpt_name}\"\n",
    "    #     model_ckpt_dir = ckpt_dir / \"model\"\n",
    "    #     opt_ckpt_dir = ckpt_dir / \"opt\"\n",
    "    #     # load model and optimizer states\n",
    "    #     params = load_checkpoint(model_ckpt_dir)\n",
    "    #     opt_state = load_checkpoint(opt_ckpt_dir)\n",
    "    #     return params, opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.data import TabularDataModule\n",
    "from relax.module import PredictiveTrainingModule\n",
    "\n",
    "data_configs = {\n",
    "    \"data_dir\": \"assets/data/s_adult.csv\",\n",
    "    \"data_name\": \"adult\",\n",
    "    \"batch_size\": 256,\n",
    "    'sample_frac': 0.1,\n",
    "    \"continous_cols\": [\n",
    "        \"age\",\n",
    "        \"hours_per_week\"\n",
    "    ],\n",
    "    \"discret_cols\": [\n",
    "        \"workclass\",\n",
    "        \"education\",\n",
    "        \"marital_status\",\n",
    "        \"occupation\",\n",
    "        \"race\",\n",
    "        \"gender\"\n",
    "    ],\n",
    "}\n",
    "# dm = \n",
    "m_configs = {\n",
    "    \"sizes\": [50, 10, 50],\n",
    "    \"dropout_rate\": 0.3,\n",
    "    'lr': 0.003,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "key = hk.PRNGSequence(42)\n",
    "ckpt_manager = CheckpointManager(\n",
    "    log_dir='log', \n",
    "    monitor_metrics='train/train_loss_1',\n",
    "    max_n_checkpoints=3\n",
    ")\n",
    "dm = TabularDataModule(data_configs)\n",
    "module = PredictiveTrainingModule(m_configs)\n",
    "params, opt_state = module.init_net_opt(dm, next(key))\n",
    "logs = {'train/train_loss_1': 0.1}\n",
    "ckpt_manager.update_checkpoints(params, opt_state, logs, epochs=1)\n",
    "logs = {'train/train_loss_1': 0.2}\n",
    "ckpt_manager.update_checkpoints(params, opt_state, logs, epochs=2)\n",
    "logs = {'train/train_loss_1': 0.15}\n",
    "ckpt_manager.update_checkpoints(params, opt_state, logs, epochs=3)\n",
    "logs = {'train/train_loss_1': 0.05}\n",
    "ckpt_manager.update_checkpoints(params, opt_state, logs, epochs=4)\n",
    "logs = {'train/train_loss_1': 0.14}\n",
    "ckpt_manager.update_checkpoints(params, opt_state, logs, epochs=5)\n",
    "assert ckpt_manager.n_checkpoints == len(ckpt_manager.checkpoints)\n",
    "assert ckpt_manager.checkpoints.popitem(last=True)[0] == 0.14\n",
    "\n",
    "shutil.rmtree(Path('log/epoch=1'), ignore_errors=True)\n",
    "shutil.rmtree(Path('log/epoch=2'), ignore_errors=True)\n",
    "shutil.rmtree(Path('log/epoch=3'), ignore_errors=True)\n",
    "shutil.rmtree(Path('log/epoch=4'), ignore_errors=True)\n",
    "shutil.rmtree(Path('log/epoch=5'), ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev2",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
