{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "> Evaluating and benchmarking the quality of CF explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | include: false\n",
    "# | default_exp evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from nbdev import show_doc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from __future__ import annotations\n",
    "from cfnet.import_essentials import *\n",
    "from cfnet.train import train_model, TrainingConfigs\n",
    "from cfnet.datasets import TabularDataModule\n",
    "from cfnet.utils import accuracy, proximity\n",
    "from cfnet.methods.base import BaseCFModule, BaseParametricCFModule, BasePredFnCFModule\n",
    "from cfnet.methods.counternet import CounterNet\n",
    "from copy import deepcopy\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "@dataclass\n",
    "class Explanation:\n",
    "    \"\"\"Generated CF Explanations class.\"\"\"\n",
    "    cf_name: str  # cf method's name\n",
    "    data_module: TabularDataModule  # data module\n",
    "    cfs: jnp.DeviceArray  # generated cf explanation of `X`\n",
    "    total_time: float  # total runtime\n",
    "    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray]  # predict function\n",
    "    dataset_name: str = str()  # dataset name\n",
    "    X: jnp.ndarray = None  # input\n",
    "    y: jnp.ndarray = None  # label\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.data_module:\n",
    "            if self.dataset_name == str():\n",
    "                self.dataset_name = self.data_module.data_name\n",
    "            test_X, label = self.data_module.test_dataset[:]\n",
    "            if self.X is None:\n",
    "                self.X = test_X\n",
    "            if self.y is None:\n",
    "                self.y = label\n",
    "\n",
    "CFExplanationResults = Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"Explanation\" class=\"doc_header\"><code>class</code> <code>Explanation</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>Explanation</code>(**`cf_name`**:`str`, **`data_module`**:`TabularDataModule`, **`cfs`**:`DeviceArray`, **`total_time`**:`float`, **`pred_fn`**:`DeviceArray]`, **`dataset_name`**:`str`=*`''`*, **`X`**:`ndarray`=*`None`*, **`y`**:`ndarray`=*`None`*)\n",
       "\n",
       "Generated CF Explanations class."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments to `Explanation`:\n",
    "\n",
    "* `cf_name`: cf method's name\n",
    "* `dataset_name`: dataset name\n",
    "* `X`: input\n",
    "* `y`: label\n",
    "* `cfs`: generated cf explanation of `X`\n",
    "* `total_time`: total runtime\n",
    "* `pred_fn`: predict function with only one input argument, \n",
    "and output a label (i.e., its format is `y=pred_fn(x)`).\n",
    "* `data_module`: data module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating CF Explanation Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _validate_configs(\n",
    "    cf_module: BaseCFModule,\n",
    "    datamodule: TabularDataModule,\n",
    "    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray] = None,\n",
    "    t_configs=None\n",
    "):\n",
    "    if (pred_fn is None) and (not isinstance(cf_module, BasePredFnCFModule)):\n",
    "        warnings.warn(f\"`{type(cf_module).__name__}` is not a subclass of `BasePredFnCFModule`.\"\n",
    "            \"This might cause problems as you set `pred_fn=None`, \"\n",
    "            f\"which infers that `{type(cf_module).__name__}` has an attribute `pred_fn`.\")\n",
    "\n",
    "\n",
    "def _prepare_module(\n",
    "    cf_module: BaseCFModule,\n",
    "    datamodule: TabularDataModule\n",
    "):\n",
    "    cf_module.update_cat_info(datamodule)\n",
    "    return cf_module\n",
    "\n",
    "def _train_parametric_module(\n",
    "    cf_module: BaseCFModule,\n",
    "    datamodule: TabularDataModule,\n",
    "    t_configs=None\n",
    "):\n",
    "    print(f'{type(cf_module).__name__} contains parametric models. '\n",
    "        'Starts training before generating explanations...')\n",
    "    cf_module.train(datamodule, t_configs)\n",
    "    return cf_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _check_aux_pred_fn_args(pred_fn_args: dict | None):\n",
    "    if pred_fn_args is None:\n",
    "        return dict()\n",
    "    elif isinstance(pred_fn_args, dict):\n",
    "        return pred_fn_args\n",
    "    else:\n",
    "        raise ValueError(f'`pred_fn_args` should be a `dict`,',\n",
    "            f'but got `{type(pred_fn_args).__name__}`')\n",
    "\n",
    "class _AuxPredFn:\n",
    "    def __init__(self, pred_fn, pred_fn_args: dict | None):\n",
    "        self.pred_fn = pred_fn\n",
    "        self.fn_args = deepcopy(_check_aux_pred_fn_args(pred_fn_args))\n",
    "\n",
    "    def __call__(self, x: jnp.DeviceArray) -> jnp.DeviceArray:\n",
    "        return self.pred_fn(x, **self.fn_args)\n",
    "\n",
    "\n",
    "def _check_pred_fn(\n",
    "    pred_fn: callable | None, \n",
    "    cf_module: BaseCFModule\n",
    ") -> callable:\n",
    "    if pred_fn is None:\n",
    "        try:\n",
    "            pred_fn = cf_module.pred_fn\n",
    "        except AttributeError:\n",
    "            raise AttributeError(\n",
    "                    \"`generate_cf_explanations` is incorrectly configured.\"\n",
    "                    f\"It is supposed to be `pred_fn != None`,\"\n",
    "                    f\"or `{type(cf_module).__name__}` has attribute `pred_fn`.\"\n",
    "                    f\"However, we got `pred_fn={pred_fn}`, \"\n",
    "                    f\"and `{type(cf_module).__name__}` has not attribute `pred_fn`.\"\n",
    "            )\n",
    "    elif isinstance(cf_module, BasePredFnCFModule):\n",
    "        # override pred_fn if `cf_module` has `pred_fn`\n",
    "        pred_fn = cf_module.pred_fn\n",
    "    return pred_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_cf_explanations(\n",
    "    cf_module: BaseCFModule, # CF Explanation Module\n",
    "    datamodule: TabularDataModule, # Data Module\n",
    "    pred_fn: callable = None, # Predictive function\n",
    "    t_configs: TrainingConfigs = None, # training configs for `BaseParametricCFModule`\n",
    "    pred_fn_args: dict = None # auxiliary arguments for `pred_fn` \n",
    ") -> Explanation:\n",
    "    \"\"\"Generate CF explanations.\"\"\"\n",
    "\n",
    "    _validate_configs(cf_module, datamodule, pred_fn, t_configs)\n",
    "    cf_module = _prepare_module(cf_module, datamodule)\n",
    "\n",
    "    if isinstance(cf_module, BaseParametricCFModule):\n",
    "        cf_module = _train_parametric_module(\n",
    "            cf_module, datamodule, t_configs=t_configs\n",
    "        )\n",
    "    X, _ = datamodule.test_dataset[:]\n",
    "    \n",
    "    # create `pred_fn` which only takes `x` as an input\n",
    "    if pred_fn is not None:\n",
    "        pred_fn = _AuxPredFn(pred_fn, pred_fn_args=pred_fn_args)\n",
    "\n",
    "    # generate cfs\n",
    "    current_time = time.time()\n",
    "    cfs = cf_module.generate_cfs(X, pred_fn=pred_fn)\n",
    "    total_time = time.time() - current_time\n",
    "    # check pred_fn\n",
    "    pred_fn = _check_pred_fn(pred_fn, cf_module)\n",
    "\n",
    "    return Explanation(\n",
    "        cf_name=cf_module.name,\n",
    "        data_module=datamodule,\n",
    "        cfs=cfs,\n",
    "        total_time=total_time,\n",
    "        pred_fn=pred_fn,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pred_fn` in `generate_cf_explanations` is a model's prediction function. \n",
    "The general format is `y = pred_fn(x, **pred_fn_args)`. \n",
    "If `pred_fn` is not parameterized by other variables (except input `x`), \n",
    "then `pred_fn_args` is set to `None`, which is the default setting.\n",
    "Otherwise, you should pass these argument as a `dict`.\n",
    "\n",
    "For example, we have a simple linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_pred_fn(x: jnp.DeviceArray, params: jnp.DeviceArray):\n",
    "    return x @ params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass `linear_pred_fn` to `generate_cf_explanations`, \n",
    "we can either create an auxiliary function of `linear_pred_fn`,\n",
    "or pass `params` into `pred_fn_args`.\n",
    "\n",
    "Assuming we now have the input `x` and `params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "x = jax.random.normal(random.PRNGKey(0), shape=(5, 10)) # input\n",
    "params = jnp.ones((10, 1)) # params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create an auxillary function (Not recommended)\n",
    "\n",
    "```python\n",
    "aux_linear_pred_fn = lambda x: linear_pred_fn(x, params)\n",
    "explanations = generate_cf_explanations(\n",
    "    cf_module, datamodule, aux_linear_pred_fn\n",
    ")\n",
    "```\n",
    "\n",
    "This approach could work, but if `params` is changed, \n",
    "`explanations.pred_fn` might not work as expected.\n",
    "\n",
    "2. Pass `params` into `pred_fn_args`\n",
    "\n",
    "```python\n",
    "explanations = generate_cf_explanations(\n",
    "    cf_module, datamodule, linear_pred_fn, \n",
    "    pred_fn_args=dict(params=params)\n",
    ")\n",
    "```\n",
    "\n",
    "This is a recommended approach as we will deepcopy `params` inside `generate_cf_explanations`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pred_fn` in `explanations` only takes `x: jnp.DeviceArray` as an input.\n",
    "For example, to make predictions, we use\n",
    "\n",
    "```python\n",
    "y = explanations.pred_fn(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "@deprecated(removed_in='0.1.0', deprecated_in='0.0.9')\n",
    "def generate_cf_results_local_exp(\n",
    "    cf_module: BaseCFModule,\n",
    "    dm: TabularDataModule,\n",
    "    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray],\n",
    ") -> CFExplanationResults:\n",
    "    return generate_cf_explanations(cf_module, dm, pred_fn=pred_fn)\n",
    "\n",
    "\n",
    "@deprecated(removed_in='0.1.0', deprecated_in='0.0.9')\n",
    "def generate_cf_results_cfnet(\n",
    "    cf_module: CounterNet,\n",
    "    dm: TabularDataModule,\n",
    "    params: hk.Params = None,  # params of `cf_module`\n",
    "    rng_key: random.PRNGKey = None,\n",
    ") -> CFExplanationResults:\n",
    "    return generate_cf_explanations(cf_module, dm, pred_fn=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseEvalMetrics(ABC):\n",
    "    def __call__(self, cf_explanations: Explanation) -> Any:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _compute_acc(\n",
    "    input: jnp.DeviceArray, # input dim: [N, k]\n",
    "    label: jnp.DeviceArray, # label dim: [N] or [N, 1]\n",
    "    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray]\n",
    ") -> float:\n",
    "    y_pred = pred_fn(input).reshape(-1, 1).round()\n",
    "    label = label.reshape(-1, 1)\n",
    "    return accuracy(y_pred, label).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PredictiveAccuracy(BaseEvalMetrics):\n",
    "    def __call__(self, cf_explanations: Explanation) -> float:\n",
    "        X, y = cf_explanations.data_module.test_dataset[:]\n",
    "        return _compute_acc(X, y, cf_explanations.pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _compute_val(\n",
    "    input: jnp.DeviceArray, # input dim: [N, k]\n",
    "    cfs: jnp.DeviceArray, # cfs dim: [N, k]\n",
    "    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray]\n",
    "):\n",
    "    y_pred = pred_fn(input).reshape(-1, 1).round()\n",
    "    y_prime = jnp.ones_like(y_pred) - y_pred\n",
    "    cf_y = pred_fn(cfs).reshape(-1, 1).round()\n",
    "    return accuracy(y_prime, cf_y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Validity(BaseEvalMetrics):\n",
    "    def __call__(self, cf_explanations: Explanation) -> float:\n",
    "        X, _ = cf_explanations.data_module.test_dataset[:]\n",
    "        return _compute_val(\n",
    "            X, cf_explanations.cfs, cf_explanations.pred_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Proximity(BaseEvalMetrics):\n",
    "    def __call__(self, cf_explanations: Explanation) -> float:\n",
    "        X, _ = cf_explanations.data_module.test_dataset[:]\n",
    "        return proximity(X, cf_explanations.cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _compute_spar(\n",
    "    input: jnp.DeviceArray,\n",
    "    cfs: jnp.DeviceArray,\n",
    "    cat_idx: int\n",
    "):\n",
    "    # calculate sparsity\n",
    "    cat_sparsity = proximity(input[:, cat_idx:], cfs[:, cat_idx:]) / 2\n",
    "    cont_sparsity = jnp.linalg.norm(\n",
    "        jnp.abs(input[:, :cat_idx] - cfs[:, :cat_idx]), ord=0, axis=1\n",
    "    ).mean()\n",
    "    return cont_sparsity + cat_sparsity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sparsity(BaseEvalMetrics):\n",
    "    def __call__(self, cf_explanations: Explanation) -> float:\n",
    "        X, _ = cf_explanations.data_module.test_dataset[:]\n",
    "        return _compute_spar(X, cf_explanations.cfs, cf_explanations.cat_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _compute_manifold_dist(\n",
    "    input: jnp.DeviceArray,\n",
    "    cfs: jnp.DeviceArray,\n",
    "    n_neighbors: int = 1,\n",
    "    p: int = 2\n",
    "):\n",
    "    knn = NearestNeighbors(n_neighbors=n_neighbors, p=p)\n",
    "    knn.fit(input)\n",
    "    nearest_dist, nearest_points = knn.kneighbors(cfs, 1, return_distance=True)\n",
    "    return jnp.mean(nearest_dist).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ManifoldDist(BaseEvalMetrics):\n",
    "    def __init__(self, n_neighbors: int = 1, p: int = 2):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, cf_explanations: Explanation) -> float:\n",
    "        X, _ = cf_explanations.data_module.test_dataset[:]\n",
    "        return _compute_manifold_dist(\n",
    "            X, cf_explanations.cfs, self.n_neighbors, self.p\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Runtime(BaseEvalMetrics):\n",
    "    def __call__(self, cf_explanations: Explanation) -> float:\n",
    "        return cf_explanations.total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "pred_fn_test = lambda x: jnp.clip(x + 0.5, 0., 1)\n",
    "\n",
    "x_1 = jnp.array([[0.1], [0.34], [0.4], [-0.2], [0.7]])\n",
    "cf_1 = jnp.array([[-0.2], [0.4], [-0.1], [0.7], [-0.1]])\n",
    "y_1 = jnp.array([1., 0., 1., 1., 1.])\n",
    "x_2 = jnp.array([[-0.5], [0.34], [0.4], [-0.2], [0.7]])\n",
    "cf_2 = jnp.array([[0.2], [0.4], [-0.1], [0.7], [-0.1]])\n",
    "y_2 = jnp.array([[0.], [1.], [1.], [1.], [1.]])\n",
    "\n",
    "_acc_1 = _compute_acc(x_1, y_1, pred_fn_test)\n",
    "_acc_2 = _compute_acc(x_2, y_2, pred_fn_test)\n",
    "_val_1 = _compute_val(x_1, cf_1, pred_fn_test)\n",
    "_val_2 = _compute_val(x_2, cf_2, pred_fn_test)\n",
    "\n",
    "assert jnp.isclose(_acc_1, 0.6)\n",
    "assert jnp.isclose(_acc_2, 0.8)\n",
    "assert jnp.isclose(_val_1, 0.8)\n",
    "assert jnp.isclose(_val_2, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _create_second_order_cfs(cf_results: CFExplanationResults, threshold: float = 2.0):\n",
    "    X, y = cf_results.data_module.test_dataset[:]\n",
    "    cfs = cf_results.cfs\n",
    "    scaler = cf_results.data_module.normalizer\n",
    "    cat_idx = cf_results.data_module.cat_idx\n",
    "\n",
    "    # get normalized threshold = threshold / (max - min)\n",
    "    data_range = scaler.data_range_\n",
    "    thredshold_normed = threshold / data_range\n",
    "\n",
    "    # select continous features\n",
    "    x_cont = X[:, :cat_idx]\n",
    "    cf_cont = cfs[:, :cat_idx]\n",
    "    # calculate the diff between x and c\n",
    "    cont_diff = jnp.abs(x_cont - cf_cont) <= thredshold_normed\n",
    "    # new cfs\n",
    "    cfs_cont_hat = jnp.where(cont_diff, x_cont, cf_cont)\n",
    "\n",
    "    cfs_hat = jnp.concatenate((cfs_cont_hat, cfs[:, cat_idx:]), axis=-1)\n",
    "    return cfs_hat\n",
    "\n",
    "\n",
    "def compute_so_validity(cf_results: CFExplanationResults, threshold: float = 2.0):\n",
    "    cfs_hat = _create_second_order_cfs(cf_results, threshold)\n",
    "    cf_results_so = deepcopy(cf_results)\n",
    "    cf_results_so.cfs = cfs_hat\n",
    "    compute_validity = Validity()\n",
    "    return compute_validity(cf_results_so)\n",
    "\n",
    "\n",
    "def compute_so_proximity(cf_results: CFExplanationResults, threshold: float = 2.0):\n",
    "    cfs_hat = _create_second_order_cfs(cf_results, threshold)\n",
    "    cf_results_so = deepcopy(cf_results)\n",
    "    cf_results_so.cfs = cfs_hat\n",
    "    compute_proximity = Proximity()\n",
    "    return compute_proximity(cf_results_so)\n",
    "\n",
    "\n",
    "def compute_so_sparsity(cf_results: CFExplanationResults, threshold: float = 2.0):\n",
    "    cfs_hat = _create_second_order_cfs(cf_results, threshold)\n",
    "    cf_results_so = deepcopy(cf_results)\n",
    "    cf_results_so.cfs = cfs_hat\n",
    "    compute_sparsity = Sparsity()\n",
    "    return compute_sparsity(cf_results_so)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "METRICS = dict(\n",
    "    acc=PredictiveAccuracy(),\n",
    "    accuracy=PredictiveAccuracy(),\n",
    "    validity=Validity(),\n",
    "    proximity=Proximity(),\n",
    "    runtime=Runtime(),\n",
    "    manifold_dist=ManifoldDist(),\n",
    "    # validity=compute_so_validity,\n",
    "    # so_proximity=compute_so_proximity,\n",
    "    # so_sparsity=compute_so_sparsity\n",
    ")\n",
    "\n",
    "DEFAULT_METRICS = [\"acc\", \"validity\", \"proximity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _get_metric(metric: str | callable, cf_exp: Explanation):\n",
    "    if isinstance(metric, str):\n",
    "        try:\n",
    "            res = METRICS[metric](cf_exp)\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"'{metric}' is not supported. Must be one of {METRICS.keys()}\")\n",
    "    elif callable(metric):\n",
    "        res = metric(cf_exp)\n",
    "    else:\n",
    "        raise ValueError(f\"{type(metric).__name__} is not supported as a metric.\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def evaluate_cfs(\n",
    "    cf_exp: Explanation, # CF Explanations\n",
    "    metrics: Iterable[Union[str, Callable]] = None, # A list of Metrics. Can be `str` or a subclass of `BaseEvalMetrics`\n",
    "    return_dict: bool = True, # return a dictionary or not (default: True)\n",
    "    return_df: bool = False # return a pandas Dataframe or not (default: False)\n",
    "):\n",
    "    cf_name = cf_exp.cf_name\n",
    "    data_name = cf_exp.data_module.data_name\n",
    "    result_dict = { (data_name, cf_name): dict() }\n",
    "\n",
    "    if metrics is None:\n",
    "        metrics = DEFAULT_METRICS\n",
    "\n",
    "    for metric in metrics:\n",
    "        result_dict[(data_name, cf_name)][metric] = _get_metric(metric, cf_exp)\n",
    "    result_df = pd.DataFrame.from_dict(result_dict, orient=\"index\")\n",
    "    \n",
    "    if return_dict and return_df:\n",
    "        return (result_dict, result_df)\n",
    "    elif return_dict or return_df:\n",
    "        return result_df if return_df else result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def benchmark_cfs(\n",
    "    cf_results_list: Iterable[CFExplanationResults],\n",
    "    metrics: Optional[Iterable[str]] = None,\n",
    "):\n",
    "    dfs = [\n",
    "        evaluate_cfs(\n",
    "            cf_exp=cf_results, metrics=metrics, return_dict=False, return_df=True\n",
    "        )\n",
    "        for cf_results in cf_results_list\n",
    "    ]\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate a CF Explanation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfnet.module import PredictiveTrainingModule\n",
    "from cfnet.utils import load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = load_json('assets/configs/data_configs/adult.json')\n",
    "m_configs = configs['mlp_configs']\n",
    "data_configs = configs['data_configs']\n",
    "data_configs['sample_frac'] = 0.1\n",
    "\n",
    "t_configs = {\n",
    "    'n_epochs': 10,\n",
    "    'monitor_metrics': 'val/val_loss',\n",
    "    'seed': 42,\n",
    "    \"batch_size\": 256\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 10/10 [00:00<00:00, 25.01batch/s, train/train_loss_1=0.0618]\n"
     ]
    }
   ],
   "source": [
    "training_module = PredictiveTrainingModule(m_configs)\n",
    "dm = TabularDataModule(data_configs)\n",
    "\n",
    "params, opt_state = train_model(\n",
    "    training_module, \n",
    "    dm, \n",
    "    t_configs\n",
    ")\n",
    "pred_fn = lambda x, params, prng_key: \\\n",
    "    training_module.forward(params, prng_key, x, is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start to benchmark different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfnet.methods import VanillaCF, CounterNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate CF explanations for `VanillaCF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:15<00:00, 65.06it/s]\n"
     ]
    }
   ],
   "source": [
    "#| slow\n",
    "vanillacf = VanillaCF(dict(n_steps=1000, lr=0.001))\n",
    "vanillacf_exp = generate_cf_explanations(\n",
    "    vanillacf, dm, pred_fn,\n",
    "    pred_fn_args=dict(params=params, prng_key=random.PRNGKey(0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| slow\n",
    "assert vanillacf_exp.cf_name == vanillacf.name\n",
    "assert vanillacf_exp.dataset_name == dm.data_name\n",
    "assert vanillacf_exp.X.shape == vanillacf_exp.cfs.shape\n",
    "assert vanillacf_exp.pred_fn(vanillacf_exp.X).shape == vanillacf_exp.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate CF explanations for `CounterNet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CounterNet contains parametric models. Starts training before generating explanations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/code/cfnet/cfnet/_ckpt_manager.py:48: UserWarning: `monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\n",
      "  \"`monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\"\n",
      "Epoch 99: 100%|██████████| 20/20 [00:00<00:00, 63.62batch/s, train/train_loss_1=0.0319, train/train_loss_2=0.000102, train/train_loss_3=0.103]  \n"
     ]
    }
   ],
   "source": [
    "#| slow\n",
    "counternet = CounterNet()\n",
    "counternet_exp = generate_cf_explanations(counternet, dm, pred_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `CounterNet` contains a predictive module, so we set `pred_fn=None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| slow\n",
    "assert counternet_exp.cf_name == counternet.name\n",
    "assert counternet_exp.dataset_name == dm.data_name\n",
    "assert counternet_exp.X.shape == counternet_exp.cfs.shape\n",
    "assert counternet_exp.pred_fn(counternet_exp.X).shape == counternet_exp.y.shape\n",
    "assert counternet_exp.pred_fn == counternet.pred_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `cf_module` is a subclass of `BasePredFnCFModule` (e.g., `CounterNet`),\n",
    "the `pred_fn` in `Explanation` will be set to `cf_module.pred_fn`,\n",
    "and the `pred_fn` argument passed `generate_cf_explanations` will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CounterNet contains parametric models. Starts training before generating explanations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 20/20 [00:00<00:00, 175.02batch/s, train/train_loss_1=0.068, train/train_loss_2=4.58e-5, train/train_loss_3=0.0856]   \n"
     ]
    }
   ],
   "source": [
    "#| slow\n",
    "counternet_exp_1 = generate_cf_explanations(counternet, dm, pred_fn=pred_fn)\n",
    "assert counternet_exp_1.pred_fn != pred_fn\n",
    "assert counternet_exp_1.pred_fn == counternet.pred_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute metrics for benchmarking different CF explanation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>validity</th>\n",
       "      <th>proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <th>VanillaCF</th>\n",
       "      <td>0.807395</td>\n",
       "      <td>0.178725</td>\n",
       "      <td>8.370252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      acc  validity proximity\n",
       "adult VanillaCF  0.807395  0.178725  8.370252"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| slow\n",
    "evaluate_cfs(vanillacf_exp, return_df=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>validity</th>\n",
       "      <th>proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <th>CounterNet</th>\n",
       "      <td>0.821029</td>\n",
       "      <td>0.999631</td>\n",
       "      <td>5.764046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       acc  validity proximity\n",
       "adult CounterNet  0.821029  0.999631  5.764046"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| slow\n",
    "evaluate_cfs(counternet_exp, return_df=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| slow\n",
    "benchmark_cfs([vanillacf_res, counternet_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| slow\n",
    "benchmark_cfs([vanillacf_res, counternet_res])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f450d1f6173d6d96822a65433f2c9ed0b856da53e162bc0666cdf9645c72e1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
