# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04b_ckpt_manager.ipynb.

# %% auto 0
__all__ = ['save_checkpoint', 'load_checkpoint', 'CheckpointManager']

# %% ../nbs/04b_ckpt_manager.ipynb 3
from .import_essentials import *
from collections import OrderedDict

# %% ../nbs/04b_ckpt_manager.ipynb 4
# https://github.com/deepmind/dm-haiku/issues/18#issuecomment-981814403
def save_checkpoint(state, ckpt_dir: Path):
    with open(os.path.join(ckpt_dir, "params.npy"), "wb") as f:
        for x in jax.tree_leaves(state):
            np.save(f, x, allow_pickle=False)

    tree_struct = jax.tree_map(lambda t: 0, state)
    with open(os.path.join(ckpt_dir, "tree.pkl"), "wb") as f:
        pickle.dump(tree_struct, f)

def load_checkpoint(ckpt_dir: Path):
    with open(os.path.join(ckpt_dir, "tree.pkl"), "rb") as f:
        tree_struct = pickle.load(f)

    leaves, treedef = jax.tree_flatten(tree_struct)
    with open(os.path.join(ckpt_dir, "params.npy"), "rb") as f:
        flat_state = [np.load(f) for _ in leaves]

    return jax.tree_unflatten(treedef, flat_state)

# %% ../nbs/04b_ckpt_manager.ipynb 5
class CheckpointManager:
    def __init__(self,
                 log_dir: Union[Path, str],
                 monitor_metrics: Optional[str],
                 max_n_checkpoints: int = 3):
        self.log_dir = Path(log_dir)
        self.monitor_metrics = monitor_metrics
        self.max_n_checkpoints = max_n_checkpoints
        self.checkpoints = OrderedDict()
        self.n_checkpoints = 0
        if self.monitor_metrics is None:
            warnings.warn("`monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.")

    # update checkpoints based on monitor_metrics
    def update_checkpoints(self,
                           params: hk.Params,
                           opt_state: optax.OptState,
                           epoch_logs: Dict[str, float],
                           epochs: int,
                           steps: Optional[int] = None):
        if self.monitor_metrics is None:
            return
        metric = float(epoch_logs[self.monitor_metrics])
        if steps:
            ckpt_name = f'epoch={epochs}_step={steps}'
        else:
            ckpt_name = f'epoch={epochs}'

        if self.n_checkpoints < self.max_n_checkpoints:
            self.checkpoints[metric] = ckpt_name
            self.save_net_opt(params, opt_state, ckpt_name)
            self.n_checkpoints += 1
        else:
            old_metric, old_ckpt_name = self.checkpoints.popitem(last=True)
            if metric < old_metric:
                self.checkpoints[metric] = ckpt_name
                self.save_net_opt(params, opt_state, ckpt_name)
                self.delete_net_opt(old_ckpt_name)
            else:
                self.checkpoints[old_metric] = old_ckpt_name

        self.checkpoints = OrderedDict(
            sorted(self.checkpoints.items(), key=lambda x: x[0]))

    def save_net_opt(self, params, opt_state, ckpt_name: str):
        ckpt_dir = self.log_dir / f'{ckpt_name}'
        ckpt_dir.mkdir(parents=True, exist_ok=True)
        model_ckpt_dir = ckpt_dir / 'model'
        opt_ckpt_dir = ckpt_dir / 'opt'
        # create dirs for storing states of model and optimizer
        model_ckpt_dir.mkdir(parents=True, exist_ok=True)
        opt_ckpt_dir.mkdir(parents=True, exist_ok=True)
        # save model and optimizer states
        save_checkpoint(params, model_ckpt_dir)
        save_checkpoint(opt_state, opt_ckpt_dir)

    def delete_net_opt(self, ckpt_name: str):
        ckpt_dir = self.log_dir / f'{ckpt_name}'
        shutil.rmtree(ckpt_dir)

    # deprecated
    def load_net_opt(self, ckpt_name: str):
        ckpt_dir = self.log_dir / f'{ckpt_name}'
        model_ckpt_dir = ckpt_dir / 'model'
        opt_ckpt_dir = ckpt_dir / 'opt'
        # load model and optimizer states
        params = load_checkpoint(model_ckpt_dir)
        opt_state = load_checkpoint(opt_ckpt_dir)
        return params, opt_state
