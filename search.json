[
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "[source]\n\nrelax.utils.validate_configs (configs, config_cls)\n\nreturn a valid configuration object.\n\nParameters:\n\nconfigs (dict | pydantic.main.BaseModel) – A configuration of the model/dataset.\nconfig_cls (&lt;class 'pydantic.main.BaseModel'&gt;) – The desired configuration class.\n\n\n\nReturns:\n    (&lt;class 'pydantic.main.BaseModel'&gt;)\n\nWe define a configuration object (which inherent BaseParser) to manage training/model/data configurations. validate_configs ensures to return the designated configuration object.\nFor example, we define a configuration object LearningConfigs:\n\nclass LearningConfigs(BaseParser):\n    lr: float\n\nA configuration can be LearningConfigs, or the raw data in dictionary.\n\nconfigs_dict = dict(lr=0.01)\n\nvalidate_configs will return a designated configuration object.\n\nconfigs = validate_configs(configs_dict, LearningConfigs)\nassert type(configs) == LearningConfigs\nassert configs.lr == configs_dict['lr']",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#configurations",
    "href": "utils.html#configurations",
    "title": "Utils",
    "section": "",
    "text": "[source]\n\nrelax.utils.validate_configs (configs, config_cls)\n\nreturn a valid configuration object.\n\nParameters:\n\nconfigs (dict | pydantic.main.BaseModel) – A configuration of the model/dataset.\nconfig_cls (&lt;class 'pydantic.main.BaseModel'&gt;) – The desired configuration class.\n\n\n\nReturns:\n    (&lt;class 'pydantic.main.BaseModel'&gt;)\n\nWe define a configuration object (which inherent BaseParser) to manage training/model/data configurations. validate_configs ensures to return the designated configuration object.\nFor example, we define a configuration object LearningConfigs:\n\nclass LearningConfigs(BaseParser):\n    lr: float\n\nA configuration can be LearningConfigs, or the raw data in dictionary.\n\nconfigs_dict = dict(lr=0.01)\n\nvalidate_configs will return a designated configuration object.\n\nconfigs = validate_configs(configs_dict, LearningConfigs)\nassert type(configs) == LearningConfigs\nassert configs.lr == configs_dict['lr']",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#serialization",
    "href": "utils.html#serialization",
    "title": "Utils",
    "section": "Serialization",
    "text": "Serialization\n\nrelax.utils.save_pytree\n\n[source]\n\nrelax.utils.save_pytree (pytree, saved_dir)\n\nSave a pytree to a directory.\nThe pytree will be stored under a directory with two files:\n\n{saved_dir}/data.npy: This file stores the flattened leaves.\n{saved_dir}/treedef.json: This file stores the pytree structure and the information on whether the leave is an array or not.\n\nFor example, a pytree\n\npytree = {\n    'a': np.random.randn(5, 1),\n    'b': 1,\n    'c': {\n        \n        'd': True,\n        'e': \"Hello\",\n        'f': np.array([\"a\", \"b\", \"c\"])\n    }\n}\n\nwill be stored as\n\nrelax.utils.load_pytree\n\n[source]\n\nrelax.utils.load_pytree (saved_dir)\n\nLoad a pytree from a saved directory.\n\n# Store a dictionary to disk\npytree = {\n    'a': np.random.randn(100, 1),\n    'b': 1,\n    'c': {\n        'd': True,\n        'e': \"Hello\",\n        'f': np.array([\"a\", \"b\", \"c\"])\n    }\n}\nos.makedirs('tmp', exist_ok=True)\nsave_pytree(pytree, 'tmp')\npytree_loaded = load_pytree('tmp')\nassert np.allclose(pytree['a'], pytree_loaded['a'])\nassert pytree['a'].dtype == pytree_loaded['a'].dtype\nassert pytree['b'] == pytree_loaded['b']\nassert pytree['c']['d'] == pytree_loaded['c']['d']\nassert pytree['c']['e'] == pytree_loaded['c']['e']\nassert np.all(pytree['c']['f'] == pytree_loaded['c']['f'])\n\n\n# Store a list to disk\npytree = [\n    np.random.randn(100, 1),\n    {'a': 1, 'b': np.array([1, 2, 3])},\n    1,\n    [1, 2, 3],\n    \"good\"\n]\nsave_pytree(pytree, 'tmp')\npytree_loaded = load_pytree('tmp')\n\nassert np.allclose(pytree[0], pytree_loaded[0])\nassert pytree[0].dtype == pytree_loaded[0].dtype\nassert pytree[1]['a'] == pytree_loaded[1]['a']\nassert np.all(pytree[1]['b'] == pytree_loaded[1]['b'])\nassert pytree[2] == pytree_loaded[2]\nassert pytree[3] == pytree_loaded[3]\nassert isinstance(pytree_loaded[3], list)\nassert pytree[4] == pytree_loaded[4]",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#vectorization-utils",
    "href": "utils.html#vectorization-utils",
    "title": "Utils",
    "section": "Vectorization Utils",
    "text": "Vectorization Utils\n\nrelax.utils.auto_reshaping\n\n[source]\n\nrelax.utils.auto_reshaping (reshape_argname, reshape_output=True)\n\nDecorator to automatically reshape function’s input into (1, k), and out to input’s shape.\n\nParameters:\n\nreshape_argname (&lt;class 'str'&gt;) – The name of the argument to be reshaped.\nreshape_output (&lt;class 'bool'&gt;, default=True) – Whether to reshape the output. Useful to set False when returning multiple cfs.\n\n\nThis decorator ensures that the specified input argument and output of a function are in the same shape. This is particularly useful when using jax.vamp.\n\n@auto_reshaping('x')\ndef f_vmap(x): return x * jnp.ones((10,))\nassert vmap(f_vmap)(jnp.ones((10, 10))).shape == (10, 10)\n\n@auto_reshaping('x', reshape_output=False)\ndef f_vmap(x): return x * jnp.ones((10,))\nassert vmap(f_vmap)(jnp.ones((10, 10))).shape == (10, 1, 10)",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#gradient-utils",
    "href": "utils.html#gradient-utils",
    "title": "Utils",
    "section": "Gradient Utils",
    "text": "Gradient Utils\n\nrelax.utils.grad_update\n\n[source]\n\nrelax.utils.grad_update (grads, params, opt_state, opt)\n\n\nParameters:\n\ngrads – A pytree of gradients.\nparams – A pytree of parameters.\nopt_state (typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef('ArrayTree')], typing.Mapping[typing.Any, ForwardRef('ArrayTree')]])\nopt (&lt;class 'optax._src.base.GradientTransformation'&gt;)",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#functional-utils",
    "href": "utils.html#functional-utils",
    "title": "Utils",
    "section": "Functional Utils",
    "text": "Functional Utils\n\nrelax.utils.gumbel_softmax\n\n[source]\n\nrelax.utils.gumbel_softmax (key, logits, tau, axis=-1)\n\nThe Gumbel softmax function.\n\nParameters:\n\nkey (&lt;function PRNGKey at 0x7f248027e0e0&gt;) – Random key\nlogits (&lt;class 'jax.Array'&gt;) – Logits for each class. Shape (batch_size, num_classes)\ntau (&lt;class 'float'&gt;) – Temperature for the Gumbel softmax\naxis (int | tuple[int, ...], default=-1) – The axis or axes along which the gumbel softmax should be computed",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#helper-functions",
    "href": "utils.html#helper-functions",
    "title": "Utils",
    "section": "Helper functions",
    "text": "Helper functions\n\nrelax.utils.load_json\n\n[source]\n\nrelax.utils.load_json (f_name)\n\n\nParameters:\n\nf_name (&lt;class 'str'&gt;)\n\n\n\nReturns:\n    (typing.Dict[str, typing.Any]) – file name",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#config",
    "href": "utils.html#config",
    "title": "Utils",
    "section": "Config",
    "text": "Config\n\nrelax.utils.get_config\n\n[source]\n\nrelax.utils.get_config ()\n\n\nrelax.utils.set_config\n\n[source]\n\nrelax.utils.set_config (rng_reserve_size=None, global_seed=None, **kwargs)\n\nSets the global configurations.\n\nParameters:\n\nrng_reserve_size (&lt;class 'int'&gt;, default=None) – The number of random number generators to reserve.\nglobal_seed (&lt;class 'int'&gt;, default=None) – The global seed for random number generators.\nkwargs (VAR_KEYWORD)\n\n\n\nReturns:\n    (None)\n\n\n# Generic Test cases\nset_config()\nassert get_config().rng_reserve_size == 1 and get_config().global_seed == 42\nset_config(rng_reserve_size=100)\nassert get_config().rng_reserve_size == 100\nset_config(global_seed=1234)\nassert get_config().global_seed == 1234\nset_config(rng_reserve_size=2, global_seed=234)\nassert get_config().rng_reserve_size == 2 and get_config().global_seed == 234\nset_config()\nassert get_config().rng_reserve_size == 2 and get_config().global_seed == 234\nset_config(invalid_key = 80)\nassert get_config().rng_reserve_size == 2 and get_config().global_seed == 234",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Utils"
    ]
  },
  {
    "objectID": "tutorials/getting_started.html",
    "href": "tutorials/getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "This tutorial aims at introducing basics about ReLax, and how to use ReLax to generate counterfactual (or recourse) explanations for jax-based implementations of ML models.\nIn particular, we will cover the following things in this tutorial:",
    "crumbs": [
      "Overview",
      "Getting started"
    ]
  },
  {
    "objectID": "tutorials/getting_started.html#preparation",
    "href": "tutorials/getting_started.html#preparation",
    "title": "Getting started",
    "section": "Preparation",
    "text": "Preparation\nWe assume that you have already installed ReLax. If not, follow the steps in this installation tutorial, or just enter pip install jax-relax.\nWe also want to import some libraries for this tutorial.\n\nimport jax",
    "crumbs": [
      "Overview",
      "Getting started"
    ]
  },
  {
    "objectID": "tutorials/getting_started.html#load-dataset-with-datamodule",
    "href": "tutorials/getting_started.html#load-dataset-with-datamodule",
    "title": "Getting started",
    "section": "Load Dataset with DataModule",
    "text": "Load Dataset with DataModule\nDataModule is a python class which modularizes tabular dataset loading. DataModule loads a .csv file from the directory by specifying the following attributes:\n\ndata_name is the name of your dataset.\ndata_dir should contain the relative path of the directory where your dataset is located.\ncontinous_cols specifies a list of feature names representing all the continuous/numeric features in our dataset.\ndiscret_cols specifies a list of feature names representing all discrete features in our dataset. By default, all discrete features are converted via one-hot encoding for training purposes.\nimutable_cols specifies a list of feature names that represent immutable features that we do not wish to change in the generated recourse.\n\n\nfrom relax.data_module import DataModuleConfig, DataModule, load_data\n\nFor example, to load the adult dataset, we can specify the DataModuleConfig as\n\ndata_config = DataModuleConfig(\n    # The name of this dataset is \"adult\"\n    data_name=\"adult\",\n    # The data file is located in `../assets/data/s_adult.csv`.\n    data_dir=\"../assets/adult/data/data.csv\",\n    # Contains 2 features with continuous variables\n    continous_cols=[\"age\",\"hours_per_week\"],\n    # Contains 6 features with categorical (discrete) variables\n    discret_cols=[\"workclass\",\"education\",\"marital_status\",\"occupation\",\"race\",\"gender\"],\n    # Contains 2 features that we do not wish to change\n    imutable_cols=[\"race\", \"gender\"]\n)\n\nWe can then pass data_configs to the DataModule.\n\ndatamodule = DataModule.from_config(data_config)\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nAlternatively, we can also specify this config via a dictionary.\n\n# This approach is equivalent to using `DataModuleConfig`\ndata_config_dict = {\n    \"data_name\": \"adult\",\n    \"data_dir\": \"../assets/adult/data/data.csv\",\n    \"continous_cols\": [\"age\",\"hours_per_week\"],\n    \"discret_cols\": [\"workclass\",\"education\",\"marital_status\",\"occupation\",\"race\",\"gender\"],\n    \"imutable_cols\": [\"race\",\"gender\"]\n}\ndatamodule = DataModule.from_config(data_config)\n\nFor datasets supported by ReLax, we can simply call load_data:\n\n# This is equivalent to specifying configs for `DataModule`\ndatamodule = load_data('adult')\n\nFor more usage of loading datasets in ReLax, check out the data module documentation.",
    "crumbs": [
      "Overview",
      "Getting started"
    ]
  },
  {
    "objectID": "tutorials/getting_started.html#train-the-classifier",
    "href": "tutorials/getting_started.html#train-the-classifier",
    "title": "Getting started",
    "section": "Train the Classifier",
    "text": "Train the Classifier\nFor the purpose of exposing full functionality of the framework, we will train the model using the built-in functions in ReLax, which uses haiku for building neural network blocks. However, the recourse algorithms in ReLax can generate explanations for all jax-based framework (e.g., flax, haiku, vanilla jax).\n\n\n\n\n\n\nWarning\n\n\n\nThe recourse algorithms in ReLax currently only supports binary classification. The output of the classifier must be a probability score (bounded by [0, 1]). Future support for multi-class classification is planned.\n\n\nTraining a classifier using the built-in functions in ReLax is very simple. We will first specify the classifier. The classifier is called PredictiveTrainingModule, which specifies the model structure, and the optimization procedure (e.g., it specifies the loss function for optimizing the model). Next, we use train_model to train the model on TabularDataModule.\n\nDefine the Model\n\nfrom relax.ml_model import MLModuleConfig, MLModule\n\nDefining MLModule is similar to defining MLModuleConfig. We first specify the configurator as MLModuleConfig, and pass this configurator to MLModuleConfig.\n\nmodel_config = MLModuleConfig(\n    lr=0.01, # Learning rate\n    sizes=[50, 10, 50], # The sizes of the hidden layers\n    dropout_rate=0.3 # Dropout rate\n)\n\n# specify the predictive model\nmodule = MLModule(model_config)\n\n\n\nTrain the Model\nTo train MLModule for the entire dataset (specified in DataModule), we can simply call MLModule.train:\n\nmodule.train(datamodule, batch_size=128, epochs=5)\n\nEpoch 1/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.7807 - loss: 0.4597\nEpoch 2/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8135 - loss: 0.3945\nEpoch 3/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8202 - loss: 0.3769\nEpoch 4/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8205 - loss: 0.3784\nEpoch 5/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8167 - loss: 0.3817\n\n\n&lt;relax.ml_model.MLModule&gt;\n\n\n\n\nMake Predictions\nWe can directly use module.pred_fn for making the predictions.\n\npred_fn = module.pred_fn",
    "crumbs": [
      "Overview",
      "Getting started"
    ]
  },
  {
    "objectID": "tutorials/getting_started.html#generate-counterfactual-explanations",
    "href": "tutorials/getting_started.html#generate-counterfactual-explanations",
    "title": "Getting started",
    "section": "Generate Counterfactual Explanations",
    "text": "Generate Counterfactual Explanations\nNow, it is time to use ReLax to generate counterfactual explanations (or recourse).\n\nfrom relax.methods import VanillaCF, VanillaCFConfig\n\nWe use VanillaCF (a very popular recourse generation algorithm) as an example for this tutorial. Defining VanillaCF is similar to defining TabularDataModule and PredictiveTrainingModule.\n\ncf_config = VanillaCFConfig(\n    n_steps=1000, # Number of steps\n    lr=0.001 # Learning rate\n)\ncf_exp = VanillaCF(cf_config)\n\nGenerate counterfactual examples.\n\nfrom relax.explain import generate_cf_explanations\n\n\ncf_results = generate_cf_explanations(\n    cf_exp, datamodule, pred_fn, \n)",
    "crumbs": [
      "Overview",
      "Getting started"
    ]
  },
  {
    "objectID": "tutorials/getting_started.html#benchmark-the-counterfactual-method",
    "href": "tutorials/getting_started.html#benchmark-the-counterfactual-method",
    "title": "Getting started",
    "section": "Benchmark the Counterfactual Method",
    "text": "Benchmark the Counterfactual Method\nAfter we obtain the counterfactual results, we can use benchmark_cfs to evaluate the accuracy, validity, and proximity of the counterfactual example.\n\nfrom relax.evaluate import benchmark_cfs\n\n\nbenchmark_cfs([cf_results])\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\nadult\nVanillaCF\n0.828261\n0.814963\n4.79361",
    "crumbs": [
      "Overview",
      "Getting started"
    ]
  },
  {
    "objectID": "tutorials/install.html",
    "href": "tutorials/install.html",
    "title": "Installation",
    "section": "",
    "text": "Tip\n\n\n\nTL;DR: For most users, install ReLax via the Python Package Index:\npip install -U jax-relax",
    "crumbs": [
      "Overview",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/install.html#installing-relax",
    "href": "tutorials/install.html#installing-relax",
    "title": "Installation",
    "section": "Installing ReLax",
    "text": "Installing ReLax\nThis section assumes that you are an end-user of ReLax, e.g., you only want to use this library for your own developement without modifying the ReLax codebase.\nReLax is built on top of JAX. You should also check the official installation guide from the Jax team.\n\nPrerequisite: Set up your python environment\nWe suggest to create a new environment when using ReLax.\nIf you are using conda, you can create a new environment by:\nconda create -n relax python=3.9 -y\nconda activate relax\n\n\nRunning on CPU\nIf you only need to run relax on CPU, you can simply install via pip or clone the GitHub project.\nInstallation via PyPI:\npip install --upgrade pip\npip install --upgrade jax-relax\n\n\nEditable Install\nIf you wish to install ReLax from source, run:\n# Clone the ReLax Repository\ngit clone https://github.com/BirkhoffG/jax-relax.git\ncd jax-relax\n# Clone the submodule\ngit submodule update --init --recursive\npip install -e .\n\n\nRunning on GPU or TPU\nIf you wish to run ReLax on GPU or TPU, please first install this library via pypi. Next, you should install the right GPU or TPU version of JAX by following steps in the install guidelines.\nFor example, if you want to install a GPU version, you should run\npip install jax-relax\nNext, install the GPU version of jax:\npip install -U \"jax[cuda12]\"\n\n\n\n\n\n\nWarning\n\n\n\nWe do not run continuous integration (CI) for GPU and TPU environments. If you encounter issues when running on GPU/TPU, please report to us.",
    "crumbs": [
      "Overview",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/install.html#if-you-are-a-contributor-of-relax",
    "href": "tutorials/install.html#if-you-are-a-contributor-of-relax",
    "title": "Installation",
    "section": "If you are a Contributor of ReLax…",
    "text": "If you are a Contributor of ReLax…\nYou will need to install additional packages if you want to fork and make changes to the library.\nYou should install the required packages via\npip install \"jax-relax[dev]\"\nNext, follow the nbdev installation for further instructions. Check out the contribution guidance for more details.",
    "crumbs": [
      "Overview",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/methods.html",
    "href": "tutorials/methods.html",
    "title": "ReLax as a Recourse Library",
    "section": "",
    "text": "ReLax contains implementations of various recourse methods, which are decoupled from the rest of ReLax library. We give users flexibility on how to use ReLax:\nIn this tutorial, we uncover the possibility of the second option by using recourse methods under relax.methods for debugging, diagnosing, interpreting your JAX models.",
    "crumbs": [
      "Overview",
      "Tutorials",
      "`ReLax` as a Recourse Library"
    ]
  },
  {
    "objectID": "tutorials/methods.html#types-of-recourse-methods",
    "href": "tutorials/methods.html#types-of-recourse-methods",
    "title": "ReLax as a Recourse Library",
    "section": "Types of Recourse Methods",
    "text": "Types of Recourse Methods\n\nNon-parametric methods: These methods do not rely on any learned parameters. They generate counterfactuals solely based on the model’s predictions and gradients. Examples in ReLax include VanillaCF, DiverseCF and GrowingSphere . These methods inherit from CFModule.\nSemi-parametric methods: These methods learn some parameters to aid in counterfactual generation, but do not learn a full counterfactual generation model. Examples in ReLax include ProtoCF, CCHVAE and CLUE. These methods inherit from ParametricCFModule.\nParametric methods: These methods learn a full parametric model for counterfactual generation. The model is trained to generate counterfactuals that fool the model. Examples in ReLax include CounterNet and VAECF. These methods inherit from ParametricCFModule.\n\n\n\n\nMethod Type\nLearned Parameters\nTraining Required\nExample Methods\n\n\n\n\nNon-parametric\nNone\nNo\nVanillaCF, DiverseCF, GrowingSphere\n\n\nSemi-parametric\nSome (θ)\nModest amount\nProtoCF, CCHVAE, CLUE\n\n\nParametric\nFull generator model (φ)\nSubstantial amount\nCounterNet, VAECF",
    "crumbs": [
      "Overview",
      "Tutorials",
      "`ReLax` as a Recourse Library"
    ]
  },
  {
    "objectID": "tutorials/methods.html#basic-usages",
    "href": "tutorials/methods.html#basic-usages",
    "title": "ReLax as a Recourse Library",
    "section": "Basic Usages",
    "text": "Basic Usages\nAt a high level, you can use the implemented methods in ReLax to generate one recourse explanation via three lines of code:\nfrom relax.methods import VanillaCF\n\nvcf = VanillaCF()\n# x is one data point. Shape: `(K)` or `(1, K)`\ncf = vcf.generate_cf(x, pred_fn=pred_fn)\nOr generate a batch of recourse explanation via the jax.vmap primitive:\n...\nimport functools as ft\n\nvcf_gen_fn = ft.partial(vcf.generate_cf, pred_fn=pred_fn)\n# xs is a batched data. Shape: `(N, K)`\ncfs = jax.vmap(vcf_gen_fn)(xs)\nTo use parametric and semi-parametric methods, you can first train the model by calling ParametricCF.train, and then generate recourse explanations. Here is an example of using ReLax for CCHVAE.\nfrom relax.methods import CCHVAE\n\ncchvae = CCHVAE()\ncchvae.train(train_data) # Train CVAE before generation\ncf = cchvae.generate_cf(x, pred_fn=pred_fn) \nOr generate a batch of recourse explanation via the jax.vmap primitive:\n...\nimport functools as ft\n\ncchvae_gen_fn = ft.partial(cchvae.generate_cf, pred_fn=pred_fn)\ncfs = jax.vmap(cchvae_gen_fn)(xs) # Generate counterfactuals",
    "crumbs": [
      "Overview",
      "Tutorials",
      "`ReLax` as a Recourse Library"
    ]
  },
  {
    "objectID": "tutorials/methods.html#config-recourse-methods",
    "href": "tutorials/methods.html#config-recourse-methods",
    "title": "ReLax as a Recourse Library",
    "section": "Config Recourse Methods",
    "text": "Config Recourse Methods\nEach recourse method in ReLax has an associated Config class that defines the set of supported configuration parameters. To configure a method, import and instantiate its Config class and pass it as the config parameter.\nFor example, to configure VanillaCF:\nfrom relax.methods import VanillaCF \nfrom relax.methods.vanilla import VanillaCFConfig\n\nconfig = VanillaCFConfig(\n  n_steps=100,\n  lr=0.1,\n  lambda_=0.1\n)\n\nvcf = VanillaCF(config)\nEach Config class inherits from a BaseConfig that defines common options like n_steps. Method-specific parameters are defined on the individual Config classes.\nSee the documentation for each recourse method for details on its supported configuration parameters. The Config class for a method can be imported from relax.methods.[method_name].\nAlternatively, we can also specify this config via a dictionary.\nfrom relax.methods import VanillaCF\n\nconfig = {\n  \"n_steps\": 10,  \n  \"lambda_\": 0.1,\n  \"lr\": 0.1   \n}\n\nvcf = VanillaCF(config)\nThis config dictionary is passed to VanillaCF’s init method, which will set the specified parameters. Now our VanillaCF instance is configured to:\n\nNumber 10 optimization steps (n_steps=100)\nUse 0.1 validity regularization for counterfactuals (lambda_=0.1)\nUse a learning rate of 0.1 for optimization (lr=0.1)",
    "crumbs": [
      "Overview",
      "Tutorials",
      "`ReLax` as a Recourse Library"
    ]
  },
  {
    "objectID": "tutorials/methods.html#implement-your-own-recourse-methods",
    "href": "tutorials/methods.html#implement-your-own-recourse-methods",
    "title": "ReLax as a Recourse Library",
    "section": "Implement your Own Recourse Methods",
    "text": "Implement your Own Recourse Methods\nYou can easily implement your own recourse methods and leverage jax_relax to scale the recourse generation. In this section, we implement a mock “Recourse Method”, which add random perturbations to the input x.\n\nfrom relax.methods.base import CFModule, BaseConfig\nfrom relax.utils import auto_reshaping, validate_configs\nfrom relax.import_essentials import *\nimport relax\n\nFirst, we define a configuration class for the random counterfactual module. This class inherits from the BaseConfig class.\n\nclass RandomCFConfig(BaseConfig):\n    max_perturb: float = 0.2 # Max perturbation allowed for RandomCF\n\nNext, we define the random counterfactual module. This class inhertis from CFModule class. Importantly, you should override the CFModule.generate_cf and implement your CF generation procedure for each input (i.e., shape=(k,), where k is the number of features).\n\nclass RandomCF(CFModule):\n\n    def __init__(\n        self,\n        config: dict | RandomCFConfig = None,\n        name: str = None,\n    ):\n        if config is None:\n            config = RandomCFConfig()\n        config = validate_configs(config, RandomCFConfig)\n        name = \"RandomCF\" if name is None else name\n        super().__init__(config, name=name)\n\n    @auto_reshaping('x')\n    def generate_cf(\n        self,\n        x: Array, # Input data point\n        pred_fn: Callable = None, # Prediction function\n        y_target: Array = None,   # Target label\n        rng_key: jrand.PRNGKey = None, # Random key\n        **kwargs,\n    ) -&gt; Array:\n        # Generate random perturbations in the range of [-max_perturb, max_perturb].\n        x_cf = x + jrand.uniform(rng_key, x.shape, \n                                 minval=-self.config.max_perturb, \n                                 maxval=self.config.max_perturb)\n        return x_cf\n\nFinally, you can easily use jax-relax to generate recourse explanations at scale.\n\nrand_cf = RandomCF()\nexps = relax.generate_cf_explanations(\n    rand_cf, relax.load_data('dummy'), relax.load_ml_module('dummy').pred_fn, \n)\nrelax.benchmark_cfs([exps])\n\n|                       |   acc |   validity |   proximity |\n|:----------------------|------:|-----------:|------------:|\n| ('dummy', 'RandomCF') | 0.983 |  0.0599999 |    0.997049 |",
    "crumbs": [
      "Overview",
      "Tutorials",
      "`ReLax` as a Recourse Library"
    ]
  },
  {
    "objectID": "methods/proto.html",
    "href": "methods/proto.html",
    "title": "Proto CF",
    "section": "",
    "text": "relax.methods.proto.ProtoCFConfig\n\n[source]\n\nclass relax.methods.proto.ProtoCFConfig (n_steps=100, lr=0.01, c=1, beta=0.1, gamma=0.1, theta=0.1, n_samples=128, validity_fn=‘KLDivergence’, enc_sizes=[64, 32, 16], dec_sizes=[16, 32, 64], opt_name=‘adam’, ae_lr=0.001, ae_loss=‘mse’)\n\nConfigurator of ProtoCF.\n\nParameters:\n\nn_steps (int, default=100)\nlr (float, default=0.01)\nc (float, default=1) – The weight for validity loss.\nbeta (float, default=0.1) – The weight for l1_norm in the cost function, where cost = beta * l1_norm + l2_norm.\ngamma (float, default=0.1) – The weight for Autoencoder loss.\ntheta (float, default=0.1) – The weight for prototype loss.\nn_samples (int, default=128) – Number of samples for prototype.\nvalidity_fn (str, default=KLDivergence)\nenc_sizes (List[int], default=[64, 32, 16]) – List of hidden layers of Encoder.\ndec_sizes (List[int], default=[16, 32, 64]) – List of hidden layers of Decoder.\nopt_name (str, default=adam) – Optimizer name of AutoEncoder.\nae_lr (float, default=0.001) – Learning rate of AutoEncoder.\nae_loss (str, default=mse) – Loss function name of AutoEncoder.\n\n\n\nrelax.methods.proto.ProtoCF\n\n[source]\n\nclass relax.methods.proto.ProtoCF (config=None, ae=None, name=None)\n\nBase class for parametric counterfactual modules.\n\nMethods\n[source]\n\nset_apply_constraints_fn (apply_constraints_fn)\n\n[source]\n\nset_compute_reg_loss_fn (compute_reg_loss_fn)\n\n[source]\n\napply_constraints (*args, **kwargs)\n\n[source]\n\ncompute_reg_loss (*args, **kwargs)\n\n[source]\n\nsave (path)\n\n[source]\n\nload_from_path (path)\n\n[source]\n\nbefore_generate_cf (*args, **kwargs)\n\n\ngenerate_cf (*args, **kwargs)\n\n\n\ndm = load_data('oulad')\nmodel = load_ml_module('oulad')\nxs_train, ys_train = dm['train']\nxs_test, ys_test = dm['test']\n\n/home/birk/code/jax-relax/relax/data_module.py:234: UserWarning: Passing `config` will have no effect.\n  warnings.warn(\"Passing `config` will have no effect.\")\n\n\n\npcf = ProtoCF()\npcf.set_apply_constraints_fn(dm.apply_constraints)\npcf.train(dm, epochs=5)\n\nEpoch 1/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 4s 10ms/step - loss: 0.1207   \nEpoch 2/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 0.0418      \nEpoch 3/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 0.0373      \nEpoch 4/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 0.0341      \nEpoch 5/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 0.0324    \n\n\n&lt;__main__.ProtoCF&gt;\n\n\n\npartial_gen = partial(pcf.generate_cf, pred_fn=model.pred_fn)\ncfs = vmap(partial_gen)(xs_test)\n\nprint(\"Validity: \", keras.metrics.binary_accuracy(\n    (1 - model.pred_fn(xs_test)).round(),\n    model.pred_fn(cfs)\n).mean())\n\n\n\n\nValidity:  0.95471835",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "Proto CF"
    ]
  },
  {
    "objectID": "methods/dice.html",
    "href": "methods/dice.html",
    "title": "Diverse CF",
    "section": "",
    "text": "[source]\n\nrelax.methods.dice.dpp_style_vmap (cfs)\n\n\n# From the original dice implementation\n# https://github.com/interpretml/DiCE/blob/a772c8d4fcd88d1cab7f2e02b0bcc045dc0e2eab/dice_ml/explainer_interfaces/dice_pytorch.py#L222-L227\ndef dpp_style_torch(cfs: torch.Tensor):\n    compute_dist = lambda x, y: torch.abs(x-y).sum()\n\n    total_CFs = len(cfs)\n    det_entries = torch.ones((total_CFs, total_CFs))\n    for i in range(total_CFs):\n        for j in range(total_CFs):\n            det_entries[(i,j)] = 1.0/(1.0 + compute_dist(cfs[i], cfs[j]))\n            if i == j:\n                det_entries[(i,j)] += 1e-8\n    return torch.det(det_entries)\n\n\ndef jax2torch(x: Array):\n    return torch.from_numpy(x.__array__())\n\n\ncfs = jrand.normal(jrand.PRNGKey(0), (100, 100))\ncfs_tensor = jax2torch(cfs)\nassert np.allclose(\n    dpp_style_torch(cfs_tensor).numpy(),\n    dpp_style_vmap(cfs)\n)\n\n/tmp/ipykernel_11637/3412149913.py:2: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n  return torch.from_numpy(x.__array__())\n\n\nOur jax-based implementation is ~500X faster than DiCE’s pytorch implementation.\n\ntorch_res = dpp_style_torch(cfs_tensor)\n\n318 ms ± 4.24 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n\n\n\njax_res = dpp_style_vmap(cfs)\n\n571 µs ± 44.4 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "Diverse CF"
    ]
  },
  {
    "objectID": "methods/dice.html#util-functions",
    "href": "methods/dice.html#util-functions",
    "title": "Diverse CF",
    "section": "",
    "text": "[source]\n\nrelax.methods.dice.dpp_style_vmap (cfs)\n\n\n# From the original dice implementation\n# https://github.com/interpretml/DiCE/blob/a772c8d4fcd88d1cab7f2e02b0bcc045dc0e2eab/dice_ml/explainer_interfaces/dice_pytorch.py#L222-L227\ndef dpp_style_torch(cfs: torch.Tensor):\n    compute_dist = lambda x, y: torch.abs(x-y).sum()\n\n    total_CFs = len(cfs)\n    det_entries = torch.ones((total_CFs, total_CFs))\n    for i in range(total_CFs):\n        for j in range(total_CFs):\n            det_entries[(i,j)] = 1.0/(1.0 + compute_dist(cfs[i], cfs[j]))\n            if i == j:\n                det_entries[(i,j)] += 1e-8\n    return torch.det(det_entries)\n\n\ndef jax2torch(x: Array):\n    return torch.from_numpy(x.__array__())\n\n\ncfs = jrand.normal(jrand.PRNGKey(0), (100, 100))\ncfs_tensor = jax2torch(cfs)\nassert np.allclose(\n    dpp_style_torch(cfs_tensor).numpy(),\n    dpp_style_vmap(cfs)\n)\n\n/tmp/ipykernel_11637/3412149913.py:2: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n  return torch.from_numpy(x.__array__())\n\n\nOur jax-based implementation is ~500X faster than DiCE’s pytorch implementation.\n\ntorch_res = dpp_style_torch(cfs_tensor)\n\n318 ms ± 4.24 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n\n\n\njax_res = dpp_style_vmap(cfs)\n\n571 µs ± 44.4 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "Diverse CF"
    ]
  },
  {
    "objectID": "methods/dice.html#config",
    "href": "methods/dice.html#config",
    "title": "Diverse CF",
    "section": "Config",
    "text": "Config\n\nrelax.methods.dice.DiverseCFConfig\n\n[source]\n\nclass relax.methods.dice.DiverseCFConfig (n_cfs=5, n_steps=1000, lr=0.001, lambda_1=1.0, lambda_2=1.0, lambda_3=1.0, lambda_4=0.1, validity_fn=‘KLDivergence’, cost_fn=‘MeanSquaredError’, seed=42)\n\nBase class for all config classes.\n\nrelax.methods.dice.DiverseCF\n\n[source]\n\nclass relax.methods.dice.DiverseCF (config=None, name=None)\n\nBase class for all counterfactual modules.\n\nMethods\n[source]\n\nset_apply_constraints_fn (apply_constraints_fn)\n\n[source]\n\nset_compute_reg_loss_fn (compute_reg_loss_fn)\n\n[source]\n\napply_constraints (*args, **kwargs)\n\n[source]\n\ncompute_reg_loss (*args, **kwargs)\n\n[source]\n\nsave (path)\n\n[source]\n\nload_from_path (path)\n\n[source]\n\nbefore_generate_cf (*args, **kwargs)\n\n\ngenerate_cf (*args, **kwargs)\n\n\n\ndm = load_data('dummy')\nmodel = load_ml_module('dummy')\nxs_train, ys_train = dm['train']\nxs_test, ys_test = dm['test']\nx_shape = xs_test.shape\n\n/home/birk/code/jax-relax/relax/data_module.py:234: UserWarning: Passing `config` will have no effect.\n  warnings.warn(\"Passing `config` will have no effect.\")\n\n\n\ndcf = DiverseCF({'lambda_2': 4.0})\ndcf.set_apply_constraints_fn(dm.apply_constraints)\ndcf.set_compute_reg_loss_fn(dm.compute_reg_loss)\ncf = dcf.generate_cf(xs_test[0], model.pred_fn, rng_key=jrand.PRNGKey(0))\nassert cf.shape == (5, x_shape[1])\n\npartial_gen = partial(dcf.generate_cf, pred_fn=model.pred_fn)\ncfs = jax.vmap(partial_gen)(xs_test, rng_key=jrand.split(jrand.PRNGKey(0), xs_test.shape[0]))\n\nassert cfs.shape == (x_shape[0], 5, x_shape[1])\n\nprint(\"Validity: \", keras.metrics.binary_accuracy(\n    (1 - model.pred_fn(xs_test)).round(),\n    model.pred_fn(cfs[:, 0, :])\n).mean())\n\n\n\n\n\n\n\nValidity:  1.0\n\n\n\ndcf.save('tmp/dice/')\ndcf_1 = DiverseCF.load_from_path('tmp/dice/')\ndcf_1.set_apply_constraints_fn(dm.apply_constraints)\npartial_gen_1 = ft.partial(dcf_1.generate_cf, pred_fn=model.pred_fn)\ncfs_1 = jax.vmap(partial_gen_1)(xs_test, rng_key=jrand.split(jrand.PRNGKey(0), xs_test.shape[0]))\n\nassert jnp.allclose(cfs, cfs_1)\n\n\n\n\n\nexp = relax.generate_cf_explanations(\n    dcf, dm, model.pred_fn\n)\nrelax.benchmark_cfs([exp])\n\n\n\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\ndummy\nDiverseCF\n0.983\n1.0\n1.264459",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "Diverse CF"
    ]
  },
  {
    "objectID": "methods/vanilla.html",
    "href": "methods/vanilla.html",
    "title": "Vanilla CF",
    "section": "",
    "text": "relax.methods.vanilla.VanillaCFConfig\n\n[source]\n\nclass relax.methods.vanilla.VanillaCFConfig (n_steps=100, lr=0.1, lambda_=0.1, validity_fn=‘KLDivergence’)\n\nBase class for all config classes.\n\nrelax.methods.vanilla.VanillaCF\n\n[source]\n\nclass relax.methods.vanilla.VanillaCF (config=None, name=None)\n\nBase class for all counterfactual modules.\n\nMethods\n[source]\n\nset_apply_constraints_fn (apply_constraints_fn)\n\n[source]\n\nset_compute_reg_loss_fn (compute_reg_loss_fn)\n\n[source]\n\napply_constraints (*args, **kwargs)\n\n[source]\n\ncompute_reg_loss (*args, **kwargs)\n\n[source]\n\nsave (path)\n\n[source]\n\nload_from_path (path)\n\n[source]\n\nbefore_generate_cf (*args, **kwargs)\n\n\ngenerate_cf (*args, **kwargs)\n\n\n\ndm = load_data('dummy')\nmodel = load_ml_module('dummy')\nxs_train, ys_train = dm['train']\nxs_test, ys_test = dm['test']\n\n\nvcf = VanillaCF()\ncf = vcf.generate_cf(xs_test[0], model.pred_fn)\nassert cf.shape == xs_test[0].shape\n\npartial_gen = ft.partial(vcf.generate_cf, pred_fn=model.pred_fn)\ncfs = jax.vmap(partial_gen)(xs_test)\n\nprint(\"Validity: \", keras.metrics.binary_accuracy(\n    (1 - model.pred_fn(xs_test)).round(),\n    model.pred_fn(cfs)\n).mean())\n\n\n\n\n\n\n\nValidity:  0.99600005\n\n\n\ndef apply_constraint_fn(x, cf, hard=False):\n    return jax.lax.cond(\n        hard,\n        lambda: jnp.clip(cf, 0, 1),\n        lambda: cf,\n    )\n\nvcf.set_apply_constraints_fn(apply_constraint_fn)\ncfs = jax.vmap(partial_gen)(xs_test)\n\nprint(\"Validity: \", keras.metrics.binary_accuracy(\n    (1 - model.pred_fn(xs_test)).round(),\n    model.pred_fn(cfs)\n).mean())\nassert (cfs &gt;= 0).all() and (cfs &lt;= 1).all()\n\n\n\n\nValidity:  0.98800004\n\n\n\nvcf.save('tmp/vanillacf/')\nvcf_1 = VanillaCF.load_from_path('tmp/vanillacf/')\nvcf_1.set_apply_constraints_fn(apply_constraint_fn)\npartial_gen_1 = ft.partial(vcf_1.generate_cf, pred_fn=model.pred_fn)\ncfs_1 = jax.vmap(partial_gen_1)(xs_test)\n\nassert jnp.allclose(cfs, cfs_1)",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "Vanilla CF"
    ]
  },
  {
    "objectID": "methods/counternet.html",
    "href": "methods/counternet.html",
    "title": "CounterNet",
    "section": "",
    "text": "[source]\n\nclass relax.methods.counternet.CounterNetModel (enc_sizes, dec_sizes, exp_sizes, dropout_rate, name=None)\n\nCounterNet Model\n\nParameters:\n\nenc_sizes (&lt;class 'list'&gt;)\ndec_sizes (&lt;class 'list'&gt;)\nexp_sizes (&lt;class 'list'&gt;)\ndropout_rate (&lt;class 'float'&gt;)\nname (&lt;class 'str'&gt;, default=None) – Name of the module.",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "CounterNet"
    ]
  },
  {
    "objectID": "methods/counternet.html#counternet-model",
    "href": "methods/counternet.html#counternet-model",
    "title": "CounterNet",
    "section": "",
    "text": "[source]\n\nclass relax.methods.counternet.CounterNetModel (enc_sizes, dec_sizes, exp_sizes, dropout_rate, name=None)\n\nCounterNet Model\n\nParameters:\n\nenc_sizes (&lt;class 'list'&gt;)\ndec_sizes (&lt;class 'list'&gt;)\nexp_sizes (&lt;class 'list'&gt;)\ndropout_rate (&lt;class 'float'&gt;)\nname (&lt;class 'str'&gt;, default=None) – Name of the module.",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "CounterNet"
    ]
  },
  {
    "objectID": "methods/counternet.html#counternet-training-module",
    "href": "methods/counternet.html#counternet-training-module",
    "title": "CounterNet",
    "section": "CounterNet Training Module",
    "text": "CounterNet Training Module\nDefine the CounterNetTrainingModule for training CounterNetModel.\n\nrelax.methods.counternet.partition_trainable_params\n\n[source]\n\nrelax.methods.counternet.partition_trainable_params (params, trainable_name)\n\n\nrelax.methods.counternet.CounterNetTrainingModule\n\n[source]\n\nclass relax.methods.counternet.CounterNetTrainingModule\n\nHelper class that provides a standard way to create an ABC using inheritance.",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "CounterNet"
    ]
  },
  {
    "objectID": "methods/counternet.html#counternet-explanation-module",
    "href": "methods/counternet.html#counternet-explanation-module",
    "title": "CounterNet",
    "section": "CounterNet Explanation Module",
    "text": "CounterNet Explanation Module\n\n\n\nCounterNet architecture\n\n\nCounterNet consists of three objectives:\n\npredictive accuracy: the predictor network should output accurate predictions \\hat{y}_x;\ncounterfactual validity: CF examples x' produced by the CF generator network should be valid (e.g. \\hat{y}_{x} + \\hat{y}_{x'}=1);\nminimizing cost of change: minimal modifications should be required to change input instance x to CF example x'.\n\nThe objective function of CounterNet:\n\n\\operatorname*{argmin}_{\\mathbf{\\theta}} \\frac{1}{N}\\sum\\nolimits_{i=1}^{N}\n    \\bigg[\n    \\lambda_1 \\cdot \\! \\underbrace{\\left(y_i- \\hat{y}_{x_i}\\right)^2}_{\\text{Prediction Loss}\\ (\\mathcal{L}_1)} +\n    \\;\\lambda_2 \\cdot \\;\\; \\underbrace{\\left(\\hat{y}_{x_i}- \\left(1 - \\hat{y}_{x_i'}\\right)\\right)^2}_{\\text{Validity Loss}\\ (\\mathcal{L}_2)} \\,+\n    \\;\\lambda_3 \\cdot \\!\\! \\underbrace{\\left(x_i- x'_i\\right)^2}_{\\text{Cost of change Loss}\\ (\\mathcal{L}_3)}\n    \\bigg]\n\nCounterNet applies two-stage gradient updates to CounterNetModel for each training_step (see CounterNetTrainingModule).\n\nThe first gradient update optimizes for predictive accuracy: \\theta^{(1)} = \\theta^{(0)} - \\nabla_{\\theta^{(0)}} (\\lambda_1 \\cdot \\mathcal{L}_1).\nThe second gradient update optimizes for generating CF explanation: \\theta^{(2)}_g = \\theta^{(1)}_g - \\nabla_{\\theta^{(1)}_g} (\\mathcal \\lambda_2 \\cdot \\mathcal{L}_2 + \\lambda_3 \\cdot \\mathcal{L}_3)\n\nThe design choice of this optimizing procedure is made due to improved convergence of the model, and improved adversarial robustness of the predictor network. The CounterNet paper elaborates the design choices.\n\nrelax.methods.counternet.CounterNetConfig\n\n[source]\n\nclass relax.methods.counternet.CounterNetConfig (enc_sizes=[50, 10], pred_sizes=[10], exp_sizes=[50, 50], dropout_rate=0.3, lr=0.003, lambda_1=1.0, lambda_2=0.2, lambda_3=0.1)\n\nConfigurator of CounterNet.\n\nParameters:\n\nenc_sizes (List[int], default=[50, 10]) – Sequence of layer sizes for encoder network.\npred_sizes (List[int], default=[10]) – Sequence of layer sizes for predictor.\nexp_sizes (List[int], default=[50, 50]) – Sequence of layer sizes for CF generator.\ndropout_rate (float, default=0.3) – Dropout rate.\nlr (float, default=0.003) – Learning rate for training CounterNet.\nlambda_1 (float, default=1.0) – \\lambda_1 for balancing the prediction loss \\mathcal{L}_1.\nlambda_2 (float, default=0.2) – \\lambda_2 for balancing the prediction loss \\mathcal{L}_2.\nlambda_3 (float, default=0.1) – \\lambda_3 for balancing the prediction loss \\mathcal{L}_3.\n\n\n\nrelax.methods.counternet.CounterNet\n\n[source]\n\nclass relax.methods.counternet.CounterNet (config=None, cfnet_module=None, name=None)\n\nAPI for CounterNet Explanation Module.\n\nMethods\n[source]\n\nset_apply_constraints_fn (apply_constraints_fn)\n\n[source]\n\nset_compute_reg_loss_fn (compute_reg_loss_fn)\n\n[source]\n\napply_constraints (*args, **kwargs)\n\n[source]\n\ncompute_reg_loss (*args, **kwargs)\n\n[source]\n\nsave (path)\n\n[source]\n\nload_from_path (path)\n\n[source]\n\nbefore_generate_cf (*args, **kwargs)\n\n\ngenerate_cf (*args, **kwargs)\n\n\n\nBasic usage of CounterNet\nPrepare data:\n\nfrom relax.data_module import load_data\nfrom copy import deepcopy\nimport chex\n\n\ndm = load_data(\"adult\", data_configs=dict(sample_frac=0.1))\n\nDefine CounterNet:\n\ncounternet = CounterNet()\n\n\nassert isinstance(counternet, ParametricCFModule)\nassert isinstance(counternet, CFModule)\nassert isinstance(counternet, PredFnMixedin)\nassert hasattr(counternet, 'pred_fn')\nassert counternet.module is None\nassert counternet.is_trained is False\nassert not hasattr(counternet, 'params')\n\nTrain the model:\n\ncounternet.train(dm, epochs=1, batch_size=128)\nassert counternet.is_trained is True\nassert hasattr(counternet, 'params')\nparams = deepcopy(counternet.params)\n\nPredict labels\n\nxs, y = dm['test']\ny_pred = counternet.pred_fn(xs)\nassert y_pred.shape == (len(y), 2)\n\nGenerate a CF explanation for a given x.\n\ncf = counternet.generate_cf(xs[0])\nassert xs[0].shape == cf.shape\nassert cf.shape == (29,)\n\nGenerate CF explanations for given x.\n\ncfs = vmap(counternet.generate_cf)(xs)\nassert xs.shape == cfs.shape",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "CounterNet"
    ]
  },
  {
    "objectID": "methods/vaecf.html",
    "href": "methods/vaecf.html",
    "title": "VAECF",
    "section": "",
    "text": "relax.methods.vaecf.sample_latent\n[source]\n[source]\nA model grouping layers into an object with training/inference features.\nThere are three ways to instantiate a Model:",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "VAECF"
    ]
  },
  {
    "objectID": "methods/vaecf.html#with-the-functional-api",
    "href": "methods/vaecf.html#with-the-functional-api",
    "title": "VAECF",
    "section": "With the “Functional API”",
    "text": "With the “Functional API”\nYou start from Input, you chain layer calls to specify the model’s forward pass, and finally you create your model from inputs and outputs:\ninputs = keras.Input(shape=(37,))\nx = keras.layers.Dense(32, activation=\"relu\")(inputs)\noutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nNote: Only dicts, lists, and tuples of input tensors are supported. Nested inputs are not supported (e.g. lists of list or dicts of dict).\nA new Functional API model can also be created by using the intermediate tensors. This enables you to quickly extract sub-components of the model.\nExample:\ninputs = keras.Input(shape=(None, None, 3))\nprocessed = keras.layers.RandomCrop(width=128, height=128)(inputs)\nconv = keras.layers.Conv2D(filters=32, kernel_size=3)(processed)\npooling = keras.layers.GlobalAveragePooling2D()(conv)\nfeature = keras.layers.Dense(10)(pooling)\n\nfull_model = keras.Model(inputs, feature)\nbackbone = keras.Model(processed, conv)\nactivations = keras.Model(conv, feature)\nNote that the backbone and activations models are not created with keras.Input objects, but with the tensors that originate from keras.Input objects. Under the hood, the layers and weights will be shared across these models, so that user can train the full_model, and use backbone or activations to do feature extraction. The inputs and outputs of the model can be nested structures of tensors as well, and the created models are standard Functional API models that support all the existing APIs.",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "VAECF"
    ]
  },
  {
    "objectID": "methods/vaecf.html#by-subclassing-the-model-class",
    "href": "methods/vaecf.html#by-subclassing-the-model-class",
    "title": "VAECF",
    "section": "By subclassing the Model class",
    "text": "By subclassing the Model class\nIn that case, you should define your layers in __init__() and you should implement the model’s forward pass in call().\nclass MyModel(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n\n    def call(self, inputs):\n        x = self.dense1(inputs)\n        return self.dense2(x)\n\nmodel = MyModel()\nIf you subclass Model, you can optionally have a training argument (boolean) in call(), which you can use to specify a different behavior in training and inference:\nclass MyModel(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n        self.dropout = keras.layers.Dropout(0.5)\n\n    def call(self, inputs, training=False):\n        x = self.dense1(inputs)\n        x = self.dropout(x, training=training)\n        return self.dense2(x)\n\nmodel = MyModel()\nOnce the model is created, you can config the model with losses and metrics with model.compile(), train the model with model.fit(), or use the model to do prediction with model.predict().",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "VAECF"
    ]
  },
  {
    "objectID": "methods/vaecf.html#with-the-sequential-class",
    "href": "methods/vaecf.html#with-the-sequential-class",
    "title": "VAECF",
    "section": "With the Sequential class",
    "text": "With the Sequential class\nIn addition, keras.Sequential is a special case of model where the model is purely a stack of single-input, single-output layers.\nmodel = keras.Sequential([\n    keras.Input(shape=(None, None, 3)),\n    keras.layers.Conv2D(filters=32, kernel_size=3),\n])\n\nParameters:\n\nlayers (list[int])\nmc_samples (&lt;class 'int'&gt;, default=50) – pred_fn: Callable,\nkwargs (VAR_KEYWORD)\n\n\n\nrelax.methods.vaecf.VAECFConfig\n\n[source]\n\nclass relax.methods.vaecf.VAECFConfig (layers=[20, 16, 14, 12, 5], dropout_rate=0.1, opt_name=‘adam’, lr=0.001, mc_samples=50, validity_reg=42.0)\n\nConfigurator of VAECFModule.\n\nParameters:\n\nlayers (List[int], default=[20, 16, 14, 12, 5]) – Sequence of Encoder/Decoder layer sizes.\ndropout_rate (float, default=0.1) – Dropout rate.\nopt_name (str, default=adam) – Optimizer name.\nlr (float, default=0.001) – Learning rate.\nmc_samples (int, default=50) – Number of samples for mu.\nvalidity_reg (float, default=42.0) – Regularization for validity.\n\n\n\nrelax.methods.vaecf.VAECF\n\n[source]\n\nclass relax.methods.vaecf.VAECF (config=None, vae=None, name=‘VAECF’)\n\nBase class for parametric counterfactual modules.\n\nMethods\n[source]\n\nset_apply_constraints_fn (apply_constraints_fn)\n\n[source]\n\nset_compute_reg_loss_fn (compute_reg_loss_fn)\n\n[source]\n\napply_constraints (*args, **kwargs)\n\n[source]\n\ncompute_reg_loss (*args, **kwargs)\n\n[source]\n\nsave (path)\n\n[source]\n\nload_from_path (path)\n\n[source]\n\nbefore_generate_cf (*args, **kwargs)\n\n\ngenerate_cf (*args, **kwargs)\n\n\n\nfrom relax.data_module import load_data\nfrom relax.ml_model import load_ml_module\n\n\ndm = load_data('dummy')\npred_fn = load_ml_module('dummy').pred_fn\ntrain_xs, train_ys = dm['train']\ntest_xs, test_ys = dm['test']\n\n\nvaecf = VAECF()\n\n\nvaecf.train(dm, pred_fn, epochs=10)\n\nEpoch 1/10\n6/6 ━━━━━━━━━━━━━━━━━━━━ 15s 2s/step - loss: 10.8419\nEpoch 2/10\n6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step - loss: 7.9748\nEpoch 3/10\n6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step - loss: 6.1708\nEpoch 4/10\n6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - loss: 5.1577\nEpoch 5/10\n6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 38ms/step - loss: 4.3424\nEpoch 6/10\n6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - loss: 3.7982\nEpoch 7/10\n6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step - loss: 3.3180\nEpoch 8/10\n6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 38ms/step - loss: 2.9390\nEpoch 9/10\n6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step - loss: 2.6337\nEpoch 10/10\n6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step - loss: 2.3621\n\n\n&lt;__main__.VAECF&gt;\n\n\n\ncf = vaecf.generate_cf(test_xs[:1], pred_fn, rng_key=jrand.PRNGKey(42))\n\n\nn_tests = 100\npartial_gen = partial(vaecf.generate_cf, pred_fn=pred_fn)\ncfs = jax.vmap(partial_gen)(test_xs[:n_tests], rng_key=jrand.split(jrand.PRNGKey(0), n_tests))\n\nassert cfs.shape == test_xs[:100].shape\n\nprint(\"Validity: \", keras.metrics.binary_accuracy(\n    (1 - pred_fn(test_xs[:100])).round(),\n    pred_fn(cfs[:, :])\n).mean())\n\nValidity:  0.55",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "VAECF"
    ]
  },
  {
    "objectID": "02_ml_model.html",
    "href": "02_ml_model.html",
    "title": "jax-relax",
    "section": "",
    "text": "[source]\n\nclass relax.ml_model.MLP (sizes, output_size=2, dropout_rate=0.3, use_batch_norm=False, last_activation=‘softmax’, **kwargs)\n\nMLP model with multiple MLP blocks and a dense layer at the end.\n\n[source]\n\nclass relax.ml_model.MLPBlock (output_size, dropout_rate=0.3, use_batch_norm=False)\n\nMLP block with leaky relu activation and dropout/batchnorm.\n\n[source]\n\nclass relax.ml_model.MLModuleConfig (sizes=[64, 32, 16], output_size=2, dropout_rate=0.3, lr=0.001, opt_name=‘adam’, loss=‘sparse_categorical_crossentropy’, metrics=[‘accuracy’])\n\nConfigurator of MLModule.\n\nParameters:\n\nsizes (typing.List[int], default=[64, 32, 16]) – List of hidden layer sizes.\noutput_size (&lt;class 'int'&gt;, default=2) – The number of output classes.\ndropout_rate (&lt;class 'float'&gt;, default=0.3) – Dropout rate.\nlr (&lt;class 'float'&gt;, default=0.001) – Learning rate.\nopt_name (&lt;class 'str'&gt;, default=adam) – Optimizer name.\nloss (&lt;class 'str'&gt;, default=sparse_categorical_crossentropy) – Loss function name.\nmetrics (typing.List[str], default=[‘accuracy’]) – List of metrics names.\n\n\n\n[source]\n\nclass relax.ml_model.MLModule (config=None, model=None, name=None)\n\nBase class for all modules.\n\nMethods\n[source]\n\nis_trained ()\n\n[source]\n\ntrain (data, batch_size=128, epochs=10, **fit_kwargs)\n\nTrain the module.\n\n\nX, y = make_classification(\n    n_samples=5000, n_features=10, n_informative=5, random_state=42)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\nmodel = MLModule(\n    MLModuleConfig(sizes=[64, 32, 16],)\n)\nmodel.train((X_train, y_train), epochs=5)\nassert model.is_trained\n\nEpoch 1/5\n30/30 ━━━━━━━━━━━━━━━━━━━━ 2s 27ms/step - accuracy: 0.5601 - loss: 1.7022\nEpoch 2/5\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7016 - loss: 0.7342\nEpoch 3/5\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7190 - loss: 0.6272\nEpoch 4/5\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7522 - loss: 0.5503\nEpoch 5/5\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7732 - loss: 0.4973\n\n\n\nmodel.save('tmp/model')\n\n\nmodel_1 = MLModule.load_from_path('tmp/model')\nassert model_1.is_trained\nassert np.allclose(model_1.pred_fn(X_test), model.pred_fn(X_test))\n\n\n# models = []\n# for data in DEFAULT_DATA_CONFIGS.keys():\n#     rf_acc, model_acc = train_ml_model_and_rf(data)\n#     if rf_acc &gt; model_acc:\n#         models.append((data, rf_acc, model_acc))\n\n\n# data = \"dummy\"\n# dm = load_data(data)\n# file_path = f\"assets/{data}/model/model.keras\"\n# conf_path = f\"assets/{data}/model/config.json\"\n# ckpt_cb = keras.callbacks.ModelCheckpoint(\n#     filepath=file_path,\n#     monitor='val_accuracy',\n#     mode='max',\n#     save_best_only=True\n# )\n# train_xs, train_ys = dm['train']\n# test_xs, test_ys = dm['test']\n# model = MLModule({\n#     'sizes': [128, 64, 32, 16],\n#     'dropout_rate': 0.3, 'lr': 0.001,\n#     'opt_name': 'adamw'\n# }).train(\n#     dm, validation_data=dm['test'], callbacks=[ckpt_cb], batch_size=64, epochs=10\n# )\n# model.config.save(conf_path)\n# # Load the best model\n# model = MLModule.load_from_path(f\"assets/{data}/model\")\n\n\n# rf = RandomForestClassifier().fit(train_xs, train_ys.reshape(-1))\n# rf_acc = accuracy_score(test_ys, rf.predict(test_xs))\n# model_acc = accuracy_score(test_ys, model.pred_fn(test_xs).argmax(axis=1))\n\n# rf_acc, model_acc",
    "crumbs": [
      "Overview",
      "API Documentations",
      "ML Module"
    ]
  },
  {
    "objectID": "02_ml_model.html#ml-module",
    "href": "02_ml_model.html#ml-module",
    "title": "jax-relax",
    "section": "",
    "text": "[source]\n\nclass relax.ml_model.MLP (sizes, output_size=2, dropout_rate=0.3, use_batch_norm=False, last_activation=‘softmax’, **kwargs)\n\nMLP model with multiple MLP blocks and a dense layer at the end.\n\n[source]\n\nclass relax.ml_model.MLPBlock (output_size, dropout_rate=0.3, use_batch_norm=False)\n\nMLP block with leaky relu activation and dropout/batchnorm.\n\n[source]\n\nclass relax.ml_model.MLModuleConfig (sizes=[64, 32, 16], output_size=2, dropout_rate=0.3, lr=0.001, opt_name=‘adam’, loss=‘sparse_categorical_crossentropy’, metrics=[‘accuracy’])\n\nConfigurator of MLModule.\n\nParameters:\n\nsizes (typing.List[int], default=[64, 32, 16]) – List of hidden layer sizes.\noutput_size (&lt;class 'int'&gt;, default=2) – The number of output classes.\ndropout_rate (&lt;class 'float'&gt;, default=0.3) – Dropout rate.\nlr (&lt;class 'float'&gt;, default=0.001) – Learning rate.\nopt_name (&lt;class 'str'&gt;, default=adam) – Optimizer name.\nloss (&lt;class 'str'&gt;, default=sparse_categorical_crossentropy) – Loss function name.\nmetrics (typing.List[str], default=[‘accuracy’]) – List of metrics names.\n\n\n\n[source]\n\nclass relax.ml_model.MLModule (config=None, model=None, name=None)\n\nBase class for all modules.\n\nMethods\n[source]\n\nis_trained ()\n\n[source]\n\ntrain (data, batch_size=128, epochs=10, **fit_kwargs)\n\nTrain the module.\n\n\nX, y = make_classification(\n    n_samples=5000, n_features=10, n_informative=5, random_state=42)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\nmodel = MLModule(\n    MLModuleConfig(sizes=[64, 32, 16],)\n)\nmodel.train((X_train, y_train), epochs=5)\nassert model.is_trained\n\nEpoch 1/5\n30/30 ━━━━━━━━━━━━━━━━━━━━ 2s 27ms/step - accuracy: 0.5601 - loss: 1.7022\nEpoch 2/5\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7016 - loss: 0.7342\nEpoch 3/5\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7190 - loss: 0.6272\nEpoch 4/5\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7522 - loss: 0.5503\nEpoch 5/5\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7732 - loss: 0.4973\n\n\n\nmodel.save('tmp/model')\n\n\nmodel_1 = MLModule.load_from_path('tmp/model')\nassert model_1.is_trained\nassert np.allclose(model_1.pred_fn(X_test), model.pred_fn(X_test))\n\n\n# models = []\n# for data in DEFAULT_DATA_CONFIGS.keys():\n#     rf_acc, model_acc = train_ml_model_and_rf(data)\n#     if rf_acc &gt; model_acc:\n#         models.append((data, rf_acc, model_acc))\n\n\n# data = \"dummy\"\n# dm = load_data(data)\n# file_path = f\"assets/{data}/model/model.keras\"\n# conf_path = f\"assets/{data}/model/config.json\"\n# ckpt_cb = keras.callbacks.ModelCheckpoint(\n#     filepath=file_path,\n#     monitor='val_accuracy',\n#     mode='max',\n#     save_best_only=True\n# )\n# train_xs, train_ys = dm['train']\n# test_xs, test_ys = dm['test']\n# model = MLModule({\n#     'sizes': [128, 64, 32, 16],\n#     'dropout_rate': 0.3, 'lr': 0.001,\n#     'opt_name': 'adamw'\n# }).train(\n#     dm, validation_data=dm['test'], callbacks=[ckpt_cb], batch_size=64, epochs=10\n# )\n# model.config.save(conf_path)\n# # Load the best model\n# model = MLModule.load_from_path(f\"assets/{data}/model\")\n\n\n# rf = RandomForestClassifier().fit(train_xs, train_ys.reshape(-1))\n# rf_acc = accuracy_score(test_ys, rf.predict(test_xs))\n# model_acc = accuracy_score(test_ys, model.pred_fn(test_xs).argmax(axis=1))\n\n# rf_acc, model_acc",
    "crumbs": [
      "Overview",
      "API Documentations",
      "ML Module"
    ]
  },
  {
    "objectID": "02_ml_model.html#load-ml-module",
    "href": "02_ml_model.html#load-ml-module",
    "title": "jax-relax",
    "section": "Load ML Module",
    "text": "Load ML Module\nTODO: Need test cases\n\nrelax.ml_model.load_ml_module\n\n[source]\n\nrelax.ml_model.load_ml_module (name)\n\nLoad the ML module\n\nrelax.ml_model.download_ml_module\n\n[source]\n\nrelax.ml_model.download_ml_module (name, path=None)\n\n\nfor name in DEFAULT_DATA_CONFIGS.keys():\n    dm = load_data(name)\n    ml_model = load_ml_module(name)\n    X_train, y_train = dm['train']\n    X_test, y_test = dm['test']\n    model_acc = accuracy_score(y_test, ml_model.pred_fn(X_test).argmax(axis=1))",
    "crumbs": [
      "Overview",
      "API Documentations",
      "ML Module"
    ]
  },
  {
    "objectID": "02_ml_model.html#autoencoder",
    "href": "02_ml_model.html#autoencoder",
    "title": "jax-relax",
    "section": "AutoEncoder",
    "text": "AutoEncoder\n\nrelax.ml_model.AutoEncoder\n\n[source]\n\nclass relax.ml_model.AutoEncoder (enc_sizes, dec_sizes, output_size, dropout_rate=0.2, last_activation=‘sigmoid’, name=‘autoencoder’, **kwargs)\n\nA model grouping layers into an object with training/inference features.\nThere are three ways to instantiate a Model:",
    "crumbs": [
      "Overview",
      "API Documentations",
      "ML Module"
    ]
  },
  {
    "objectID": "02_ml_model.html#with-the-functional-api",
    "href": "02_ml_model.html#with-the-functional-api",
    "title": "jax-relax",
    "section": "With the “Functional API”",
    "text": "With the “Functional API”\nYou start from Input, you chain layer calls to specify the model’s forward pass, and finally you create your model from inputs and outputs:\ninputs = keras.Input(shape=(37,))\nx = keras.layers.Dense(32, activation=\"relu\")(inputs)\noutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nNote: Only dicts, lists, and tuples of input tensors are supported. Nested inputs are not supported (e.g. lists of list or dicts of dict).\nA new Functional API model can also be created by using the intermediate tensors. This enables you to quickly extract sub-components of the model.\nExample:\ninputs = keras.Input(shape=(None, None, 3))\nprocessed = keras.layers.RandomCrop(width=128, height=128)(inputs)\nconv = keras.layers.Conv2D(filters=32, kernel_size=3)(processed)\npooling = keras.layers.GlobalAveragePooling2D()(conv)\nfeature = keras.layers.Dense(10)(pooling)\n\nfull_model = keras.Model(inputs, feature)\nbackbone = keras.Model(processed, conv)\nactivations = keras.Model(conv, feature)\nNote that the backbone and activations models are not created with keras.Input objects, but with the tensors that originate from keras.Input objects. Under the hood, the layers and weights will be shared across these models, so that user can train the full_model, and use backbone or activations to do feature extraction. The inputs and outputs of the model can be nested structures of tensors as well, and the created models are standard Functional API models that support all the existing APIs.",
    "crumbs": [
      "Overview",
      "API Documentations",
      "ML Module"
    ]
  },
  {
    "objectID": "02_ml_model.html#by-subclassing-the-model-class",
    "href": "02_ml_model.html#by-subclassing-the-model-class",
    "title": "jax-relax",
    "section": "By subclassing the Model class",
    "text": "By subclassing the Model class\nIn that case, you should define your layers in __init__() and you should implement the model’s forward pass in call().\nclass MyModel(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n\n    def call(self, inputs):\n        x = self.dense1(inputs)\n        return self.dense2(x)\n\nmodel = MyModel()\nIf you subclass Model, you can optionally have a training argument (boolean) in call(), which you can use to specify a different behavior in training and inference:\nclass MyModel(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n        self.dropout = keras.layers.Dropout(0.5)\n\n    def call(self, inputs, training=False):\n        x = self.dense1(inputs)\n        x = self.dropout(x, training=training)\n        return self.dense2(x)\n\nmodel = MyModel()\nOnce the model is created, you can config the model with losses and metrics with model.compile(), train the model with model.fit(), or use the model to do prediction with model.predict().",
    "crumbs": [
      "Overview",
      "API Documentations",
      "ML Module"
    ]
  },
  {
    "objectID": "02_ml_model.html#with-the-sequential-class",
    "href": "02_ml_model.html#with-the-sequential-class",
    "title": "jax-relax",
    "section": "With the Sequential class",
    "text": "With the Sequential class\nIn addition, keras.Sequential is a special case of model where the model is purely a stack of single-input, single-output layers.\nmodel = keras.Sequential([\n    keras.Input(shape=(None, None, 3)),\n    keras.layers.Conv2D(filters=32, kernel_size=3),\n])\n\nae = AutoEncoder([10, 5], [5, 10], output_size=10, last_activation=None)\nae.compile(optimizer='adam', loss='mse')\n\n\nae.fit(X_train, X_train, epochs=5, batch_size=128)\n\nEpoch 1/5\n6/6 ━━━━━━━━━━━━━━━━━━━━ 2s 162ms/step - loss: 0.6734\nEpoch 2/5\n6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 973us/step - loss: 0.5926\nEpoch 3/5\n6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 0.5185\nEpoch 4/5\n6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 0.4764\nEpoch 5/5\n6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 0.4179\n\n\n&lt;keras.src.callbacks.history.History&gt;",
    "crumbs": [
      "Overview",
      "API Documentations",
      "ML Module"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data Module",
    "section": "",
    "text": "High-level interfaces for DataModule. Docs to be added.\n\n[source]\n\nclass relax.data_module.BaseDataModule (config, name=None)\n\nDataModule Interface",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Data Module"
    ]
  },
  {
    "objectID": "data.html#data-module-interfaces",
    "href": "data.html#data-module-interfaces",
    "title": "Data Module",
    "section": "",
    "text": "High-level interfaces for DataModule. Docs to be added.\n\n[source]\n\nclass relax.data_module.BaseDataModule (config, name=None)\n\nDataModule Interface",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Data Module"
    ]
  },
  {
    "objectID": "data.html#data-module",
    "href": "data.html#data-module",
    "title": "Data Module",
    "section": "Data Module",
    "text": "Data Module\nDataModule for processing data, training models, and benchmarking CF explanations.\n\nConfig\n\nrelax.data_module.DataModuleConfig\n\n[source]\n\nclass relax.data_module.DataModuleConfig (data_dir=None, data_name=None, continous_cols=[], discret_cols=[], imutable_cols=[], continuous_transformation=‘minmax’, discret_transformation=‘ohe’, sample_frac=None, train_indices=[], test_indices=[])\n\nConfigurator of DataModule.\n\nParameters:\n\ndata_dir (str) – The directory of dataset.\ndata_name (str) – The name of DataModule.\ncontinous_cols (List[str], default=[]) – Continuous features/columns in the data.\ndiscret_cols (List[str], default=[]) – Categorical features/columns in the data.\nimutable_cols (List[str], default=[]) – Immutable features/columns in the data.\ncontinuous_transformation (Optional[str], default=minmax) – Transformation for continuous features. None indicates unknown.\ndiscret_transformation (Optional[str], default=ohe) – Transformation for categorical features. None indicates unknown.\nsample_frac (Optional[float]) – Sample fraction of the data. Default to use the entire data.\ntrain_indices (List[int], default=[]) – Indices of training data.\ntest_indices (List[int], default=[]) – Indices of testing data.\n\n\n\n\nUtils\n\nutil functions for DataModule\n\n\nrelax.data_module.features2config\n\n[source]\n\nrelax.data_module.features2config (features, name, return_dict=False)\n\nGet DataModuleConfig from FeaturesList.\n\nParameters:\n\nfeatures (&lt;class 'relax.data_utils.features.FeaturesList'&gt;) – FeaturesList to be converted\nname (&lt;class 'str'&gt;) – Name of the data used for DataModuleConfig\nreturn_dict (&lt;class 'bool'&gt;, default=False) – Whether to return a dict or DataModuleConfig\n\n\n\nReturns:\n    (typing.Union[__main__.DataModuleConfig, typing.Dict]) – Return configs\n\n\nrelax.data_module.features2pandas\n\n[source]\n\nrelax.data_module.features2pandas (features, labels)\n\nConvert FeaturesList to pandas dataframe.\n\nParameters:\n\nfeatures (&lt;class 'relax.data_utils.features.FeaturesList'&gt;) – FeaturesList to be converted\nlabels (&lt;class 'relax.data_utils.features.FeaturesList'&gt;) – labels to be converted\n\n\n\nReturns:\n    (&lt;class 'pandas.core.frame.DataFrame'&gt;) – Return pandas dataframe\n\nExample:\n\nfeats = FeaturesList([\n    Feature(\"age\", np.random.normal(0, 1, (10, 1)), \n            transformation='minmax', is_immutable=True),\n    Feature(\"workclass\", np.random.randint(0, 2, (10, 1)), \n            transformation='ohe'),\n    Feature(\"education\", np.random.randint(0, 2, (10, 1)), \n            transformation='ordinal'),    \n])\nlabels = FeaturesList([\n    Feature(\"income\", np.random.randint(0, 2, (10, 1)), \n            transformation='identity'),\n])\ndf = features2pandas(feats, labels)\nassert isinstance(df, pd.DataFrame)\nassert df.shape == (10, 4)\n\n\nrelax.data_module.dataframe2labels\n\n[source]\n\nrelax.data_module.dataframe2labels (data, config)\n\nConvert pandas dataframe of labels to FeaturesList.\n\nrelax.data_module.dataframe2features\n\n[source]\n\nrelax.data_module.dataframe2features (data, config)\n\nConvert pandas dataframe of features to FeaturesList.\n\n\nMain Data Module\n\nMain module.\n\n\nrelax.data_module.DataModule\n\n[source]\n\nclass relax.data_module.DataModule (features, label, config=None, data=None, **kwargs)\n\nDataModule for tabular data.\n\nMethods\n[source]\n\nload_from_path (path, config=None)\n\nLoad DataModule from a directory.\n\nParameters:\n\npath (&lt;class 'str'&gt;) – Path to the directory to load DataModule\nconfig (typing.Union[typing.Dict, __main__.DataModuleConfig], default=None) – Configs of DataModule. This argument is ignored.\n\n\n\nReturns:\n    (&lt;class '__main__.DataModule'&gt;) – Initialized DataModule from path\n\n[source]\n\nfrom_config (config, data=None)\n\n\nParameters:\n\nconfig (typing.Union[typing.Dict, __main__.DataModuleConfig]) – Configs of DataModule\ndata (&lt;class 'pandas.core.frame.DataFrame'&gt;, default=None) – Passed in pd.Dataframe\n\n\n\nReturns:\n    (&lt;class '__main__.DataModule'&gt;) – Initialized DataModule from configs and data\n\n[source]\n\nfrom_features (features, label, name=None)\n\nCreate DataModule from FeaturesList.\n\nParameters:\n\nfeatures (&lt;class 'relax.data_utils.features.FeaturesList'&gt;) – Features of DataModule\nlabel (&lt;class 'relax.data_utils.features.FeaturesList'&gt;) – Labels of DataModule\nname (&lt;class 'str'&gt;, default=None) – Name of DataModule\n\n\n\nReturns:\n    (&lt;class '__main__.DataModule'&gt;) – Initialized DataModule from features and labels\n\n[source]\n\nfrom_numpy (xs, ys, name=None, transformation=‘minmax’)\n\nCreate DataModule from numpy arrays. Note that the xs are treated as continuous features.\n\nParameters:\n\nxs (&lt;class 'numpy.ndarray'&gt;) – Input data\nys (&lt;class 'numpy.ndarray'&gt;) – Labels\nname (&lt;class 'str'&gt;, default=None) – Name of DataModule\ntransformation (&lt;class 'str'&gt;, default=minmax)\n\n\n\nReturns:\n    (&lt;class '__main__.DataModule'&gt;) – Initialized DataModule from numpy arrays\n\n[source]\n\nsave (path)\n\nSave DataModule to a directory.\n\nParameters:\n\npath (&lt;class 'str'&gt;) – Path to the directory to save DataModule\n\n\n[source]\n\ntransform (data)\n\nTransform data to jax.Array.\n\nParameters:\n\ndata (typing.Union[pandas.core.frame.DataFrame, typing.Dict[str, jax.Array]]) – Data to be transformed\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;) – Transformed data\n\n[source]\n\ninverse_transform (data, return_type=‘pandas’)\n\nInverse transform data to pandas.DataFrame.\n\nParameters:\n\ndata (&lt;class 'jax.Array'&gt;) – Data to be inverse transformed\nreturn_type (&lt;class 'str'&gt;, default=pandas) – Type of the returned data. Should be one of [‘pandas’, ‘dict’]\n\n\n\nReturns:\n    (&lt;class 'pandas.core.frame.DataFrame'&gt;) – Inverse transformed data\n\n[source]\n\napply_constraints (xs, cfs, hard=False, rng_key=None, **kwargs)\n\nApply constraints to counterfactuals.\n\nParameters:\n\nxs (&lt;class 'jax.Array'&gt;) – Input data\ncfs (&lt;class 'jax.Array'&gt;) – Counterfactuals to be constrained\nhard (&lt;class 'bool'&gt;, default=False) – Whether to apply hard constraints or not\nrng_key (&lt;function PRNGKey at 0x7fcaaae82d40&gt;, default=None) – Random key\nkwargs (VAR_KEYWORD)\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;) – Constrained counterfactuals\n\n[source]\n\ncompute_reg_loss (xs, cfs, hard=False)\n\nCompute regularization loss.\n\nParameters:\n\nxs (&lt;class 'jax.Array'&gt;) – Input data\ncfs (&lt;class 'jax.Array'&gt;) – Counterfactuals to be constrained\nhard (&lt;class 'bool'&gt;, default=False) – Whether to apply hard constraints or not\n\n\n\nReturns:\n    (&lt;class 'float'&gt;)\n\n[source]\n\nset_transformations (feature_names_to_transformation)\n\nReset transformations for features.\n\nParameters:\n\nfeature_names_to_transformation (typing.Dict[str, typing.Union[str, typing.Dict, relax.data_utils.transforms.BaseTransformation]]) – Dict[feature_name, Transformation]\n\n\n\nReturns:\n    (&lt;class '__main__.DataModule'&gt;)\n\n[source]\n\nsample (size, stage=‘train’, key=None)\n\nSample data from DataModule.\n\nParameters:\n\nsize (float | int) – Size of the sample. If float, should be 0&lt;=size&lt;=1.\nstage (&lt;class 'str'&gt;, default=train) – Stage of data to sample from. Should be one of [‘train’, ‘valid’, ‘test’]\nkey (&lt;function PRNGKey at 0x7fcaaae82d40&gt;, default=None) – Random key.\n\n\n\nReturns:\n    (typing.Tuple[jax.Array, jax.Array]) – Sampled data\n\n\n\n# Test initialization\nconfig = DataModuleConfig.load_from_json(\"assets/adult/data/config.json\")\nconfig_1 = config.dict()\nconfig_1.update({\"imutable_cols\": []})\ndm = DataModule.from_config(config)\ndm_1 = DataModule.from_config(config.dict())\nassert dm_equals(dm, dm_1)\ndm_2 = DataModule.from_path(\"assets/adult/data\")\nassert dm_equals(dm, dm_2)\ndm_3 = DataModule.from_config(config_1)\nassert dm_equals(dm, dm_3)\nassert dm_3.config.imutable_cols == []\nfeats = FeaturesList.load_from_path(\"assets/adult/data/features\")\nlabel = FeaturesList.load_from_path(\"assets/adult/data/label\")\ndm_4 = DataModule.from_features(feats, label)\nassert dm_equals(dm, dm_4, indices_equals=False) # Indices are not supposed to be equal\n\n\n# Test from_numpy\nxs, ys = make_classification(n_samples=100, n_features=5, n_informative=3, random_state=0)\ndm_5 = DataModule.from_numpy(xs, ys, name=\"test\", transformation='identity')\nconfig_5 = dm_5.config\nassert dm_5.config.data_name == \"test\"\nassert dm_5.data.shape == (100, 6)\nassert np.allclose(dm_5.data.to_numpy(), np.concatenate([xs, ys.reshape(-1, 1)], axis=1))\nassert np.allclose(\n    xs[config_5.train_indices],\n    dm_5['train'][0]\n)\nassert np.allclose(\n    xs[config_5.test_indices],\n    dm_5['test'][0]\n)\ndm_5.save('tmp/test')\ndm_6 = DataModule.load_from_path('tmp/test')\nassert dm_equals(dm_5, dm_6)\nshutil.rmtree(\"tmp/test\")\n\n\n# Test save and load\ndm.save(\"tmp/adult\")\ndm_5 = DataModule.load_from_path(\"tmp/adult\")\nassert dm_equals(dm, dm_5)\nshutil.rmtree(\"tmp/adult\")\n\n\n# Test set_transformations\ndm_6 = deepcopy(dm)\ndm_6.set_transformations({\"age\": 'identity'})\nassert dm_6.features['age'].transformation.name == 'identity'\nassert np.array_equal(dm_6.xs[:, :1], dm_6.data[['age']].to_numpy())\ndm_6.set_transformations({feat: 'ordinal' for feat in config.discret_cols})\nassert dm_6.xs.shape == (dm.data.shape[0], len(config.continous_cols) + len(config.discret_cols))\n\nassert np.array_equal(dm_6.xs[:, :1], dm_6.data[['age']].to_numpy())\n\ntest_fail(lambda: dm_6.set_transformations({1: 'identity'}), contains=\"Invalid idx type\")\ntest_fail(lambda: dm_6.set_transformations({\"❤\": 'identity'}), contains=\"Invalid feature name\")\ntest_fail(lambda: dm_6.set_transformations({\"age\": '❤'}), contains=\"Unknown transformation\")\ntest_fail(lambda: dm_6.set_transformations('❤'), contains=\"Invalid feature_names_to_transformation type\")\n\ndm_6.set_transformations({\"age\": MinMaxTransformation()})\nassert np.allclose(dm_6.xs[:, :1], dm.xs[:, :1])\n\n\n# Test sample\nsampled_xs, sampled_ys = dm.sample(0.1)\nassert len(sampled_xs) == len(sampled_ys)\nassert sampled_xs.shape[0] == int(0.1 * dm['train'][0].shape[0])\nassert not jnp.all(sampled_xs == dm['train'][0][:sampled_xs.shape[0]])\n\nsampled_xs, sampled_ys = dm.sample(100)\nassert len(sampled_xs) == len(sampled_ys)\nassert sampled_xs.shape[0] == 100\nassert not jnp.all(sampled_xs == dm['train'][0][:100])\n\ntest_fail(lambda: dm.sample(1.1), contains='should be a floating number 0&lt;=size&lt;=1,')\ntest_fail(lambda: dm.sample('train'), contains='or an integer')\n\nxs = dm['train'][0]\ncfs = jrand.uniform(jrand.PRNGKey(0), shape=xs.shape, minval=0.01, maxval=0.99)\ncfs = dm.apply_constraints(xs, cfs, hard=False)\nassert cfs.shape == xs.shape\n\ncfs = dm.apply_constraints(xs, cfs, hard=True)\nassert cfs.shape == xs.shape\n\n\n# Test transform\ndata = dm.transform(dm.data)\nassert np.allclose(data, dm.xs)",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Data Module"
    ]
  },
  {
    "objectID": "data.html#load-data",
    "href": "data.html#load-data",
    "title": "Data Module",
    "section": "Load Data",
    "text": "Load Data\n\n# from sklearn.datasets import make_classification\n\n# xs, ys = make_classification(n_samples=1000, n_features=10)\n# xs = pd.DataFrame(xs, columns=[f\"col_{i}\" for i in range(10)])\n# ys = pd.DataFrame(ys, columns=['label'])\n# data = pd.concat([xs, ys], axis=1)\n# os.makedirs('assets/dummy/data', exist_ok=True)\n# data.to_csv('assets/dummy/data/data.csv', index=False)\n# config = DataModuleConfig(\n#     data_name=\"dummy\", \n#     data_dir=\"assets/dummy/data/data.csv\", \n#     continous_cols=[f\"col_{i}\" for i in range(10)]\n# )\n# dm = DataModule(config)\n# dm.save('assets/dummy/data')\n\n\n# for data_name in DEFAULT_DATA_CONFIGS.keys():\n#     print(f\"Loading {data_name}...\")\n#     shutil.rmtree(f'../relax-assets/{data_name}', ignore_errors=True)\n#     conf_path = DEFAULT_DATA_CONFIGS[data_name]['conf']\n#     config = load_json(conf_path)['data_configs']\n#     dm_config = DataModuleConfig(**config)\n#     dm = DataModule(dm_config)\n#     dm.save(f'../relax-assets/{data_name}/data')\n\n\n# for data_name in DEFAULT_DATA_CONFIGS.keys():\n#     print(f\"Loading {data_name}...\")\n#     DataModule.load_from_path(f'../relax-assets/{data_name}/data')\n\n\n# config = load_json('assets/adult/configs.json')['data_configs']\n# dm_config = DataModuleConfig(**config)\n# dm = DataModule(dm_config)\n\n\nrelax.data_module.load_data\n\n[source]\n\nrelax.data_module.load_data (data_name, return_config=False, data_configs=None)\n\nHigh-level util function for loading data and data_config.\n\nParameters:\n\ndata_name (&lt;class 'str'&gt;) – The name of data\nreturn_config (&lt;class 'bool'&gt;, default=False) – Deprecated\ndata_configs (&lt;class 'dict'&gt;, default=None) – Data configs to override default configuration\n\n\n\nReturns:\n    (typing.Union[__main__.DataModule, typing.Tuple[__main__.DataModule, __main__.DataModuleConfig]]) – Return DataModule or (DataModule, DataModuleConfig)\n\n\nrelax.data_module.download_data_module_files\n\n[source]\n\nrelax.data_module.download_data_module_files (data_name, data_parent_dir, download_original_data=False)\n\n\nParameters:\n\ndata_name (&lt;class 'str'&gt;) – The name of data\ndata_parent_dir (&lt;class 'pathlib.Path'&gt;) – The directory to save data.\ndownload_original_data (&lt;class 'bool'&gt;, default=False) – Download original data or not\n\n\nload_data easily loads example datasets by passing the data_name. For example, you can load the adult as:\n\ndm = load_data(data_name = 'adult')\n\n\nSupported Datasets\nload_data currently supports following datasets:\n\n\n\n\n\n\n\n\n\n# Cont Features\n# Cat Features\n# of Data Points\n\n\n\n\nadult\n2\n6\n32561\n\n\nheloc\n21\n2\n10459\n\n\noulad\n23\n8\n32593\n\n\ncredit\n20\n3\n30000\n\n\ncancer\n30\n0\n569\n\n\nstudent_performance\n2\n14\n649\n\n\ntitanic\n2\n24\n891\n\n\ngerman\n7\n13\n1000\n\n\nspam\n57\n0\n4601\n\n\nozone\n72\n0\n2534\n\n\nqsar\n38\n3\n1055\n\n\nbioresponse\n1776\n0\n3751\n\n\nchurn\n3\n16\n7043\n\n\nroad\n29\n3\n111762\n\n\ndummy\n10\n0\n1000",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Data Module"
    ]
  },
  {
    "objectID": "legacy/utils.html",
    "href": "legacy/utils.html",
    "title": "Utils",
    "section": "",
    "text": "[source]\n\nrelax.utils.validate_configs (configs, config_cls)\n\nreturn a valid configuration object.\n\nParameters:\n\nconfigs (dict | pydantic.main.BaseModel) – A configuration of the model/dataset.\nconfig_cls (&lt;class 'pydantic.main.BaseModel'&gt;) – The desired configuration class.\n\n\n\nReturns:\n    (&lt;class 'pydantic.main.BaseModel'&gt;)\n\nWe define a configuration object (which inherent BaseParser) to manage training/model/data configurations. validate_configs ensures to return the designated configuration object.\nFor example, we define a configuration object LearningConfigs:\n\nclass LearningConfigs(BaseParser):\n    lr: float\n\nA configuration can be LearningConfigs, or the raw data in dictionary.\n\nconfigs_dict = dict(lr=0.01)\n\nvalidate_configs will return a designated configuration object.\n\nconfigs = validate_configs(configs_dict, LearningConfigs)\nassert type(configs) == LearningConfigs\nassert configs.lr == configs_dict['lr']"
  },
  {
    "objectID": "legacy/utils.html#configurations",
    "href": "legacy/utils.html#configurations",
    "title": "Utils",
    "section": "",
    "text": "[source]\n\nrelax.utils.validate_configs (configs, config_cls)\n\nreturn a valid configuration object.\n\nParameters:\n\nconfigs (dict | pydantic.main.BaseModel) – A configuration of the model/dataset.\nconfig_cls (&lt;class 'pydantic.main.BaseModel'&gt;) – The desired configuration class.\n\n\n\nReturns:\n    (&lt;class 'pydantic.main.BaseModel'&gt;)\n\nWe define a configuration object (which inherent BaseParser) to manage training/model/data configurations. validate_configs ensures to return the designated configuration object.\nFor example, we define a configuration object LearningConfigs:\n\nclass LearningConfigs(BaseParser):\n    lr: float\n\nA configuration can be LearningConfigs, or the raw data in dictionary.\n\nconfigs_dict = dict(lr=0.01)\n\nvalidate_configs will return a designated configuration object.\n\nconfigs = validate_configs(configs_dict, LearningConfigs)\nassert type(configs) == LearningConfigs\nassert configs.lr == configs_dict['lr']"
  },
  {
    "objectID": "legacy/utils.html#categorical-normalization",
    "href": "legacy/utils.html#categorical-normalization",
    "title": "Utils",
    "section": "Categorical normalization",
    "text": "Categorical normalization\n\nrelax.legacy.utils.cat_normalize\n\n[source]\n\nrelax.legacy.utils.cat_normalize (cf, cat_arrays, cat_idx, hard=False)\n\nEnsure generated counterfactual explanations to respect one-hot encoding constraints.\n\nParameters:\n\ncf (&lt;class 'jax.Array'&gt;) – Unnormalized counterfactual explanations [n_samples, n_features]\ncat_arrays (typing.List[typing.List[str]]) – A list of a list of each categorical feature name\ncat_idx (&lt;class 'int'&gt;) – Index that starts categorical features\nhard (&lt;class 'bool'&gt;, default=False) – If True, return one-hot vectors; If False, return probability normalized via softmax\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;)\n\nA tabular data point is encoded as x = [\\underbrace{x_{0}, x_{1}, ..., x_{m}}_{\\text{cont features}},\n\\underbrace{x_{m+1}^{c=1},..., x_{m+p}^{c=1}}_{\\text{cat feature (1)}}, ...,\n\\underbrace{x_{k-q}^{c=i},..., x_{k}^{^{c=i}}}_{\\text{cat feature (i)}}]\ncat_normalize ensures the generated cf that satisfy the categorical constraints, i.e., \\sum_j x^{c=i}_j=1, x^{c=i}_j &gt; 0, \\forall c=[1, ..., i].\ncat_idx is the index of the first categorical feature. In the above example, cat_idx is m+1.\nFor example, let’s define a valid input data point:\n\nx = np.array([\n    [1., .9, 'dog', 'gray'],\n    [.3, .3, 'cat', 'gray'],\n    [.7, .1, 'fish', 'red'],\n    [1., .6, 'dog', 'gray'],\n    [.1, .2, 'fish', 'yellow']\n])\n\nWe encode the categorical features via the OneHotEncoder in sklearn.\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ncat_idx = 2\nohe = OneHotEncoder(sparse_output=False)\nx_cat = ohe.fit_transform(x[:, cat_idx:])\nx_cont = x[:, :cat_idx].astype(float)\nx_transformed = np.concatenate(\n    (x_cont, x_cat), axis=1\n)\n\nIf hard=True, the categorical features are in one-hot format.\n\ncfs = np.random.randn(*x_transformed.shape)\ncfs = cat_normalize(cfs, ohe.categories_, \n    cat_idx=cat_idx, hard=True)\ncfs[:1]\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nArray([[-0.47835127, -0.32345298,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ]], dtype=float32)\n\n\nIf hard=False, the categorical features are normalized via softmax function.\n\ncfs = np.random.randn(*x_transformed.shape)\ncfs = cat_normalize(cfs, ohe.categories_, \n    cat_idx=cat_idx, hard=False)\nn_cat_feats = len(ohe.categories_)\n\nassert (cfs[:, cat_idx:].sum(axis=1) - n_cat_feats * jnp.ones(len(cfs))).sum() &lt; 1e-6"
  },
  {
    "objectID": "legacy/utils.html#training-utils",
    "href": "legacy/utils.html#training-utils",
    "title": "Utils",
    "section": "Training Utils",
    "text": "Training Utils\n\nrelax.legacy.utils.make_model\n\n[source]\n\nrelax.legacy.utils.make_model (m_configs, model)\n\n\nParameters:\n\nm_configs (typing.Dict[str, typing.Any])\nmodel (&lt;class 'haiku._src.module.Module'&gt;) – model configs\n\n\n\nReturns:\n    (&lt;class 'haiku._src.transform.Transformed'&gt;)\n\n\nrelax.legacy.utils.make_hk_module\n\n[source]\n\nrelax.legacy.utils.make_hk_module (module, *args, **kargs)\n\n\nParameters:\n\nmodule (&lt;class 'haiku._src.module.Module'&gt;) – haiku module\nargs (VAR_POSITIONAL) – haiku module arguments\nkargs (VAR_KEYWORD)\n\n\n\nReturns:\n    (&lt;class 'haiku._src.transform.Transformed'&gt;) – haiku module arguments\n\n\nrelax.legacy.utils.init_net_opt\n\n[source]\n\nrelax.legacy.utils.init_net_opt (net, opt, X, key)\n\n\nrelax.utils.grad_update\n\n[source]\n\nrelax.utils.grad_update (grads, params, opt_state, opt)\n\n\nrelax.legacy.utils.check_cat_info\n\n[source]\n\nrelax.legacy.utils.check_cat_info (method)"
  },
  {
    "objectID": "legacy/utils.html#helper-functions",
    "href": "legacy/utils.html#helper-functions",
    "title": "Utils",
    "section": "Helper functions",
    "text": "Helper functions\n\nrelax.utils.load_json\n\n[source]\n\nrelax.utils.load_json (f_name)\n\n\nParameters:\n\nf_name (&lt;class 'str'&gt;)\n\n\n\nReturns:\n    (typing.Dict[str, typing.Any]) – file name"
  },
  {
    "objectID": "legacy/utils.html#loss-functions",
    "href": "legacy/utils.html#loss-functions",
    "title": "Utils",
    "section": "Loss Functions",
    "text": "Loss Functions\n\nrelax.legacy.utils.binary_cross_entropy\n\n[source]\n\nrelax.legacy.utils.binary_cross_entropy (preds, labels)\n\nPer-sample binary cross-entropy loss function.\n\nParameters:\n\npreds (&lt;class 'jax.Array'&gt;) – The predicted values\nlabels (&lt;class 'jax.Array'&gt;) – The ground-truth labels\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;) – Loss value\n\n\nrelax.legacy.utils.sigmoid\n\n[source]\n\nrelax.legacy.utils.sigmoid (x)"
  },
  {
    "objectID": "legacy/utils.html#metrics",
    "href": "legacy/utils.html#metrics",
    "title": "Utils",
    "section": "Metrics",
    "text": "Metrics\n\nrelax.legacy.utils.proximity\n\n[source]\n\nrelax.legacy.utils.proximity (x, cf)\n\n\nrelax.legacy.utils.dist\n\n[source]\n\nrelax.legacy.utils.dist (x, cf, ord=2)\n\n\nrelax.legacy.utils.accuracy\n\n[source]\n\nrelax.legacy.utils.accuracy (y_true, y_pred)"
  },
  {
    "objectID": "legacy/utils.html#config",
    "href": "legacy/utils.html#config",
    "title": "Utils",
    "section": "Config",
    "text": "Config\n\nrelax.utils.get_config\n\n[source]\n\nrelax.utils.get_config ()"
  },
  {
    "objectID": "legacy/module.html",
    "href": "legacy/module.html",
    "title": "Module",
    "section": "",
    "text": "Networks are haiku.module, which define model architectures.\n\n[source]\n\nclass relax.legacy.module.BaseNetwork ()\n\nBaseNetwork needs a is_training argument\n\n[source]\n\nclass relax.legacy.module.DenseBlock (output_size, dropout_rate=0.3, name=None)\n\nA DenseBlock consists of a dense layer, followed by Leaky Relu and a dropout layer.\n\nParameters:\n\noutput_size (&lt;class 'int'&gt;) – Output dimensionality.\ndropout_rate (&lt;class 'float'&gt;, default=0.3) – Dropout rate.\nname (str | None, default=None) – Name of the Module\n\n\n\n[source]\n\nclass relax.ml_model.MLP (sizes, dropout_rate=0.3, name=None)\n\nA MLP consists of a list of DenseBlock layers.\n\nParameters:\n\nsizes (typing.Iterable[int]) – Sequence of layer sizes.\ndropout_rate (&lt;class 'float'&gt;, default=0.3) – Dropout rate.\nname (str | None, default=None) – Name of the Module"
  },
  {
    "objectID": "legacy/module.html#networks",
    "href": "legacy/module.html#networks",
    "title": "Module",
    "section": "",
    "text": "Networks are haiku.module, which define model architectures.\n\n[source]\n\nclass relax.legacy.module.BaseNetwork ()\n\nBaseNetwork needs a is_training argument\n\n[source]\n\nclass relax.legacy.module.DenseBlock (output_size, dropout_rate=0.3, name=None)\n\nA DenseBlock consists of a dense layer, followed by Leaky Relu and a dropout layer.\n\nParameters:\n\noutput_size (&lt;class 'int'&gt;) – Output dimensionality.\ndropout_rate (&lt;class 'float'&gt;, default=0.3) – Dropout rate.\nname (str | None, default=None) – Name of the Module\n\n\n\n[source]\n\nclass relax.ml_model.MLP (sizes, dropout_rate=0.3, name=None)\n\nA MLP consists of a list of DenseBlock layers.\n\nParameters:\n\nsizes (typing.Iterable[int]) – Sequence of layer sizes.\ndropout_rate (&lt;class 'float'&gt;, default=0.3) – Dropout rate.\nname (str | None, default=None) – Name of the Module"
  },
  {
    "objectID": "legacy/module.html#predictive-model",
    "href": "legacy/module.html#predictive-model",
    "title": "Module",
    "section": "Predictive Model",
    "text": "Predictive Model\n\nrelax.legacy.module.PredictiveModel\n\n[source]\n\nclass relax.legacy.module.PredictiveModel (sizes, dropout_rate=0.3, name=None)\n\nA basic predictive model for binary classification.\n\nParameters:\n\nsizes (typing.List[int]) – Sequence of layer sizes.\ndropout_rate (&lt;class 'float'&gt;, default=0.3) – Dropout rate.\nname (typing.Optional[str], default=None) – Name of the module.\n\n\nUse make_hk_module to create a haiku.Transformed model.\n\nfrom relax.legacy.utils import make_hk_module\n\n\nnet = make_hk_module(PredictiveModel, sizes=[50, 20, 10], dropout_rate=0.3)\n\nWe make some random data.\n\nkey = hk.PRNGSequence(42)\nxs = random.normal(next(key), (1000, 10))\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nWe can then initalize the model\n\nparams = net.init(next(key), xs, is_training=True)\n\nWe can view model’s structure via jax.tree_map.\n\njax.tree_map(lambda x: x.shape, params)\n\n{'predictive_model/linear': {'b': (1,), 'w': (10, 1)},\n 'predictive_model/mlp/dense_block/linear': {'b': (50,), 'w': (10, 50)},\n 'predictive_model/mlp/dense_block_1/linear': {'b': (20,), 'w': (50, 20)},\n 'predictive_model/mlp/dense_block_2/linear': {'b': (10,), 'w': (20, 10)}}\n\n\nModel output is produced via apply function.\n\ny = net.apply(params, next(key), xs, is_training=True)\n\nFor more usage of haiku.module, please refer to Haiku documentation."
  },
  {
    "objectID": "legacy/module.html#training-modules-api",
    "href": "legacy/module.html#training-modules-api",
    "title": "Module",
    "section": "Training Modules API",
    "text": "Training Modules API\n\nrelax.legacy.module.BaseTrainingModule\n\n[source]\n\nclass relax.legacy.module.BaseTrainingModule ()\n\nHelper class that provides a standard way to create an ABC using inheritance."
  },
  {
    "objectID": "legacy/module.html#predictive-training-module",
    "href": "legacy/module.html#predictive-training-module",
    "title": "Module",
    "section": "Predictive Training Module",
    "text": "Predictive Training Module\n\nrelax.legacy.module.PredictiveTrainingModuleConfigs\n\n[source]\n\nclass relax.legacy.module.PredictiveTrainingModuleConfigs (lr, sizes, dropout_rate=0.3)\n\nConfigurator of PredictiveTrainingModule.\n\nParameters:\n\nlr (float) – Learning rate.\nsizes (List[int]) – Sequence of layer sizes.\ndropout_rate (float, default=0.3) – Dropout rate\n\n\n\nrelax.legacy.module.PredictiveTrainingModule\n\n[source]\n\nclass relax.legacy.module.PredictiveTrainingModule (m_configs)\n\nA training module for predictive models."
  },
  {
    "objectID": "legacy/trainer.html",
    "href": "legacy/trainer.html",
    "title": "Training",
    "section": "",
    "text": "relax.legacy.trainer.TrainingConfigs\n[source]\nConfigurator of train_model.\n[source]\nTrain models with params and opt_state.\n[source]\nTrain models."
  },
  {
    "objectID": "legacy/trainer.html#examples",
    "href": "legacy/trainer.html#examples",
    "title": "Training",
    "section": "Examples",
    "text": "Examples\nA siimple example to train a predictive model.\n\nfrom relax.legacy.module import PredictiveTrainingModule, PredictiveModelConfigs\nfrom relax.data_module import load_data\n\n\ndatamodule = load_data('adult')\n\nparams, opt_state = train_model(\n    PredictiveTrainingModule({'sizes': [64, 32, 16], 'lr': 0.003}), \n    datamodule,\n)\n\n/home/birk/code/jax-relax/relax/legacy/ckpt_manager.py:47: UserWarning: `monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\n  warnings.warn(\nEpoch 0: 100%|██████████| 191/191 [00:01&lt;00:00, 106.57batch/s, train/train_loss=0.08575804] \n\n\n\nfrom relax.ml_model import MLModule\n\n\nmodel = MLModule()\nmodel.train(datamodule, batch_size=128, epochs=1)\n\n191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.6769 - loss: 0.6131\n\n\n&lt;relax.ml_model.MLModule&gt;"
  },
  {
    "objectID": "data_utils/transform.html",
    "href": "data_utils/transform.html",
    "title": "Feature Transformation",
    "section": "",
    "text": "relax.data_utils.transforms.BaseTransformation\n\n[source]\n\nclass relax.data_utils.transforms.BaseTransformation (name, transformer=None)\n\nBase class for all transformations.\n\nrelax.data_utils.transforms.MinMaxTransformation\n\n[source]\n\nclass relax.data_utils.transforms.MinMaxTransformation ()\n\nBase class for all transformations.\n\nxs = np.random.randn(100, 1)\nminmax_t = MinMaxTransformation()\ntransformed_xs = minmax_t.fit_transform(xs)\nassert np.allclose(minmax_t.inverse_transform(transformed_xs), xs)\nassert minmax_t.is_categorical is False\n\nx = np.random.randn(100, 1)\ncf_constrained = minmax_t.apply_constraints(xs, x)\nassert np.all(cf_constrained &gt;= 0) and np.all(cf_constrained &lt;= 1)\n\n# Test from_dict and to_dict\nscaler_1 = MinMaxTransformation().from_dict(minmax_t.to_dict())\nassert np.allclose(minmax_t.transform(xs), scaler_1.transform(xs))\n\n\nrelax.data_utils.transforms.OneHotTransformation\n\n[source]\n\nrelax.data_utils.transforms.OneHotTransformation ()\n\n\nrelax.data_utils.transforms.GumbelSoftmaxTransformation\n\n[source]\n\nclass relax.data_utils.transforms.GumbelSoftmaxTransformation (tau=0.1)\n\nApply Gumbel softmax tricks for categorical transformation.\n\nrelax.data_utils.transforms.SoftmaxTransformation\n\n[source]\n\nclass relax.data_utils.transforms.SoftmaxTransformation ()\n\nBase class for all transformations.\n\ndef test_ohe_t(ohe_cls):\n    xs = np.random.choice(['a', 'b', 'c'], size=(100, 1))\n    ohe_t = ohe_cls().fit(xs)\n    transformed_xs = ohe_t.transform(xs)\n    rng_key = jax.random.PRNGKey(get_config().global_seed)\n    assert ohe_t.is_categorical\n\n    x = jax.random.uniform(rng_key, shape=(100, 3))\n    # Test hard=True which applies softmax function.\n    soft = ohe_t.apply_constraints(transformed_xs, x, hard=False, rng_key=rng_key)\n    assert jnp.allclose(soft.sum(axis=-1), 1)\n    assert jnp.all(soft &gt;= 0)\n    assert jnp.all(soft &lt;= 1)\n    assert jnp.allclose(jnp.zeros((len(x), 1)), ohe_t.compute_reg_loss(xs, soft, hard=False))\n    assert jnp.allclose(soft, ohe_t.apply_constraints(transformed_xs, x, hard=False))\n\n    # Test hard=True which enforce one-hot constraint.\n    hard = ohe_t.apply_constraints(transformed_xs, x, hard=True, rng_key=rng_key)\n    assert np.all([1 in x for x in hard])\n    assert np.all([0 in x for x in hard])\n    assert jnp.allclose(hard.sum(axis=-1), 1)\n    assert jnp.allclose(jnp.zeros((len(x), 1)), ohe_t.compute_reg_loss(xs, hard, hard=False))\n\n    # Test compute_reg_loss\n    assert jnp.ndim(ohe_t.compute_reg_loss(xs, soft, hard=False)) == 0\n\n    # Test from_dict and to_dict\n    ohe_t_1 = ohe_cls().from_dict(ohe_t.to_dict())\n    assert np.allclose(ohe_t.transform(xs), ohe_t_1.transform(xs))\n\n\ntest_ohe_t(SoftmaxTransformation)\ntest_ohe_t(GumbelSoftmaxTransformation)\n\n\nrelax.data_utils.transforms.IdentityTransformation\n\n[source]\n\nclass relax.data_utils.transforms.IdentityTransformation ()\n\nBase class for all transformations.\n\nrelax.data_utils.transforms.OrdinalTransformation\n\n[source]\n\nclass relax.data_utils.transforms.OrdinalTransformation ()\n\nBase class for all transformations.\n\nxs = np.random.choice(['a', 'b', 'c'], size=(100, 1))\nencoder = OrdinalTransformation().fit(xs)\ntransformed_xs = encoder.transform(xs)\nassert np.all(encoder.inverse_transform(transformed_xs) == xs)\nassert encoder.is_categorical\n\n# Test from_dict and to_dict\nencoder_1 = OrdinalTransformation().from_dict(encoder.to_dict())\nassert np.allclose(encoder.transform(xs), encoder_1.transform(xs))\n\nxs = np.random.randn(100, 1)\nscaler = IdentityTransformation()\ntransformed_xs = scaler.fit_transform(xs)\nassert np.all(transformed_xs == xs)\n\n# Test from_dict and to_dict\nscaler_1 = IdentityTransformation().from_dict(scaler.to_dict())\nassert np.allclose(scaler.transform(xs), scaler_1.transform(xs))",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Data Utils",
      "Feature Transformation"
    ]
  },
  {
    "objectID": "data_utils/preprocessing.html",
    "href": "data_utils/preprocessing.html",
    "title": "Data Preprocessors",
    "section": "",
    "text": "relax.data_utils.preprocessing.DataPreprocessor\n\n[source]\n\nclass relax.data_utils.preprocessing.DataPreprocessor (name=None)\n\nBase class for data preprocessors.\n\nParameters:\n\nname (&lt;class 'str'&gt;, default=None) – The name of the preprocessor. If None, the class name will be used.\n\n\n\nMethods\n[source]\n\nfit (xs, y=None)\n\nFit the preprocessor with xs and y.\n[source]\n\ntransform (xs)\n\nTransform xs.\n[source]\n\nfit_transform (xs, y=None)\n\nFit the preprocessor with xs and y, then transform xs.\n[source]\n\ninverse_transform (xs)\n\nInverse transform xs.\n[source]\n\nto_dict ()\n\nConvert the preprocessor to a dictionary.\n[source]\n\nfrom_dict (params)\n\nLoad the attributes of the preprocessor from a dictionary.\n\n\nrelax.data_utils.preprocessing.MinMaxScaler\n\n[source]\n\nclass relax.data_utils.preprocessing.MinMaxScaler ()\n\nBase class for data preprocessors.\n\nMethods\n[source]\n\nfit (xs, y=None)\n\nFit the preprocessor with xs and y.\n[source]\n\ntransform (xs)\n\nTransform xs.\n[source]\n\nfit_transform (xs, y=None)\n\nFit the preprocessor with xs and y, then transform xs.\n[source]\n\ninverse_transform (xs)\n\nInverse transform xs.\n[source]\n\nto_dict ()\n\nConvert the preprocessor to a dictionary.\n[source]\n\nfrom_dict (params)\n\nLoad the attributes of the preprocessor from a dictionary.\n\n\nxs = np.random.randn(100, )\nscaler = MinMaxScaler()\ntransformed_xs = scaler.fit_transform(xs)\nassert transformed_xs.shape == (100, )\nassert np.allclose(xs, scaler.inverse_transform(transformed_xs))\n# Test correctness \nassert np.allclose(\n    transformed_xs, \n    skp.MinMaxScaler().fit_transform(xs.reshape(100, 1)).reshape(100,)\n)\n# Also work with 2D array\nxs = xs.reshape(100, 1)\nscaler = MinMaxScaler()\ntransformed_xs = scaler.fit_transform(xs)\nassert np.allclose(xs, scaler.inverse_transform(transformed_xs))\nassert np.allclose(\n    transformed_xs, \n    skp.MinMaxScaler().fit_transform(xs.reshape(100, 1))\n)\n\nMinMaxScaler only supports scaling a single feature.\n\nxs = xs.reshape(50, 2)\nscaler = MinMaxScaler()\ntest_fail(lambda: scaler.fit_transform(xs), \n          contains=\"`MinMaxScaler` only supports array with a single feature\")\n\nConvert to a dictionary (or the pytree representations).\n\nxs = xs.reshape(-1, 1)\nscaler = MinMaxScaler().fit(xs)\nscaler_1 = MinMaxScaler().from_dict(scaler.to_dict())\nassert np.allclose(scaler.transform(xs), scaler_1.transform(xs))\n\n\nrelax.data_utils.preprocessing.EncoderPreprocessor\n\n[source]\n\nclass relax.data_utils.preprocessing.EncoderPreprocessor (name=None)\n\nEncode categorical features as an integer array.\n\nParameters:\n\nname (&lt;class 'str'&gt;, default=None) – The name of the preprocessor. If None, the class name will be used.\n\n\n\nMethods\n[source]\n\nfit (xs, y=None)\n\nFit the preprocessor with xs and y.\n[source]\n\ntransform (xs)\n\nTransform xs.\n[source]\n\nfit_transform (xs, y=None)\n\nFit the preprocessor with xs and y, then transform xs.\n[source]\n\ninverse_transform (xs)\n\nInverse transform xs.\n[source]\n\nto_dict ()\n\nConvert the preprocessor to a dictionary.\n[source]\n\nfrom_dict (params)\n\nLoad the attributes of the preprocessor from a dictionary.\n\n\nrelax.data_utils.preprocessing.OrdinalPreprocessor\n\n[source]\n\nclass relax.data_utils.preprocessing.OrdinalPreprocessor (name=None)\n\nOrdinal encoder for a single feature.\n\nParameters:\n\nname (&lt;class 'str'&gt;, default=None) – The name of the preprocessor. If None, the class name will be used.\n\n\n\nMethods\n[source]\n\nfit (xs, y=None)\n\nFit the preprocessor with xs and y.\n[source]\n\ntransform (xs)\n\nTransform xs.\n[source]\n\nfit_transform (xs, y=None)\n\nFit the preprocessor with xs and y, then transform xs.\n[source]\n\ninverse_transform (xs)\n\nInverse transform xs.\n[source]\n\nto_dict ()\n\nConvert the preprocessor to a dictionary.\n[source]\n\nfrom_dict (params)\n\nLoad the attributes of the preprocessor from a dictionary.\n\n\nxs = np.random.choice(['a', 'b', 'c'], size=(100, 1))\nenc = OrdinalPreprocessor().fit(xs)\ntransformed_xs = enc.transform(xs)\nassert np.all(enc.inverse_transform(transformed_xs) == xs)\n# Test from_dict and to_dict\nenc_1 = OrdinalPreprocessor().from_dict(enc.to_dict())\nassert np.all(enc.transform(xs) == enc_1.transform(xs))\n\nxs = np.array(['a', 'b', 'c', np.nan, 'a', 'b', 'c', np.nan], dtype=object).reshape(-1, 1)\nenc = OrdinalPreprocessor().fit(xs)\n# Check categories_\nassert np.array_equiv(enc.categories_, np.array(['a', 'b', 'c', np.nan], dtype=str)) \ntransformed_xs = enc.transform(xs)\nassert transformed_xs.shape == (8, 1)\ninverse_transformed_xs = enc.inverse_transform(transformed_xs)\nassert np.all(inverse_transformed_xs == xs.astype(str))\n# Test from_dict and to_dict\nenc_1 = OrdinalPreprocessor().from_dict(enc.to_dict())\nassert np.all(enc.transform(xs) == enc_1.transform(xs))\nassert np.array_equal(enc.categories_, enc_1.categories_)\n\nxs = np.random.choice(['a', 'b', 'c'], size=(100, ))\ntest_fail(lambda: OrdinalPreprocessor().fit_transform(xs), \n    contains=\"OrdinalPreprocessor only supports 2D array with a single feature\")\n\n\nrelax.data_utils.preprocessing.OneHotEncoder\n\n[source]\n\nclass relax.data_utils.preprocessing.OneHotEncoder (name=None)\n\nOne-hot encoder for a single categorical feature.\n\nParameters:\n\nname (&lt;class 'str'&gt;, default=None) – The name of the preprocessor. If None, the class name will be used.\n\n\n\nMethods\n[source]\n\nfit (xs, y=None)\n\nFit the preprocessor with xs and y.\n[source]\n\ntransform (xs)\n\nTransform xs.\n[source]\n\nfit_transform (xs, y=None)\n\nFit the preprocessor with xs and y, then transform xs.\n[source]\n\ninverse_transform (xs)\n\nInverse transform xs.\n[source]\n\nto_dict ()\n\nConvert the preprocessor to a dictionary.\n[source]\n\nfrom_dict (params)\n\nLoad the attributes of the preprocessor from a dictionary.\n\n\nxs = np.random.choice(['a', 'b', 'c'], size=(100, 1))\nenc = OneHotEncoder().fit(xs)\ntransformed_xs = enc.transform(xs)\nassert np.all(enc.inverse_transform(transformed_xs) == xs)\n# Test from_dict and to_dict\nenc_1 = OneHotEncoder().from_dict(enc.to_dict())\nassert np.all(enc.transform(xs) == enc_1.transform(xs))\n\nxs = np.array(['a', 'b', 'c', np.nan, 'a', 'b', 'c', np.nan], dtype=object).reshape(-1, 1)\nenc = OneHotEncoder().fit(xs)\n# Check categories_\nassert np.array_equiv(enc.categories_, np.array(['a', 'b', 'c', np.nan], dtype=str)) \ntransformed_xs = enc.transform(xs)\nassert np.all(enc.inverse_transform(transformed_xs) == xs.astype(str))\nassert np.array_equal(\n    transformed_xs, skp.OneHotEncoder(sparse_output=False).fit_transform(xs)\n) \n# Test from_dict and to_dict\nenc_1 = OneHotEncoder().from_dict(enc.to_dict())\nenc_2 = OneHotEncoder()\nenc_2.from_dict(enc_1.to_dict())\nassert np.all(enc.transform(xs) == enc_1.transform(xs))\nassert np.all(enc.transform(xs) == enc_2.transform(xs))\n\nxs = np.random.choice(['a', 'b', 'c'], size=(100, ))\ntest_fail(lambda: OneHotEncoder().fit_transform(xs), \n    contains=\"OneHotEncoder only supports 2D array with a single feature\")",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Data Utils",
      "Data Preprocessors"
    ]
  },
  {
    "objectID": "explain.strategy.html",
    "href": "explain.strategy.html",
    "title": "Parallelism Strategy",
    "section": "",
    "text": "relax.strategy.BaseStrategy\n\n[source]\n\nclass relax.strategy.BaseStrategy ()\n\nBase class for mapping strategy.\n\nMethods\n[source]\n\ncall (fn, xs, pred_fn, y_targets, rng_keys, **kwargs)\n\nCall self as a function.\n\nParameters:\n\nfn (typing.Callable) – Function to generate cf for a single input\nxs (&lt;class 'jax.Array'&gt;) – Input instances to be explained\npred_fn (typing.Callable[[jax.Array], jax.Array])\ny_targets (&lt;class 'jax.Array'&gt;)\nrng_keys (typing.Iterable[PRNGKey])\nkwargs (VAR_KEYWORD)\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;) – Generated counterfactual explanations\n\n\n\nrelax.strategy.IterativeStrategy\n\n[source]\n\nclass relax.strategy.IterativeStrategy ()\n\nIterativly generate counterfactuals.\n\nMethods\n[source]\n\ncall (fn, xs, pred_fn, y_targets, rng_keys, **kwargs)\n\nCall self as a function.\n\nParameters:\n\nfn (typing.Callable) – Function to generate cf for a single input\nxs (&lt;class 'jax.Array'&gt;) – Input instances to be explained\npred_fn (typing.Callable[[jax.Array], jax.Array])\ny_targets (&lt;class 'jax.Array'&gt;)\nrng_keys (typing.Iterable[PRNGKey])\nkwargs (VAR_KEYWORD)\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;) – Generated counterfactual explanations\n\n\n\nrelax.strategy.VmapStrategy\n\n[source]\n\nclass relax.strategy.VmapStrategy ()\n\nGenerate counterfactuals via jax.vmap.\n\nMethods\n[source]\n\ncall (fn, xs, pred_fn, y_targets, rng_keys, **kwargs)\n\nCall self as a function.\n\nParameters:\n\nfn (typing.Callable) – Function to generate cf for a single input\nxs (&lt;class 'jax.Array'&gt;) – Input instances to be explained\npred_fn (typing.Callable[[jax.Array], jax.Array])\ny_targets (&lt;class 'jax.Array'&gt;)\nrng_keys (typing.Iterable[PRNGKey])\nkwargs (VAR_KEYWORD)\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;) – Generated counterfactual explanations\n\n\n\nrelax.strategy.PmapStrategy\n\n[source]\n\nclass relax.strategy.PmapStrategy (n_devices=None, strategy=‘auto’, **kwargs)\n\nBase class for mapping strategy.\n\nParameters:\n\nn_devices (&lt;class 'int'&gt;, default=None) – Number of devices. If None, use all available devices\nstrategy (&lt;class 'str'&gt;, default=auto) – Strategy to generate counterfactuals\nkwargs (VAR_KEYWORD)\n\n\n\nMethods\n[source]\n\ncall (fn, xs, pred_fn, y_targets, rng_keys, **kwargs)\n\nCall self as a function.\n\nParameters:\n\nfn (typing.Callable) – Function to generate cf for a single input\nxs (&lt;class 'jax.Array'&gt;) – Input instances to be explained\npred_fn (typing.Callable[[jax.Array], jax.Array])\ny_targets (&lt;class 'jax.Array'&gt;)\nrng_keys (typing.Iterable[PRNGKey])\nkwargs (VAR_KEYWORD)\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;) – Generated counterfactual explanations\n\n\n\nrelax.strategy.BatchedVmapStrategy\n\n[source]\n\nclass relax.strategy.BatchedVmapStrategy (batch_size)\n\nAuto-batching for generate counterfactuals via jax.vmap.\n\nMethods\n[source]\n\ncall (fn, xs, pred_fn, y_targets, rng_keys, **kwargs)\n\nCall self as a function.\n\nParameters:\n\nfn (typing.Callable) – Function to generate cf for a single input\nxs (&lt;class 'jax.Array'&gt;) – Input instances to be explained\npred_fn (typing.Callable[[jax.Array], jax.Array])\ny_targets (&lt;class 'jax.Array'&gt;)\nrng_keys (typing.Iterable[PRNGKey])\nkwargs (VAR_KEYWORD)\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;) – Generated counterfactual explanations\n\n\n\nrelax.strategy.BatchedPmapStrategy\n\n[source]\n\nclass relax.strategy.BatchedPmapStrategy (batch_size, n_devices=None)\n\nAuto-batching for generate counterfactuals via jax.vmap.\n\nMethods\n[source]\n\ncall (fn, xs, pred_fn, y_targets, rng_keys, **kwargs)\n\nCall self as a function.\n\nParameters:\n\nfn (typing.Callable) – Function to generate cf for a single input\nxs (&lt;class 'jax.Array'&gt;) – Input instances to be explained\npred_fn (typing.Callable[[jax.Array], jax.Array])\ny_targets (&lt;class 'jax.Array'&gt;)\nrng_keys (typing.Iterable[PRNGKey])\nkwargs (VAR_KEYWORD)\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;) – Generated counterfactual explanations\n\n\n\nos.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n\nw = jrand.normal(jrand.PRNGKey(0), (100, 100))\nxs = jrand.normal(jrand.PRNGKey(0), (1000, 100))\n\n@jit\ndef pred_fn(x): return jnp.dot(x, w.T)\n\ndef f(x, pred_fn=None, y_target=None, rng_key=None, **kwargs):\n    return pred_fn(x) + jrand.normal(rng_key, (1,))\n\nrng_keys = jrand.split(jrand.PRNGKey(0), 1000)\ny_targets = jnp.ones((1000, 100))\n\niter_gen = IterativeStrategy()\nvmap_gen = VmapStrategy()\npmap_gen = PmapStrategy()\nbvmap_gen = BatchedVmapStrategy(128)\nbpmap_gen = BatchedPmapStrategy(128)\n\n\ncf_iter = iter_gen(f, xs, pred_fn=pred_fn, y_targets=y_targets, rng_keys=rng_keys)\n\n\ncf_vmap = vmap_gen(f, xs, pred_fn=pred_fn, y_targets=y_targets, rng_keys=rng_keys)\n\n\ncf_pmap = pmap_gen(f, xs, pred_fn=pred_fn, y_targets=y_targets, rng_keys=rng_keys)\n\n\ncf_bvmap = bvmap_gen(f, xs, pred_fn=pred_fn, y_targets=y_targets, rng_keys=rng_keys)\n\n\ndef f_mul(x, pred_fn=None, **kwargs):\n    cf = pred_fn(x)\n    return einops.repeat(cf, 'k -&gt; c k', c=5)\n\n\ncf_iter = iter_gen(f_mul, xs, pred_fn=pred_fn, y_targets=y_targets, rng_keys=rng_keys)\ncf_vmap = vmap_gen(f_mul, xs, pred_fn=pred_fn, y_targets=y_targets, rng_keys=rng_keys)\ncf_pmap = pmap_gen(f_mul, xs, pred_fn=pred_fn, y_targets=y_targets, rng_keys=rng_keys)\ncf_bvmap = bvmap_gen(f_mul, xs, pred_fn=pred_fn, y_targets=y_targets, rng_keys=rng_keys)\ncf_bpmap = bpmap_gen(f_mul, xs, pred_fn=pred_fn, y_targets=y_targets, rng_keys=rng_keys)\n\nassert jnp.allclose(cf_iter, cf_vmap, atol=1e-4)\nassert jnp.allclose(cf_iter, cf_bvmap, atol=1e-4)\nassert jnp.allclose(cf_iter, cf_pmap, atol=1e-4)\nassert jnp.allclose(cf_iter, cf_bpmap, atol=1e-4)\nassert cf_bvmap.shape == (xs.shape[0], 5, xs.shape[1])\n\n\nrelax.strategy.StrategyFactory\n\n[source]\n\nclass relax.strategy.StrategyFactory ()\n\nFactory class for Parallelism Strategy.\n\nMethods\n[source]\n\nget_default_strategy ()\n\nGet default strategy.\n[source]\n\nget_strategy (strategy)\n\nGet strategy.\n\n\nit = StrategyFactory.get_strategy('iter')\nvm = StrategyFactory.get_strategy('vmap')\npm = StrategyFactory.get_strategy('pmap')\ndefault = StrategyFactory.get_default_strategy()\ncus = StrategyFactory.get_strategy(VmapStrategy())\n\nassert isinstance(it, IterativeStrategy)\nassert isinstance(vm, VmapStrategy)\nassert isinstance(pm, PmapStrategy)\nassert isinstance(default, VmapStrategy)\nassert isinstance(cus, VmapStrategy)",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Parallelism Strategy"
    ]
  },
  {
    "objectID": "data_utils/features.html",
    "href": "data_utils/features.html",
    "title": "Feature and Features List",
    "section": "",
    "text": "relax.data_utils.features.Feature\n\n[source]\n\nclass relax.data_utils.features.Feature (name, data, transformation, transformed_data=None, is_immutable=False, is_categorical=None)\n\nTHe feature class which represents a column in the dataset.\n\nfeat_cont = Feature(\n    name='continuous',\n    data=np.random.randn(100, 1),\n    transformation='minmax',\n    is_immutable=False,\n)\nassert feat_cont.transformed_data.shape == (100, 1)\nassert feat_cont.transformed_data.min() &gt;= 0\nassert feat_cont.transformed_data.max() &lt;= 1\nassert jnp.allclose(\n    feat_cont.inverse_transform(feat_cont.transformed_data), feat_cont.data)\nassert feat_cont.is_categorical is False\n\nfeat_cont_1 = feat_cont.with_transformed_data(feat_cont.transformed_data)\nassert isinstance(feat_cont_1, Feature)\nassert feat_cont_1 is not feat_cont\nassert np.allclose(\n    feat_cont_1.data, feat_cont.data\n)\nassert feat_cont.transformation.to_dict() == feat_cont_1.transformation.to_dict()\n\nfeat_cat = Feature(\n    name='category',\n    data=np.random.choice(['a', 'b', 'c'], size=(100, 1)),\n    transformation='ohe',\n    is_immutable=False,\n)\nassert feat_cat.transformed_data.shape == (100, 3)\nassert np.all(feat_cat.inverse_transform(feat_cat.transformed_data) == feat_cat.data)\nassert feat_cat.is_categorical\n\nfeat_cat_1 = feat_cat.with_transformed_data(jax.nn.one_hot(jnp.array([0, 1, 2, 0, 1, 2]), 3))\nassert feat_cat_1 is not feat_cat\nassert np.array_equal(\n    feat_cat_1.data, np.array(['a', 'b', 'c', 'a', 'b', 'c']).reshape(-1, 1)\n) \n\n# Test serialization\nd = feat_cont.to_dict()\nfeat_cont_1 = Feature.from_dict(d)\nassert feat_cont_1.name == feat_cont.name\nassert np.allclose(feat_cont_1.data, feat_cont.data)\nassert np.allclose(feat_cont_1.transformed_data, feat_cont.transformed_data)\nassert feat_cont_1.is_immutable == feat_cont.is_immutable\n\n\n# Test set_transformation\nfeat_cat = Feature(\n    name='category',\n    data=np.random.choice(['a', 'b', 'c'], size=(100, 1)),\n    transformation='ohe',\n    is_immutable=False,\n)\nassert feat_cat.transformation.name == 'ohe'\nassert feat_cat.transformed_data.shape == (100, 3)\nfeat_cat.set_transformation('ordinal')\nassert feat_cat.transformation.name == 'ordinal'\nassert feat_cat.is_categorical\nassert feat_cat.transformed_data.shape == (100, 1)\nassert feat_cat.is_immutable is False\n\n\nrelax.data_utils.features.FeaturesList\n\n[source]\n\nclass relax.data_utils.features.FeaturesList (features, *args, **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ndf = pd.read_csv('../assets/adult/data/data.csv')\ncont_feats = ['age', 'hours_per_week']\ncat_feats = [\"workclass\", \"education\", \"marital_status\",\"occupation\", \"race\", \"gender\"]\n\nfeats_list = FeaturesList([\n    Feature(name, df[name].to_numpy().reshape(-1, 1), 'minmax') for name in cont_feats\n] + [\n    Feature(name, df[name].to_numpy().reshape(-1, 1), 'ohe') for name in cat_feats\n])\nassert feats_list.transformed_data.shape == (32561, 29)\n\n\n# test __get_item__\nassert np.allclose(\n    feats_list['age'].transformed_data,\n    feats_list.transformed_data[:, 0:1]\n)\nassert np.allclose(\n    FeaturesList(feats_list[['age', 'hours_per_week', 'workclass']]).transformed_data,\n    feats_list.transformed_data[:, :6]\n)\n\n\n# Test with_transformed_data\ntransformed_xs = feats_list.transformed_data\nindices = np.random.choice(len(transformed_xs), size=100)\nfeats_list_1 = feats_list.with_transformed_data(transformed_xs[indices])\n\npd.testing.assert_frame_equal(\n    feats_list.to_pandas().iloc[indices].reset_index(drop=True),\n    feats_list_1.to_pandas(),\n    check_exact=False,\n    check_dtype=False,\n    check_index_type=False\n)\n\n\ndef test_set_transformations(transformation, correct_shape):\n    T = transformation\n    feats_list_2 = deepcopy(feats_list)\n    feats_list_2.set_transformations({\n        feat: T for feat in cat_feats\n    })\n    assert feats_list_2.transformed_data.shape == correct_shape\n    name = T.name if isinstance(T, BaseTransformation) else T\n\n    for feat in feats_list_2:\n        if feat.name in cat_feats:  \n            assert feat.transformation.name == name\n            assert feat.is_categorical\n        else:\n            assert feat.transformation.name == 'minmax'                       \n            assert feat.is_categorical is False\n        assert feat.is_immutable is False\n\n    x = jax.random.uniform(jax.random.PRNGKey(0), shape=(100, correct_shape[-1]))\n    _ = feats_list_2.apply_constraints(feats_list_2.transformed_data[:100], x, hard=False)\n    _ = feats_list_2.apply_constraints(feats_list_2.transformed_data[:100], x, hard=True)\n\n\ntest_set_transformations('ordinal', (32561, 8))\ntest_set_transformations('ohe', (32561, 29))\ntest_set_transformations('gumbel', (32561, 29))\n# TODO: [bug] raise error when set_transformations is called with \n# SoftmaxTransformation() or GumbelSoftmaxTransformation(),\n# instead of \"ohe\" or \"gumbel\".\ntest_set_transformations(SoftmaxTransformation(), (32561, 29))\ntest_set_transformations(GumbelSoftmaxTransformation(), (32561, 29))\n\n\n# Test transform and inverse_transform\n# Convert df to dict[str, np.ndarray]\ndf_dict = {k: np.array(v).reshape(-1, 1) for k, v in df.iloc[:, :-1].to_dict(orient='list').items()}\n# feats_list.transform(df_dict) should be the same as feats_list.transformed_data\ntransformed_data = feats_list.transform(df_dict)\nassert np.equal(feats_list.transformed_data, transformed_data).all()\n# feats_list.inverse_transform(transformed_data) should be the same as df_dict\ninverse_transformed_data = feats_list.inverse_transform(transformed_data)\npd.testing.assert_frame_equal(\n    pd.DataFrame.from_dict({k: v.reshape(-1) for k, v in inverse_transformed_data.items()}),\n    pd.DataFrame.from_dict({k: v.reshape(-1) for k, v in df_dict.items()}),\n    check_dtype=False, check_exact=False,\n)\n\n\n# Test apply_constraints and compute_reg_loss\nx = np.random.randn(10, 29)\nconstraint_cfs = feats_list.apply_constraints(feats_list.transformed_data[:10, :], x, hard=False)\nassert constraint_cfs.shape == (10, 29)\nassert np.allclose(\n    constraint_cfs[:, 2:].sum(axis=-1),\n    np.ones((10,)) * 6\n)\nassert constraint_cfs[: :2].min() &gt;= 0 and constraint_cfs[: :2].max() &lt;= 1\nassert feats_list.apply_constraints(feats_list.transformed_data[:10, :], x, hard=True).shape == (10, 29)\n\nreg_loss = feats_list.compute_reg_loss(feats_list.transformed_data, x)\nassert jnp.ndim(reg_loss) == 0\nassert np.all(reg_loss &gt; 0)\nassert np.allclose(feats_list.compute_reg_loss(x, constraint_cfs), 0)\n\n\n# Test `to_pandas`\nfeats_pd = feats_list.to_pandas()\npd.testing.assert_frame_equal(\n    feats_pd,\n    pd.DataFrame.from_dict({k: v.reshape(-1) for k, v in df_dict.items()}),\n    check_dtype=False,\n)\n\n\n# Test save and load\nfeats_list.save('tmp/data_module/')\nfeats_list_1 = FeaturesList.load_from_path('tmp/data_module/')\n# remove tmp folder\nshutil.rmtree('tmp/data_module/')\n\n\nsk_ohe = skp.OneHotEncoder(sparse_output=False)\nsk_minmax = skp.MinMaxScaler()\n\n# for feat in feats_list.features:\nfor feat in feats_list:\n    if feat.name in cont_feats:\n        assert np.allclose(\n            sk_minmax.fit_transform(feat.data),\n            feat.transformed_data,\n        ), f\"Failed at {feat.name}. \"\n    else:\n        assert np.allclose(\n            sk_ohe.fit_transform(feat.data),\n            feat.transformed_data,\n        ), f\"Failed at {feat.name}\"",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Data Utils",
      "Feature and Features List"
    ]
  },
  {
    "objectID": "03_explain.html",
    "href": "03_explain.html",
    "title": "jax-relax",
    "section": "",
    "text": "[source]\n\nclass relax.explain.Explanation (cfs, pred_fn, data_module=None, xs=None, ys=None, total_time=None, cf_name=‘CFModule’, data=None)\n\nGenerated CF Explanations class. It inherits a DataModule.\n\nParameters:\n\ncfs (&lt;class 'jax.Array'&gt;) – Generated cf explanation of xs in data\npred_fn (typing.Callable[[jax.Array], jax.Array]) – Predict function\ndata_module (&lt;class 'relax.data_module.DataModule'&gt;, default=None) – Data module\nxs (&lt;class 'jax.Array'&gt;, default=None) – Input data\nys (&lt;class 'jax.Array'&gt;, default=None) – Target data\ntotal_time (&lt;class 'float'&gt;, default=None) – Total runtime\ncf_name (&lt;class 'str'&gt;, default=CFModule) – CF method’s name\ndata (&lt;class 'NoneType'&gt;, default=None) – Deprecated argument\n\n\n\nMethods\n[source]\n\ncopy ()\n\nReturn a deep copy of the explanation.\nWarning: this method will not create a deepcopy of pred_fn.\n[source]\n\nsave (path)\n\nSave the explanation to a directory.\n[source]\n\nload_from_path (path, ml_module_path=None)\n\nLoad DataModule from a directory.\n\n\n[source]\n\nrelax.explain.fake_explanation (n_cfs=1)\n\n\nexp = fake_explanation(n_cfs=1)\nxs_shape = exp.xs.shape\nassert exp.cfs.shape == (xs_shape[0], 1, xs_shape[-1])\ntrain_exp = exp['train']\nval_exp = exp['val']\ntest_exp = exp['test']\nassert jnp.concatenate(\n    [train_exp['cfs'], val_exp['cfs']], axis=0\n).shape == exp.cfs.shape\nassert test_exp['cfs'].shape == val_exp['cfs'].shape\n\nexp = fake_explanation(n_cfs=5)\nassert exp.cfs.shape == (xs_shape[0], 5, xs_shape[-1])\n\n\nexp.save('tmp/exp/')\nexp = Explanation.load_from_path('tmp/exp/', \n    ml_module_path='relax-assets/dummy/model/')\n\n\nexp_1 = exp.copy()\nassert exp_1 is not exp\nassert np.array_equal(exp_1.cfs, exp.cfs)",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Explanation"
    ]
  },
  {
    "objectID": "03_explain.html#explanation",
    "href": "03_explain.html#explanation",
    "title": "jax-relax",
    "section": "",
    "text": "[source]\n\nclass relax.explain.Explanation (cfs, pred_fn, data_module=None, xs=None, ys=None, total_time=None, cf_name=‘CFModule’, data=None)\n\nGenerated CF Explanations class. It inherits a DataModule.\n\nParameters:\n\ncfs (&lt;class 'jax.Array'&gt;) – Generated cf explanation of xs in data\npred_fn (typing.Callable[[jax.Array], jax.Array]) – Predict function\ndata_module (&lt;class 'relax.data_module.DataModule'&gt;, default=None) – Data module\nxs (&lt;class 'jax.Array'&gt;, default=None) – Input data\nys (&lt;class 'jax.Array'&gt;, default=None) – Target data\ntotal_time (&lt;class 'float'&gt;, default=None) – Total runtime\ncf_name (&lt;class 'str'&gt;, default=CFModule) – CF method’s name\ndata (&lt;class 'NoneType'&gt;, default=None) – Deprecated argument\n\n\n\nMethods\n[source]\n\ncopy ()\n\nReturn a deep copy of the explanation.\nWarning: this method will not create a deepcopy of pred_fn.\n[source]\n\nsave (path)\n\nSave the explanation to a directory.\n[source]\n\nload_from_path (path, ml_module_path=None)\n\nLoad DataModule from a directory.\n\n\n[source]\n\nrelax.explain.fake_explanation (n_cfs=1)\n\n\nexp = fake_explanation(n_cfs=1)\nxs_shape = exp.xs.shape\nassert exp.cfs.shape == (xs_shape[0], 1, xs_shape[-1])\ntrain_exp = exp['train']\nval_exp = exp['val']\ntest_exp = exp['test']\nassert jnp.concatenate(\n    [train_exp['cfs'], val_exp['cfs']], axis=0\n).shape == exp.cfs.shape\nassert test_exp['cfs'].shape == val_exp['cfs'].shape\n\nexp = fake_explanation(n_cfs=5)\nassert exp.cfs.shape == (xs_shape[0], 5, xs_shape[-1])\n\n\nexp.save('tmp/exp/')\nexp = Explanation.load_from_path('tmp/exp/', \n    ml_module_path='relax-assets/dummy/model/')\n\n\nexp_1 = exp.copy()\nassert exp_1 is not exp\nassert np.array_equal(exp_1.cfs, exp.cfs)",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Explanation"
    ]
  },
  {
    "objectID": "03_explain.html#generate-explanations",
    "href": "03_explain.html#generate-explanations",
    "title": "jax-relax",
    "section": "Generate Explanations",
    "text": "Generate Explanations\n\nrelax.explain.prepare_rng_keys\n\n[source]\n\nrelax.explain.prepare_rng_keys (rng_key, n_instances)\n\nPrepare random number generator keys.\n\nrelax.explain.prepare_cf_module\n\n[source]\n\nrelax.explain.prepare_cf_module (cf_module, data_module, pred_fn=None, train_config=None)\n\nPrepare the CF module. It will hook up the data module, and its apply functions via the init_apply_fns method (e.g., apply_constraints_fn and compute_reg_loss_fn). Next, it will train the model if cf_module is a ParametricCFModule. Finally, it will call before_generate_cf method.\n\nrelax.explain.prepare_pred_fn\n\n[source]\n\nrelax.explain.prepare_pred_fn (cf_module, data, pred_fn, pred_fn_args=None)\n\nPrepare the predictive function for the CF module. We will train the model if pred_fn is not provided and cf_module does not have pred_fn. If pred_fn is found in cf_module, we will use it irrespective of pred_fn argument. If pred_fn is provided, we will use it.\n\nParameters:\n\ncf_module (&lt;class 'relax.methods.base.CFModule'&gt;)\ndata (&lt;class 'relax.data_module.DataModule'&gt;)\npred_fn (typing.Callable[[jax.Array, ...], jax.Array]) – Predictive function.\npred_fn_args (typing.Dict, default=None)\n\n\n\nReturns:\n    (typing.Callable[[jax.Array], jax.Array]) – Return predictive function with signature (x: Array) -&gt; Array.\n\n\nrelax.explain.generate_cf_explanations\n\n[source]\n\nrelax.explain.generate_cf_explanations (cf_module, data, pred_fn=None, strategy=None, train_config=None, pred_fn_args=None, rng_key=None)\n\nGenerate CF explanations.\n\nParameters:\n\ncf_module (&lt;class 'relax.methods.base.CFModule'&gt;) – CF Explanation Module\ndata (&lt;class 'relax.data_module.DataModule'&gt;) – Data Module\npred_fn (typing.Callable[[jax.Array, ...], jax.Array], default=None) – Predictive function\nstrategy (str | relax.strategy.BaseStrategy, default=None) – Parallelism Strategy for generating CFs. Default to vmap.\ntrain_config (typing.Dict[str, typing.Any], default=None)\npred_fn_args (&lt;class 'dict'&gt;, default=None) – auxiliary arguments for pred_fn\nrng_key (&lt;function PRNGKey at 0x7f08b3fbdea0&gt;, default=None) – Random number generator key\n\n\n\nReturns:\n    (&lt;class '__main__.Explanation'&gt;) – Return counterfactual explanations.\n\n\ndm = load_data(\"adult\")\nml_model = load_ml_module(\"adult\")\n\n\nexps = generate_cf_explanations(\n    VanillaCF(),\n    dm, ml_model.pred_fn,\n)\n\n\n\n\n/tmp/ipykernel_5475/4129963786.py:17: DeprecationWarning: Argument `data` is deprecated. Use `data_module` instead.\n  warnings.warn(\n\n\n\ncfnet = CounterNet()\ncfnet.train(dm, epochs=1)\n# Test cases for checking if ParametricCFModule is trained twice.\n# If it is trained twice, cfs will be different.\ncfs = jax.vmap(cfnet.generate_cf)(dm.xs)\nassert cfnet.is_trained == True\nexp = generate_cf_explanations(cfnet, dm)\nassert np.allclose(einops.rearrange(exp.cfs, 'N 1 K -&gt; N K'), cfs)\n\n/home/birk/miniconda3/envs/dev/lib/python3.10/site-packages/relax/legacy/ckpt_manager.py:47: UserWarning: `monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\n  warnings.warn(\nEpoch 0: 100%|██████████| 191/191 [00:08&lt;00:00, 22.21batch/s, train/train_loss_1=0.06329722, train/train_loss_2=0.07011371, train/train_loss_3=0.101814255]   \n/tmp/ipykernel_5475/4129963786.py:17: DeprecationWarning: Argument `data` is deprecated. Use `data_module` instead.\n  warnings.warn(\n\n\n\n# hide\n# dm = load_data(\"dummy\")\n# ml_model = load_ml_module(\"dummy\")\n\n# for cf_module in [CounterNet, CCHVAE, VAECF, L2C, ProtoCF, CLUE]:\n#     m = cf_module()\n#     assert m.is_trained == False\n#     m.train(dm, pred_fn=ml_model.pred_fn, epochs=1)\n#     assert m.is_trained == True\n#     exp = generate_cf_explanations(m, dm, pred_fn=ml_model.pred_fn)",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Explanation"
    ]
  },
  {
    "objectID": "legacy/logger.html",
    "href": "legacy/logger.html",
    "title": "Logger",
    "section": "",
    "text": "relax.legacy.logger.Logger\n\n[source]\n\nclass relax.legacy.logger.Logger (log_dir, name, on_step=False)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n# logger = TensorboardLogger('log', name='debug')\nlogger = Logger('log', name='debug')\nlogger.save_hyperparams({'lr': 0.01})\nlogger.log_dict({'train/train_loss_1': 0.1, 'epoch': 0})\nlogger.log_dict({'train/train_loss_1': 0.1})\nlogger.log_dict({'train/train_loss_1': 0.05})\n\nlogger.log_dict({'train/train_loss_1': 0.07, 'epoch': 1})\nlogger.log_dict({'train/train_loss_1': 0.05, })\nlogger.log_dict({'train/train_loss_1': 0.05, 'epoch': 2})\n\nlogger.close()"
  },
  {
    "objectID": "legacy/ckpt_manager.html",
    "href": "legacy/ckpt_manager.html",
    "title": "Checkpoint Manager",
    "section": "",
    "text": "relax.legacy.ckpt_manager.load_checkpoint\n\n[source]\n\nrelax.legacy.ckpt_manager.load_checkpoint (ckpt_dir)\n\n\nrelax.legacy.ckpt_manager.save_checkpoint\n\n[source]\n\nrelax.legacy.ckpt_manager.save_checkpoint (state, ckpt_dir)\n\n\nrelax.legacy.ckpt_manager.CheckpointManager\n\n[source]\n\nclass relax.legacy.ckpt_manager.CheckpointManager (log_dir, monitor_metrics, max_n_checkpoints=3)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nExample\n\nfrom relax.legacy.module import MLP\nfrom relax.legacy.utils import make_hk_module\n\n\nmodule = make_hk_module(MLP, sizes=[10, 10])\n\n\nparams = module.init(jrand.PRNGKey(0), jnp.ones((1, 2)))\nopt_state = optax.adam(1e-3).init(params)\n\n\nckpt_manager = CheckpointManager(\n    log_dir='log', \n    monitor_metrics='train/train_loss_1',\n    max_n_checkpoints=3\n)\n\n# module = PredictiveTrainingModule({'lr': 0.01, 'sizes': [50, 10, 50]})\n# params, opt_state = module.init_net_opt(dm, next(key))\nlogs = {'train/train_loss_1': 0.1}\nckpt_manager.update_checkpoints(params, opt_state, logs, epochs=1)\nlogs = {'train/train_loss_1': 0.2}\nckpt_manager.update_checkpoints(params, opt_state, logs, epochs=2)\nlogs = {'train/train_loss_1': 0.15}\nckpt_manager.update_checkpoints(params, opt_state, logs, epochs=3)\nlogs = {'train/train_loss_1': 0.05}\nckpt_manager.update_checkpoints(params, opt_state, logs, epochs=4)\nlogs = {'train/train_loss_1': 0.14}\nckpt_manager.update_checkpoints(params, opt_state, logs, epochs=5)\nassert ckpt_manager.n_checkpoints == len(ckpt_manager.checkpoints)\nassert ckpt_manager.checkpoints.popitem(last=True)[0] == 0.14\n\nshutil.rmtree(Path('log/epoch=1'), ignore_errors=True)\nshutil.rmtree(Path('log/epoch=2'), ignore_errors=True)\nshutil.rmtree(Path('log/epoch=3'), ignore_errors=True)\nshutil.rmtree(Path('log/epoch=4'), ignore_errors=True)\nshutil.rmtree(Path('log/epoch=5'), ignore_errors=True)"
  },
  {
    "objectID": "base.html",
    "href": "base.html",
    "title": "Base APIs",
    "section": "",
    "text": "relax.base.BaseConfig\n\n[source]\n\nclass relax.base.BaseConfig ()\n\nBase class for all config classes.\n\nclass ConfigTest(BaseConfig):\n    a: int = 1\n    b: str = 'b'\n    c: float = 3.14\n\nconf = ConfigTest()\nconf.save('test.json')\nconf2 = ConfigTest.load_from_json('test.json')\nassert conf == conf2\n# remove test.json\nos.remove('test.json')\n\nconf = ConfigTest()\nconf.save('tmp/test.json')\nconf2 = ConfigTest.load_from_json('tmp/test.json')\nassert conf == conf2\nos.remove('tmp/test.json')\n\ntest_fail(lambda: conf.save('test'), contains=\"Path must end with `.json`,\")\ntest_fail(lambda: ConfigTest.load_from_json('test.json'), contains=\"File not found\")\n\n\nrelax.base.BaseModule\n\n[source]\n\nclass relax.base.BaseModule (config, name=None)\n\nBase class for all modules.\n\nclass TestModule(BaseModule):\n    def save(self, path):\n        self.config.save(Path(path) / 'config.json')\n\n    def load_from_path(self, path):\n        self.config = ConfigTest.load_from_json(Path(path) / 'config.json')\n\nconf = ConfigTest()\nmodule = TestModule(conf)\nassert module.name == 'TestModule'\nmodule.save('tmp/module/')\nmodule.load_from_path('tmp/module/')\nassert module.config == conf\nshutil.rmtree('tmp/module/')\n\n\nrelax.base.PredFnMixedin\n\n[source]\n\nclass relax.base.PredFnMixedin ()\n\nMixin class for modules that have a pred_fn method.\n\nMethods\n[source]\n\npred_fn (x)\n\nReturn the prediction/probability of the model on x.\n\n\nrelax.base.TrainableMixedin\n\n[source]\n\nclass relax.base.TrainableMixedin ()\n\nMixin class for trainable modules.\n\nMethods\n[source]\n\nis_trained ()\n\nReturn whether the module is trained or not.\n[source]\n\ntrain (data, **kwargs)\n\nTrain the module.",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Base APIs"
    ]
  },
  {
    "objectID": "docs.html",
    "href": "docs.html",
    "title": "Docs",
    "section": "",
    "text": "relax.docs.ListDocment\n\n[source]\n\nclass relax.docs.ListDocment (tbl)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nrelax.docs.CustomizedMarkdownRenderer\n\n[source]\n\nclass relax.docs.CustomizedMarkdownRenderer (sym, name=None, title_level=3)\n\nDisplaying documents of functions, classes, haiku.module, and BaseParser.\nCustomizedMarkdownRenderer is the customized markdown render for the ReLax documentation site. We can use it to displaying documents of functions, classes, haiku.module, and BaseParser.\nWe can display documentations for functions:\n\ndef validate_config(\n    configs: Dict|BaseParser, # A configuration of the model/data.\n    config_cls: BaseParser # The desired configuration class.\n) -&gt; BaseParser:\n    \"\"\"Return a valid configuration object.\"\"\"\n    ...\n\nCustomizedMarkdownRenderer(validate_config)\n\n\nvalidate_config\n\n\nvalidate_config (configs, config_cls)\n\nReturn a valid configuration object.\n\nParameters:\n\nconfigs (Dict | BaseParser) – A configuration of the model/data.\nconfig_cls (BaseParser) – The desired configuration class.\n\n\n\nReturns:\n    (BaseParser)\n\n\n\nWe can display documentations for classes:\n\nclass VanillaCF:\n    \"\"\"VanillaCF Explanation of the model.\"\"\"\n\n    def __init__(\n        self, \n        configs: Dict|BaseParser=None # A configuration of the model.\n    ): ...\n\n    def generate_cf(\n        self,\n        x: np.ndarray, # A data point.\n        pred_fn: Callable, # A prediction function.\n    ) -&gt; Array:\n        \"\"\"Generate counterfactuals for the given data point.\"\"\"\n        pass\n\n    __ALL__ = [\"generate_cf\"]\nCustomizedMarkdownRenderer(VanillaCF)\n\n\nrelax.methods.vanilla.VanillaCF\n\n[source]\n\nclass relax.methods.vanilla.VanillaCF (configs=None)\n\nVanillaCF Explanation of the model.\n\nParameters:\n\nconfigs (Dict | BaseParser, default=None) – A configuration of the model.\n\n\n\nMethods\n[source]\n\ngenerate_cf (x, pred_fn)\n\nGenerate counterfactuals for the given data point.\n\nParameters:\n\nx (np.ndarray) – A data point.\npred_fn (Callable) – A prediction function.\n\n\n\nReturns:\n    (Array)\n\n\n\n\n\nWe can display documentations for BaseParser:\n\nclass VanillaCFConfig(BaseParser):\n    \"\"\"Configuration for the `Model`.\"\"\"\n    pass\n\nCustomizedMarkdownRenderer(VanillaCFConfig)\n\n\nrelax.methods.vanilla.VanillaCFConfig\n\n[source]\n\nclass relax.methods.vanilla.VanillaCFConfig ()\n\nConfiguration for the Model.\n\n\n\nclass VanillaCFConfig(BaseParser):\n    \"\"\"Configuration for the `Model`.\"\"\"\n\n    lr: float = Field(1e-3, description=\"Learning rate.\")\n    n_steps: int = Field(100, description=\"Number of iteration steps.\")\n\nCustomizedMarkdownRenderer(VanillaCFConfig)\n\n\nrelax.methods.vanilla.VanillaCFConfig\n\n[source]\n\nclass relax.methods.vanilla.VanillaCFConfig (lr=0.001, n_steps=100)\n\nConfiguration for the Model.\n\nParameters:\n\nlr (float, default=0.001) – Learning rate.\nn_steps (int, default=100) – Number of iteration steps.",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Docs"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ReLax",
    "section": "",
    "text": "Overview | Installation | Tutorials | Documentation | Citing ReLax",
    "crumbs": [
      "Overview",
      "ReLax"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "ReLax",
    "section": "Overview",
    "text": "Overview\nReLax (Recourse Explanation Library in Jax) is an efficient and scalable benchmarking library for recourse and counterfactual explanations, built on top of jax. By leveraging language primitives such as vectorization, parallelization, and just-in-time compilation in jax, ReLax offers massive speed improvements in generating individual (or local) explanations for predictions made by Machine Learning algorithms.\nSome of the key features are as follows:\n\n🏃 Fast and scalable recourse generation.\n🚀 Accelerated over cpu, gpu, tpu.\n🪓 Comprehensive set of recourse methods implemented for benchmarking.\n👐 Customizable API to enable the building of entire modeling and interpretation pipelines for new recourse algorithms.",
    "crumbs": [
      "Overview",
      "ReLax"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "ReLax",
    "section": "Installation",
    "text": "Installation\npip install jax-relax\n# Or install the latest version of `jax-relax`\npip install git+https://github.com/BirkhoffG/jax-relax.git \nTo futher unleash the power of accelerators (i.e., GPU/TPU), we suggest to first install this library via pip install jax-relax. Then, follow steps in the official install guidelines to install the right version for GPU or TPU.",
    "crumbs": [
      "Overview",
      "ReLax"
    ]
  },
  {
    "objectID": "index.html#dive-into-relax",
    "href": "index.html#dive-into-relax",
    "title": "ReLax",
    "section": "Dive into ReLax",
    "text": "Dive into ReLax\nReLax is a recourse explanation library for explaining (any) JAX-based ML models. We believe that it is important to give users flexibility to choose how to use ReLax. You can\n\nonly use methods implemeted in ReLax (as a recourse methods library);\nbuild a pipeline using ReLax to define data module, training ML models, and generating CF explanation (for constructing recourse benchmarking pipeline).\n\n\nReLax as a Recourse Explanation Library\nWe introduce basic use cases of using methods in ReLax to generate recourse explanations. For more advanced usages of methods in ReLax, See this tutorials.\n\nfrom relax.methods import VanillaCF\nfrom relax import DataModule, MLModule, generate_cf_explanations, benchmark_cfs\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nimport functools as ft\nimport jax\n\nLet’s first generate synthetic data:\n\nxs, ys = make_classification(n_samples=1000, n_features=10, random_state=42)\ntrain_xs, test_xs, train_ys, test_ys = train_test_split(xs, ys, random_state=42)\n\nNext, we fit an MLP model for this data. Note that this model can be any model implmented in JAX. We will use the MLModule in ReLax as an example.\n\nmodel = MLModule()\nmodel.train((train_xs, train_ys), epochs=10, batch_size=64)\n\nGenerating recourse explanations are straightforward. We can simply call generate_cf of an implemented recourse method to generate one recourse explanation:\n\nvcf = VanillaCF(config={'n_steps': 1000, 'lr': 0.05})\ncf = vcf.generate_cf(test_xs[0], model.pred_fn)\nassert cf.shape == test_xs[0].shape\n\nOr generate a bunch of recourse explanations with jax.vmap:\n\ngenerate_fn = ft.partial(vcf.generate_cf, pred_fn=model.pred_fn)\ncfs = jax.vmap(generate_fn)(test_xs)\nassert cfs.shape == test_xs.shape\n\n\n\nReLax for Building Recourse Explanation Pipelines\nThe above example illustrates the usage of the decoupled relax.methods to generate recourse explanations. However, users are required to write boilerplate code for tasks such as data preprocessing, model training, and generating recourse explanations with feature constraints.\nReLax additionally offers a one-liner framework, streamlining the process and helping users in building a standardized pipeline for generating recourse explanations. You can write three lines of code to benchmark recourse explanations:\n\ndata_module = DataModule.from_numpy(xs, ys)\nexps = generate_cf_explanations(vcf, data_module, model.pred_fn)\nbenchmark_cfs([exps])\n\nSee Getting Started with ReLax for an end-to-end example of using ReLax.",
    "crumbs": [
      "Overview",
      "ReLax"
    ]
  },
  {
    "objectID": "index.html#supported-recourse-methods",
    "href": "index.html#supported-recourse-methods",
    "title": "ReLax",
    "section": "Supported Recourse Methods",
    "text": "Supported Recourse Methods\nReLax currently provides implementations of 9 recourse explanation methods.\n\n\n\n\n\n\n\n\n\nMethod\nType\nPaper Title\nRef\n\n\n\n\nVanillaCF\nNon-Parametric\nCounterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR.\n[1]\n\n\nDiverseCF\nNon-Parametric\nExplaining Machine Learning Classifiers through Diverse Counterfactual Explanations.\n[2]\n\n\nProtoCF\nSemi-Parametric\nInterpretable Counterfactual Explanations Guided by Prototypes.\n[3]\n\n\nCounterNet\nParametric\nCounterNet: End-to-End Training of Prediction Aware Counterfactual Explanations.\n[4]\n\n\nGrowingSphere\nNon-Parametric\nInverse Classification for Comparison-based Interpretability in Machine Learning.\n[5]\n\n\nCCHVAE\nSemi-Parametric\nLearning Model-Agnostic Counterfactual Explanations for Tabular Data.\n[6]\n\n\nVAECF\nParametric\nPreserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers.\n[7]\n\n\nCLUE\nSemi-Parametric\nGetting a CLUE: A Method for Explaining Uncertainty Estimates.\n[8]\n\n\nL2C\nParametric\nFeature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations\n[9]",
    "crumbs": [
      "Overview",
      "ReLax"
    ]
  },
  {
    "objectID": "index.html#citing-relax",
    "href": "index.html#citing-relax",
    "title": "ReLax",
    "section": "Citing ReLax",
    "text": "Citing ReLax\nTo cite this repository:\n@software{relax2023github,\n  author = {Hangzhi Guo and Xinchang Xiong and Amulya Yadav},\n  title = {{R}e{L}ax: Recourse Explanation Library in Jax},\n  url = {http://github.com/birkhoffg/jax-relax},\n  version = {0.2.0},\n  year = {2023},\n}",
    "crumbs": [
      "Overview",
      "ReLax"
    ]
  },
  {
    "objectID": "methods/cchvae.html",
    "href": "methods/cchvae.html",
    "title": "CCHVAE",
    "section": "",
    "text": "relax.methods.cchvae.CHVAE\n[source]\nA model grouping layers into an object with training/inference features.\nThere are three ways to instantiate a Model:",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "CCHVAE"
    ]
  },
  {
    "objectID": "methods/cchvae.html#with-the-functional-api",
    "href": "methods/cchvae.html#with-the-functional-api",
    "title": "CCHVAE",
    "section": "With the “Functional API”",
    "text": "With the “Functional API”\nYou start from Input, you chain layer calls to specify the model’s forward pass, and finally you create your model from inputs and outputs:\ninputs = keras.Input(shape=(37,))\nx = keras.layers.Dense(32, activation=\"relu\")(inputs)\noutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nNote: Only dicts, lists, and tuples of input tensors are supported. Nested inputs are not supported (e.g. lists of list or dicts of dict).\nA new Functional API model can also be created by using the intermediate tensors. This enables you to quickly extract sub-components of the model.\nExample:\ninputs = keras.Input(shape=(None, None, 3))\nprocessed = keras.layers.RandomCrop(width=128, height=128)(inputs)\nconv = keras.layers.Conv2D(filters=32, kernel_size=3)(processed)\npooling = keras.layers.GlobalAveragePooling2D()(conv)\nfeature = keras.layers.Dense(10)(pooling)\n\nfull_model = keras.Model(inputs, feature)\nbackbone = keras.Model(processed, conv)\nactivations = keras.Model(conv, feature)\nNote that the backbone and activations models are not created with keras.Input objects, but with the tensors that originate from keras.Input objects. Under the hood, the layers and weights will be shared across these models, so that user can train the full_model, and use backbone or activations to do feature extraction. The inputs and outputs of the model can be nested structures of tensors as well, and the created models are standard Functional API models that support all the existing APIs.",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "CCHVAE"
    ]
  },
  {
    "objectID": "methods/cchvae.html#by-subclassing-the-model-class",
    "href": "methods/cchvae.html#by-subclassing-the-model-class",
    "title": "CCHVAE",
    "section": "By subclassing the Model class",
    "text": "By subclassing the Model class\nIn that case, you should define your layers in __init__() and you should implement the model’s forward pass in call().\nclass MyModel(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n\n    def call(self, inputs):\n        x = self.dense1(inputs)\n        return self.dense2(x)\n\nmodel = MyModel()\nIf you subclass Model, you can optionally have a training argument (boolean) in call(), which you can use to specify a different behavior in training and inference:\nclass MyModel(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n        self.dropout = keras.layers.Dropout(0.5)\n\n    def call(self, inputs, training=False):\n        x = self.dense1(inputs)\n        x = self.dropout(x, training=training)\n        return self.dense2(x)\n\nmodel = MyModel()\nOnce the model is created, you can config the model with losses and metrics with model.compile(), train the model with model.fit(), or use the model to do prediction with model.predict().",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "CCHVAE"
    ]
  },
  {
    "objectID": "methods/cchvae.html#with-the-sequential-class",
    "href": "methods/cchvae.html#with-the-sequential-class",
    "title": "CCHVAE",
    "section": "With the Sequential class",
    "text": "With the Sequential class\nIn addition, keras.Sequential is a special case of model where the model is purely a stack of single-input, single-output layers.\nmodel = keras.Sequential([\n    keras.Input(shape=(None, None, 3)),\n    keras.layers.Conv2D(filters=32, kernel_size=3),\n])\n\nrelax.methods.cchvae.CCHVAEConfig\n\n[source]\n\nclass relax.methods.cchvae.CCHVAEConfig (vae_layers=[20, 16, 14, 12], opt_name=‘adam’, vae_lr=0.001, max_steps=100, n_search_samples=100, step_size=0.1)\n\nBase class for all config classes.\n\nParameters:\n\nvae_layers (List[int], default=[20, 16, 14, 12]) – List of hidden layer sizes for VAE.\nopt_name (str, default=adam) – Optimizer name of VAE.\nvae_lr (float, default=0.001) – Learning rate of VAE.\nmax_steps (int, default=100) – Max steps\nn_search_samples (int, default=100) – Number of generated candidate counterfactuals.\nstep_size (float, default=0.1) – Step size\n\n\n\nrelax.methods.cchvae.CCHVAE\n\n[source]\n\nclass relax.methods.cchvae.CCHVAE (config=None, chvae=None, name=‘cchvae’)\n\nBase class for parametric counterfactual modules.\n\nMethods\n[source]\n\nset_apply_constraints_fn (apply_constraints_fn)\n\n[source]\n\nset_compute_reg_loss_fn (compute_reg_loss_fn)\n\n[source]\n\napply_constraints (*args, **kwargs)\n\n[source]\n\ncompute_reg_loss (*args, **kwargs)\n\n[source]\n\nsave (path)\n\n[source]\n\nload_from_path (path)\n\n[source]\n\nbefore_generate_cf (*args, **kwargs)\n\n\ngenerate_cf (*args, **kwargs)\n\n\n\ndata = load_data('adult')\npred_fn = load_ml_module('adult').pred_fn\nxs_train, ys_train = data['train']\nxs_test, ys_test = data['test']\n\n/home/birk/code/jax-relax/relax/data_module.py:234: UserWarning: Passing `config` will have no effect.\n  warnings.warn(\"Passing `config` will have no effect.\")\n\n\n\ncchvae = CCHVAE()\ncchvae.train(data, epochs=5)\ncchvae.set_apply_constraints_fn(data.apply_constraints)\n\nEpoch 1/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 3s 9ms/step - loss: 103.6776     \nEpoch 2/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 808us/step - loss: 3.1196     \nEpoch 3/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 788us/step - loss: 1.3849    \nEpoch 4/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 811us/step - loss: 0.8786    \nEpoch 5/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 785us/step - loss: 0.6225    \n\n\n\ncf = cchvae.generate_cf(xs_train[0], pred_fn, rng_key=jrand.PRNGKey(0))\n\n\n\n\n\nn_tests = 100\npartial_gen = partial(cchvae.generate_cf, pred_fn=pred_fn)\ncfs = jax.vmap(partial_gen)(xs_test[:n_tests], rng_key=jrand.split(jrand.PRNGKey(0), n_tests))\n\nassert cfs.shape == xs_test[:100].shape\n\nprint(\"Validity: \", keras.metrics.binary_accuracy(\n    (1 - pred_fn(xs_test[:100])).round(),\n    pred_fn(cfs[:, :])\n).mean())\n\n\n\n\nValidity:  1.0",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "CCHVAE"
    ]
  },
  {
    "objectID": "methods/l2c.html",
    "href": "methods/l2c.html",
    "title": "L2C",
    "section": "",
    "text": "start_end = jnp.array([[0, 1], [1, 2], [2, 3], [3, 5]])\nxs = jrand.normal(jrand.PRNGKey(0), (4, 5),)\ncfs = jrand.normal(jrand.PRNGKey(1), (4, 5),)\nprob = jrand.uniform(jrand.PRNGKey(2), (4, 4),)\n\n# split xs into 4 parts according to start_end\nxs_split = jnp.split(xs, start_end[:-1, 1], axis=1)\ncfs_split = jnp.split(cfs, start_end[:-1, 1], axis=1)\nprob_split = jnp.split(prob, start_end.shape[0], axis=1)\n\ndef perturb(x, cf, prob):\n    return x * (1 - prob) + cf * prob\n\nperturbed = jax.tree_util.tree_map(\n    perturb, xs_split, cfs_split, prob_split\n)",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "L2C"
    ]
  },
  {
    "objectID": "methods/l2c.html#l2c-model",
    "href": "methods/l2c.html#l2c-model",
    "title": "L2C",
    "section": "L2C Model",
    "text": "L2C Model\n\nrelax.utils.gumbel_softmax\n\n[source]\n\nrelax.utils.gumbel_softmax (key, logits, tau)\n\nThe Gumbel softmax function.\n\nParameters:\n\nkey (&lt;function PRNGKey at 0x7f248027e0e0&gt;) – Random key\nlogits (&lt;class 'jax.Array'&gt;) – Logits for each class. Shape (batch_size, num_classes)\ntau (&lt;class 'float'&gt;) – Temperature for the Gumbel softmax\n\n\n\nrelax.methods.sphere.sample_categorical\n\n[source]\n\nrelax.methods.sphere.sample_categorical (key, logits, tau, training=True)\n\nSample from a categorical distribution.\n\nParameters:\n\nkey (&lt;function PRNGKey at 0x7f248027e0e0&gt;) – Random key\nlogits (&lt;class 'jax.Array'&gt;) – Logits for each class. Shape (batch_size, num_classes)\ntau (&lt;class 'float'&gt;) – Temperature for the Gumbel softmax\ntraining (&lt;class 'bool'&gt;, default=True) – Apply gumbel softmax if training\n\n\n\nlogits = jnp.array([[2.0, 1.0, 0.1], [1.0, 2.0, 3.0]])\nkey = jrand.PRNGKey(0)\noutput = sample_categorical(key, logits, tau=0.5, training=True)\nassert output.shape == logits.shape\nassert jnp.allclose(output.sum(axis=-1), 1.0)\n# low temperature -&gt; one-hot\noutput = sample_categorical(key, logits, tau=0.001, training=True)\nassert jnp.array_equal(\n    output.argmax(axis=-1), logits.argmax(axis=-1)\n)\n# high temperature -&gt; uniform\noutput = sample_categorical(key, logits, tau=100, training=True)\nassert jnp.max(output) - jnp.min(output) &lt; 0.5\n\noutput = sample_categorical(key, logits, tau=0.5, training=False)\nassert output.shape == logits.shape\nassert jnp.array_equal(\n    output.argmax(axis=-1), logits.argmax(axis=-1)\n)\n\n\nrelax.methods.l2c.sample_bernouli\n\n[source]\n\nrelax.methods.l2c.sample_bernouli (key, prob, tau, training=True)\n\n“Sample from a bernouli distribution.\n\nParameters:\n\nkey (&lt;function PRNGKey at 0x7f248027e0e0&gt;) – Random key\nprob (&lt;class 'jax.Array'&gt;) – Logits for each class. Shape (batch_size, 1)\ntau (&lt;class 'float'&gt;) – Temperature for the Gumbel softmax\ntraining (&lt;class 'bool'&gt;, default=True) – Apply gumbel softmax if training\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;)\n\n\nrelax.methods.l2c.split_fn\n\n[source]\n\nrelax.methods.l2c.split_fn (feature_indices)\n\n\nstart_end = [(0, 1), (1, 2), (2, 3), (3, 5)]\nsplit_xs, split_prob = split_fn(start_end)\nassert len(split_xs(xs)) == len(start_end)\nassert len(split_prob(prob)) == len(start_end)\n\n\nrelax.methods.l2c.L2CModel\n\n[source]\n\nclass relax.methods.l2c.L2CModel (generator_layers, selector_layers, feature_indices=None, immutable_mask=None, pred_fn=None, alpha=0.0001, tau=0.7, seed=None, **kwargs)\n\nA model grouping layers into an object with training/inference features.\nThere are three ways to instantiate a Model:",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "L2C"
    ]
  },
  {
    "objectID": "methods/l2c.html#with-the-functional-api",
    "href": "methods/l2c.html#with-the-functional-api",
    "title": "L2C",
    "section": "With the “Functional API”",
    "text": "With the “Functional API”\nYou start from Input, you chain layer calls to specify the model’s forward pass, and finally you create your model from inputs and outputs:\ninputs = keras.Input(shape=(37,))\nx = keras.layers.Dense(32, activation=\"relu\")(inputs)\noutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nNote: Only dicts, lists, and tuples of input tensors are supported. Nested inputs are not supported (e.g. lists of list or dicts of dict).\nA new Functional API model can also be created by using the intermediate tensors. This enables you to quickly extract sub-components of the model.\nExample:\ninputs = keras.Input(shape=(None, None, 3))\nprocessed = keras.layers.RandomCrop(width=128, height=128)(inputs)\nconv = keras.layers.Conv2D(filters=32, kernel_size=3)(processed)\npooling = keras.layers.GlobalAveragePooling2D()(conv)\nfeature = keras.layers.Dense(10)(pooling)\n\nfull_model = keras.Model(inputs, feature)\nbackbone = keras.Model(processed, conv)\nactivations = keras.Model(conv, feature)\nNote that the backbone and activations models are not created with keras.Input objects, but with the tensors that originate from keras.Input objects. Under the hood, the layers and weights will be shared across these models, so that user can train the full_model, and use backbone or activations to do feature extraction. The inputs and outputs of the model can be nested structures of tensors as well, and the created models are standard Functional API models that support all the existing APIs.",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "L2C"
    ]
  },
  {
    "objectID": "methods/l2c.html#by-subclassing-the-model-class",
    "href": "methods/l2c.html#by-subclassing-the-model-class",
    "title": "L2C",
    "section": "By subclassing the Model class",
    "text": "By subclassing the Model class\nIn that case, you should define your layers in __init__() and you should implement the model’s forward pass in call().\nclass MyModel(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n\n    def call(self, inputs):\n        x = self.dense1(inputs)\n        return self.dense2(x)\n\nmodel = MyModel()\nIf you subclass Model, you can optionally have a training argument (boolean) in call(), which you can use to specify a different behavior in training and inference:\nclass MyModel(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n        self.dropout = keras.layers.Dropout(0.5)\n\n    def call(self, inputs, training=False):\n        x = self.dense1(inputs)\n        x = self.dropout(x, training=training)\n        return self.dense2(x)\n\nmodel = MyModel()\nOnce the model is created, you can config the model with losses and metrics with model.compile(), train the model with model.fit(), or use the model to do prediction with model.predict().",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "L2C"
    ]
  },
  {
    "objectID": "methods/l2c.html#with-the-sequential-class",
    "href": "methods/l2c.html#with-the-sequential-class",
    "title": "L2C",
    "section": "With the Sequential class",
    "text": "With the Sequential class\nIn addition, keras.Sequential is a special case of model where the model is purely a stack of single-input, single-output layers.\nmodel = keras.Sequential([\n    keras.Input(shape=(None, None, 3)),\n    keras.layers.Conv2D(filters=32, kernel_size=3),\n])\n\nParameters:\n\ngenerator_layers (list[int])\nselector_layers (list[int])\nfeature_indices (list[tuple[int, int]], default=None)\nimmutable_mask (&lt;class 'jax.Array'&gt;, default=None)\npred_fn (typing.Callable, default=None)\nalpha (&lt;class 'float'&gt;, default=0.0001) – Sparsity regularization\ntau (&lt;class 'float'&gt;, default=0.7)\nseed (&lt;class 'int'&gt;, default=None)\nkwargs (VAR_KEYWORD)",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "L2C"
    ]
  },
  {
    "objectID": "methods/l2c.html#discretizer",
    "href": "methods/l2c.html#discretizer",
    "title": "L2C",
    "section": "Discretizer",
    "text": "Discretizer\n\nrelax.methods.l2c.qcut\n\n[source]\n\nrelax.methods.l2c.qcut (x, q, axis=0)\n\nQuantile binning.\n\nParameters:\n\nx (&lt;class 'jax.Array'&gt;) – Input array\nq (&lt;class 'int'&gt;) – Number of quantiles\naxis (&lt;class 'int'&gt;, default=0) – Axis to quantile\n\n\n\nReturns:\n    (tuple[jax.Array, jax.Array]) – (digitized array, quantiles)\n\n\ndigitized, quantiles = qcut(jnp.arange(10), 4)\nassert digitized.shape == (10,)\nassert quantiles.shape == (3,)\nassert jnp.allclose(\n    digitized, jnp.array([0,0,0,1,1,2,2,3,3,3])\n)\n\nquantiles_true = jnp.array([0, 2.25, 4.5, 6.75, 9])\nassert jnp.allclose(\n    quantiles, quantiles_true[1:-1]\n)\nx_empty = jnp.array([])\nq = 2\ndigitized_empty, quantiles_empty = qcut(x_empty, q)\nassert digitized_empty.size == 0 and quantiles_empty.size == 0\n# Test with single element array\nx_single = jnp.array([1])\ndigitized_single, quantiles_single = qcut(x_single, q)\nassert digitized_single.size == 1 and quantiles_single.size == 0\n\n# Test with large q value\nxs = jnp.array([1, 2, 3, 4, 5, 6])\nq_large = 10\n_, quantiles_large = qcut(xs, q_large)\nassert len(quantiles_large) == q_large - 1\n\n\nrelax.methods.l2c.qcut_inverse\n\n[source]\n\nrelax.methods.l2c.qcut_inverse (digitized, quantiles)\n\nInverse of qcut.\n\nParameters:\n\ndigitized (&lt;class 'jax.Array'&gt;) – Digitized One-Hot Encoding Array\nquantiles (&lt;class 'jax.Array'&gt;) – Quantiles\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;)\n\n\ndigitized, quantiles = qcut(jnp.arange(10), 4)\nohe_digitized = jax.nn.one_hot(digitized, 4)\n# continuous feats\nquantiles_inv = qcut_inverse(ohe_digitized, jnp.arange(4))\nassert quantiles_inv.shape == (10, 1)\n# discrete feats\nquantiles_inv = qcut_inverse(ohe_digitized, jnp.identity(4))\nassert jnp.array_equal(quantiles_inv, ohe_digitized)\n\n\nrelax.methods.l2c.cut_quantiles\n\n[source]\n\nrelax.methods.l2c.cut_quantiles (quantiles, xs)\n\n\nParameters:\n\nquantiles (&lt;class 'jax.Array'&gt;) – Quantiles\nxs (&lt;class 'jax.Array'&gt;) – Input array\n\n\n\nrelax.methods.l2c.discretize_xs\n\n[source]\n\nrelax.methods.l2c.discretize_xs (xs, is_categorical_and_indices, q=4)\n\nDiscretize continuous features.\n\nParameters:\n\nxs (&lt;class 'jax.Array'&gt;) – Input array\nis_categorical_and_indices (list[tuple[bool, tuple[int, int]]]) – Features list\nq (&lt;class 'int'&gt;, default=4) – Number of quantiles\n\n\n\nReturns:\n    (tuple[list[jax.Array], list[jax.Array], list[jax.Array], list[list[int, int]]]) – (discretized array, indices_and_quantiles_and_mid)\n\n\ndm = relax.load_data(\"dummy\")\nxs, ys = dm['train']\nis_categorical_and_indices = [\n    (feat.is_categorical, indices) for feat, indices in zip(dm.features, dm.features.feature_indices)\n]\ndiscretized_xs, quantiles_feats, mid_quantiles, feature_indices = discretize_xs(xs, is_categorical_and_indices)\nassert len(discretized_xs) == len(is_categorical_and_indices)\nassert all(discretized_xs[i].shape[1] == 4 for i in range(len(discretized_xs)))\n\nassert len(quantiles_feats) == len(is_categorical_and_indices)\nassert all(len(quantiles_feats[i]) == 3 for i in range(len(quantiles_feats)))\nassert len(mid_quantiles) == len(is_categorical_and_indices)\nassert all(len(mid_quantiles[i]) == 4 for i in range(len(mid_quantiles)))\n\n\nrelax.methods.l2c.Discretizer\n\n[source]\n\nclass relax.methods.l2c.Discretizer (is_cat_and_indices, q=4)\n\nDiscretize continuous features.\n\nParameters:\n\nis_cat_and_indices (list[tuple[bool, tuple[int, int]]]) – Features list\nq (&lt;class 'int'&gt;, default=4) – Number of quantiles\n\n\n\ndm = relax.load_data(\"adult\")\nxs, ys = dm['train']\nis_categorical_and_indices = [\n    (feat.is_categorical, indices) for feat, indices in zip(dm.features, dm.features.feature_indices)\n]\n\ndis = Discretizer(is_categorical_and_indices)\ndis.fit(xs)\ndigitized_xs_1 = dis.transform(xs)\nassert digitized_xs_1.shape == (xs.shape[0], 35)\n# assert jnp.array_equal(jnp.concatenate(discretized_xs, axis=-1), digitized_xs_1)\ninversed_xs = dis.inverse_transform(digitized_xs_1)\nassert xs.shape == inversed_xs.shape\n# assert jnp.unique(inversed_xs).size == xs.shape[1] * 4\n\nml_module = relax.load_ml_module(\"adult\")\npred_fn = dis.get_pred_fn(ml_module.pred_fn)\n# digitized_xs_1 = split_xs(xs)\ny = pred_fn(digitized_xs_1)\nassert y.shape == (xs.shape[0], 2)\n\ndef f(x, y):\n    y_pred = pred_fn(x)\n    return jnp.mean((y_pred - y) ** 2)\n\ngrad = jax.grad(f)(digitized_xs_1, ys)\nassert grad.shape == digitized_xs_1.shape",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "L2C"
    ]
  },
  {
    "objectID": "methods/l2c.html#l2c-module",
    "href": "methods/l2c.html#l2c-module",
    "title": "L2C",
    "section": "L2C Module",
    "text": "L2C Module\n\nrelax.methods.l2c.L2CConfig\n\n[source]\n\nclass relax.methods.l2c.L2CConfig (generator_layers=[64, 64, 64], selector_layers=[64], lr=0.001, opt_name=‘adam’, alpha=0.0001, tau=0.7, q=4)\n\nBase class for all config classes.\n\nParameters:\n\ngenerator_layers (list[int], default=[64, 64, 64]) – Generator MLP layers.\nselector_layers (list[int], default=[64]) – Selector MLP layers.\nlr (float, default=0.001) – Model learning rate.\nopt_name (str, default=adam) – Optimizer name of training L2C.\nalpha (float, default=0.0001) – Sparsity regularization.\ntau (float, default=0.7) – Temperature for the Gumbel softmax.\nq (int, default=4) – Number of quantiles.\n\n\n\nrelax.methods.l2c.L2C\n\n[source]\n\nclass relax.methods.l2c.L2C (config=None, l2c_model=None, name=‘l2c’)\n\nBase class for parametric counterfactual modules.\n\nMethods\n[source]\n\nset_apply_constraints_fn (apply_constraints_fn)\n\n[source]\n\nset_compute_reg_loss_fn (compute_reg_loss_fn)\n\n[source]\n\napply_constraints (*args, **kwargs)\n\n[source]\n\ncompute_reg_loss (*args, **kwargs)\n\n[source]\n\nsave (path)\n\n[source]\n\nload_from_path (path)\n\n[source]\n\nbefore_generate_cf (*args, **kwargs)\n\n\ngenerate_cf (*args, **kwargs)\n\n\n\ndm = relax.load_data('adult')\nml_module = relax.load_ml_module('adult')\n\n\nl2c = L2C()\nexp = relax.generate_cf_explanations(\n    l2c, dm, ml_module.pred_fn,\n)\n\nEpoch 1/10\n191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 7ms/step - loss: 0.8718 \nEpoch 2/10\n191/191 ━━━━━━━━━━━━━━━━━━━━ 1s 697us/step - loss: 0.1734\nEpoch 3/10\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 703us/step - loss: 0.1531\nEpoch 4/10\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 693us/step - loss: 0.1465\nEpoch 5/10\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 684us/step - loss: 0.1387\nEpoch 6/10\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 687us/step - loss: 0.1392\nEpoch 7/10\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 680us/step - loss: 0.1365\nEpoch 8/10\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 689us/step - loss: 0.1390\nEpoch 9/10\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 723us/step - loss: 0.1389\nEpoch 10/10\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 681us/step - loss: 0.1341\n\n\n\nrelax.benchmark_cfs([exp])\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\nadult\nl2c\n0.827124\n0.981235\n7.798692\n\n\n\n\n\n\n\n\npartial_gen = ft.partial(l2c.generate_cf, pred_fn=ml_module.pred_fn)\ncfs = jax.vmap(partial_gen)(dm.xs, rng_key=jrand.split(jrand.PRNGKey(0), dm.xs.shape[0]))",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "L2C"
    ]
  },
  {
    "objectID": "methods/base.html",
    "href": "methods/base.html",
    "title": "Base API",
    "section": "",
    "text": "relax.methods.base.CFModule\n\n[source]\n\nclass relax.methods.base.CFModule (config, name=None, apply_constraints_fn=None, compute_reg_loss_fn=None, **kwargs)\n\nBase class for all counterfactual modules.\n\nMethods\n[source]\n\nset_apply_constraints_fn (apply_constraints_fn)\n\n[source]\n\nset_compute_reg_loss_fn (compute_reg_loss_fn)\n\n[source]\n\napply_constraints (*args, **kwargs)\n\n[source]\n\ncompute_reg_loss (*args, **kwargs)\n\n[source]\n\nsave (path)\n\n[source]\n\nload_from_path (path)\n\n[source]\n\nbefore_generate_cf (*args, **kwargs)\n\n[source]\n\ngenerate_cf (x, pred_fn=None, y_target=None, rng_key=None, **kwargs)\n\n\nParameters:\n\nx (&lt;class 'jax.Array'&gt;)\npred_fn (typing.Callable, default=None)\ny_target (&lt;class 'jax.Array'&gt;, default=None)\nrng_key (&lt;function PRNGKey at 0x7fcaaae82d40&gt;, default=None)\nkwargs (VAR_KEYWORD)\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;) – Return counterfactual of x.\n\n\n\nrelax.methods.base.ParametricCFModule\n\n[source]\n\nclass relax.methods.base.ParametricCFModule (config, name=None, apply_constraints_fn=None, compute_reg_loss_fn=None, **kwargs)\n\nBase class for parametric counterfactual modules.\n\nMethods\n[source]\n\nset_apply_constraints_fn (apply_constraints_fn)\n\n[source]\n\nset_compute_reg_loss_fn (compute_reg_loss_fn)\n\n[source]\n\napply_constraints (*args, **kwargs)\n\n[source]\n\ncompute_reg_loss (*args, **kwargs)\n\n[source]\n\nsave (path)\n\n[source]\n\nload_from_path (path)\n\n[source]\n\nbefore_generate_cf (*args, **kwargs)\n\n[source]\n\ngenerate_cf (x, pred_fn=None, y_target=None, rng_key=None, **kwargs)\n\n\nParameters:\n\nx (&lt;class 'jax.Array'&gt;)\npred_fn (typing.Callable, default=None)\ny_target (&lt;class 'jax.Array'&gt;, default=None)\nrng_key (&lt;function PRNGKey at 0x7fcaaae82d40&gt;, default=None)\nkwargs (VAR_KEYWORD)\n\n\n\nReturns:\n    (&lt;class 'jax.Array'&gt;) – Return counterfactual of x.",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "Base API"
    ]
  },
  {
    "objectID": "methods/sphere.html",
    "href": "methods/sphere.html",
    "title": "Growing Sphere",
    "section": "",
    "text": "relax.methods.sphere.hyper_sphere_coordindates\n\n[source]\n\nrelax.methods.sphere.hyper_sphere_coordindates (rng_key, x, n_samples, high, low, p_norm=2)\n\n\nParameters:\n\nrng_key (&lt;function PRNGKey at 0x7fcaaae82d40&gt;) – Random number generator key\nx (&lt;class 'jax.Array'&gt;) – Input instance with only continuous features. Shape: (1, n_features)\nn_samples (&lt;class 'int'&gt;) – Number of samples\nhigh (&lt;class 'float'&gt;) – Upper bound\nlow (&lt;class 'float'&gt;) – Lower bound\np_norm (&lt;class 'int'&gt;, default=2) – Norm\n\n\n\nrelax.methods.sphere.sample_categorical\n\n[source]\n\nrelax.methods.sphere.sample_categorical (rng_key, col_size, n_samples)\n\n\nrelax.methods.sphere.default_perturb_function\n\n[source]\n\nrelax.methods.sphere.default_perturb_function (rng_key, x, n_samples, high, low, p_norm)\n\n\nParameters:\n\nrng_key (&lt;function PRNGKey at 0x7fcaaae82d40&gt;)\nx (&lt;class 'numpy.ndarray'&gt;) – Shape: (1, k)\nn_samples (&lt;class 'int'&gt;)\nhigh (&lt;class 'float'&gt;)\nlow (&lt;class 'float'&gt;)\np_norm (&lt;class 'int'&gt;)\n\n\n\nrelax.methods.sphere.perturb_function_with_features\n\n[source]\n\nrelax.methods.sphere.perturb_function_with_features (rng_key, x, n_samples, high, low, p_norm, cont_masks, immut_masks, num_categories, cat_perturb_fn)\n\n\nParameters:\n\nrng_key (&lt;function PRNGKey at 0x7fcaaae82d40&gt;)\nx (&lt;class 'numpy.ndarray'&gt;) – Shape: (1, k)\nn_samples (&lt;class 'int'&gt;)\nhigh (&lt;class 'float'&gt;)\nlow (&lt;class 'float'&gt;)\np_norm (&lt;class 'int'&gt;)\ncont_masks (&lt;class 'jax.Array'&gt;)\nimmut_masks (&lt;class 'jax.Array'&gt;)\nnum_categories (list[int])\ncat_perturb_fn (typing.Callable)\n\n\n\ndm = load_data('adult')\nx_sliced = dm.xs[:1]\nfeats_info, perturb_fn = features_to_infos_and_perturb_fn(dm.features)\ncont_masks, immut_masks, num_categories = feats_info\nassert np.array_equal(cont_masks, np.array([1, 1] + [0] * 27))\nassert immut_masks.sum() == 2 + 2\nassert x_sliced.ndim == 2\ncfs = perturb_function_with_features(\n    jrand.PRNGKey(0), x_sliced, 1000, 1, 0, 2, *feats_info, perturb_fn\n)\nassert cfs.shape == (1000, 29)\nassert cfs[:, 2:].sum() == 1000 * 6\nassert default_perturb_function(\n    jrand.PRNGKey(0), x_sliced, 100, 1, 0, 2,\n).shape == (100, 29)\n\n\nrelax.methods.sphere.GSConfig\n\n[source]\n\nclass relax.methods.sphere.GSConfig (n_steps=100, n_samples=100, step_size=0.05, p_norm=2)\n\nBase class for all config classes.\n\nrelax.methods.sphere.GrowingSphere\n\n[source]\n\nclass relax.methods.sphere.GrowingSphere (config=None, name=None, perturb_fn=None)\n\nBase class for all counterfactual modules.\n\nMethods\n[source]\n\nset_apply_constraints_fn (apply_constraints_fn)\n\n[source]\n\nset_compute_reg_loss_fn (compute_reg_loss_fn)\n\n[source]\n\napply_constraints (*args, **kwargs)\n\n[source]\n\ncompute_reg_loss (*args, **kwargs)\n\n[source]\n\nsave (path, save_data_module=True)\n\n[source]\n\nload_from_path (path)\n\n[source]\n\nbefore_generate_cf (*args, **kwargs)\n\n\ngenerate_cf (*args, **kwargs)\n\n\n\ndm = load_data('dummy')\nmodel = load_ml_module('dummy')\nxs_train, ys_train = dm['train']\nxs_test, ys_test = dm['test']\nx_shape = xs_test.shape\n\n\ngs = GrowingSphere()\nassert not gs.has_data_module()\ngs.set_data_module(dm)\nassert gs.has_data_module()\ngs.set_apply_constraints_fn(dm.apply_constraints)\ngs.before_generate_cf()\n\ncf = gs.generate_cf(xs_test[0], pred_fn=model.pred_fn, rng_key=jax.random.PRNGKey(0))\n\n\n\n\n\ngs.save('tmp/gs/')\ngs_1 = GrowingSphere.load_from_path('tmp/gs/')\nassert gs_1.has_data_module()\ngs_1.set_apply_constraints_fn(dm.apply_constraints)\ngs_1.before_generate_cf()\n\ncf_1 = gs_1.generate_cf(xs_test[0], pred_fn=model.pred_fn, rng_key=jax.random.PRNGKey(0))\nassert jnp.allclose(cf, cf_1)\n\nshutil.rmtree('tmp/gs/')\ngs.save('tmp/gs/', save_data_module=False)\ngs_2 = GrowingSphere.load_from_path('tmp/gs/')\nassert not gs_2.has_data_module()\n\n\n\n\n\npartial_gen = partial(gs.generate_cf, pred_fn=model.pred_fn)\ncfs = jax.jit(jax.vmap(partial_gen))(xs_test, rng_key=jrand.split(jrand.PRNGKey(0), len(xs_test)))\n\nassert cfs.shape == (x_shape[0], x_shape[1])\nassert cfs.min() &gt;= 0 and cfs.max() &lt;= 1\n\nprint(\"Validity: \", keras.metrics.binary_accuracy(\n    (1 - model.pred_fn(xs_test)).round(),\n    model.pred_fn(cfs[:, :])\n).mean())\n\n\n\n\nValidity:  1.0",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "Growing Sphere"
    ]
  },
  {
    "objectID": "methods/clue.html",
    "href": "methods/clue.html",
    "title": "CLUE",
    "section": "",
    "text": "relax.methods.clue.Decoder\n[source]\nThis is the class from which all layers inherit.\nA layer is a callable object that takes as input one or more tensors and that outputs one or more tensors. It involves computation, defined in the call() method, and a state (weight variables). State can be created:\nLayers are recursively composable: If you assign a Layer instance as an attribute of another Layer, the outer layer will start tracking the weights created by the inner layer. Nested layers should be instantiated in the __init__() method or build() method.\nUsers will just instantiate a layer and then treat it as a callable.\nArgs: trainable: Boolean, whether the layer’s variables should be trainable. name: String name of the layer. dtype: The dtype of the layer’s computations and weights. Can also be a keras.DTypePolicy, which allows the computation and weight dtype to differ. Defaults to None. None means to use keras.config.dtype_policy(), which is a float32 policy unless set to different value (via keras.config.set_dtype_policy()).\nAttributes: name: The name of the layer (string). dtype: Dtype of the layer’s weights. Alias of layer.variable_dtype. variable_dtype: Dtype of the layer’s weights. compute_dtype: The dtype of the layer’s computations. Layers automatically cast inputs to this dtype, which causes the computations and output to also be in this dtype. When mixed precision is used with a keras.DTypePolicy, this will be different than variable_dtype. trainable_weights: List of variables to be included in backprop. non_trainable_weights: List of variables that should not be included in backprop. weights: The concatenation of the lists trainable_weights and non_trainable_weights (in this order). trainable: Whether the layer should be trained (boolean), i.e. whether its potentially-trainable weights should be returned as part of layer.trainable_weights. input_spec: Optional (list of) InputSpec object(s) specifying the constraints on inputs that can be accepted by the layer.\nWe recommend that descendants of Layer implement the following methods:\nExamples:\nHere’s a basic example: a layer with two variables, w and b, that returns y = w . x + b. It shows how to implement build() and call(). Variables set as attributes of a layer are tracked as weights of the layers (in layer.weights).\nBesides trainable weights, updated via backpropagation during training, layers can also have non-trainable weights. These weights are meant to be updated manually during call(). Here’s a example layer that computes the running sum of its inputs:\n[source]\nThis is the class from which all layers inherit.\nA layer is a callable object that takes as input one or more tensors and that outputs one or more tensors. It involves computation, defined in the call() method, and a state (weight variables). State can be created:\nLayers are recursively composable: If you assign a Layer instance as an attribute of another Layer, the outer layer will start tracking the weights created by the inner layer. Nested layers should be instantiated in the __init__() method or build() method.\nUsers will just instantiate a layer and then treat it as a callable.\nArgs: trainable: Boolean, whether the layer’s variables should be trainable. name: String name of the layer. dtype: The dtype of the layer’s computations and weights. Can also be a keras.DTypePolicy, which allows the computation and weight dtype to differ. Defaults to None. None means to use keras.config.dtype_policy(), which is a float32 policy unless set to different value (via keras.config.set_dtype_policy()).\nAttributes: name: The name of the layer (string). dtype: Dtype of the layer’s weights. Alias of layer.variable_dtype. variable_dtype: Dtype of the layer’s weights. compute_dtype: The dtype of the layer’s computations. Layers automatically cast inputs to this dtype, which causes the computations and output to also be in this dtype. When mixed precision is used with a keras.DTypePolicy, this will be different than variable_dtype. trainable_weights: List of variables to be included in backprop. non_trainable_weights: List of variables that should not be included in backprop. weights: The concatenation of the lists trainable_weights and non_trainable_weights (in this order). trainable: Whether the layer should be trained (boolean), i.e. whether its potentially-trainable weights should be returned as part of layer.trainable_weights. input_spec: Optional (list of) InputSpec object(s) specifying the constraints on inputs that can be accepted by the layer.\nWe recommend that descendants of Layer implement the following methods:\nExamples:\nHere’s a basic example: a layer with two variables, w and b, that returns y = w . x + b. It shows how to implement build() and call(). Variables set as attributes of a layer are tracked as weights of the layers (in layer.weights).\nBesides trainable weights, updated via backpropagation during training, layers can also have non-trainable weights. These weights are meant to be updated manually during call(). Here’s a example layer that computes the running sum of its inputs:\ninputs = jrand.normal(jrand.PRNGKey(0), (100, 10))\nencoded_x = Encoder([100, 10])(inputs, training=True)\nassert encoded_x[0].shape == (100, 5)\nassert encoded_x[1].shape == (100, 5)\n\ndecoded_x = Decoder([100, 10], 10)(inputs, training=True)\nassert decoded_x.shape == (100, 10)\n[source]\n[source]\nA model grouping layers into an object with training/inference features.\nThere are three ways to instantiate a Model:",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "CLUE"
    ]
  },
  {
    "objectID": "methods/clue.html#with-the-functional-api",
    "href": "methods/clue.html#with-the-functional-api",
    "title": "CLUE",
    "section": "With the “Functional API”",
    "text": "With the “Functional API”\nYou start from Input, you chain layer calls to specify the model’s forward pass, and finally you create your model from inputs and outputs:\ninputs = keras.Input(shape=(37,))\nx = keras.layers.Dense(32, activation=\"relu\")(inputs)\noutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nNote: Only dicts, lists, and tuples of input tensors are supported. Nested inputs are not supported (e.g. lists of list or dicts of dict).\nA new Functional API model can also be created by using the intermediate tensors. This enables you to quickly extract sub-components of the model.\nExample:\ninputs = keras.Input(shape=(None, None, 3))\nprocessed = keras.layers.RandomCrop(width=128, height=128)(inputs)\nconv = keras.layers.Conv2D(filters=32, kernel_size=3)(processed)\npooling = keras.layers.GlobalAveragePooling2D()(conv)\nfeature = keras.layers.Dense(10)(pooling)\n\nfull_model = keras.Model(inputs, feature)\nbackbone = keras.Model(processed, conv)\nactivations = keras.Model(conv, feature)\nNote that the backbone and activations models are not created with keras.Input objects, but with the tensors that originate from keras.Input objects. Under the hood, the layers and weights will be shared across these models, so that user can train the full_model, and use backbone or activations to do feature extraction. The inputs and outputs of the model can be nested structures of tensors as well, and the created models are standard Functional API models that support all the existing APIs.",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "CLUE"
    ]
  },
  {
    "objectID": "methods/clue.html#by-subclassing-the-model-class",
    "href": "methods/clue.html#by-subclassing-the-model-class",
    "title": "CLUE",
    "section": "By subclassing the Model class",
    "text": "By subclassing the Model class\nIn that case, you should define your layers in __init__() and you should implement the model’s forward pass in call().\nclass MyModel(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n\n    def call(self, inputs):\n        x = self.dense1(inputs)\n        return self.dense2(x)\n\nmodel = MyModel()\nIf you subclass Model, you can optionally have a training argument (boolean) in call(), which you can use to specify a different behavior in training and inference:\nclass MyModel(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n        self.dropout = keras.layers.Dropout(0.5)\n\n    def call(self, inputs, training=False):\n        x = self.dense1(inputs)\n        x = self.dropout(x, training=training)\n        return self.dense2(x)\n\nmodel = MyModel()\nOnce the model is created, you can config the model with losses and metrics with model.compile(), train the model with model.fit(), or use the model to do prediction with model.predict().",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "CLUE"
    ]
  },
  {
    "objectID": "methods/clue.html#with-the-sequential-class",
    "href": "methods/clue.html#with-the-sequential-class",
    "title": "CLUE",
    "section": "With the Sequential class",
    "text": "With the Sequential class\nIn addition, keras.Sequential is a special case of model where the model is purely a stack of single-input, single-output layers.\nmodel = keras.Sequential([\n    keras.Input(shape=(None, None, 3)),\n    keras.layers.Conv2D(filters=32, kernel_size=3),\n])\n\nvae_model = VAEGaussCat()\nvae_model.compile(optimizer=keras.optimizers.Adam(0.001), loss=None)\ndm = load_data('dummy')\nxs, _ = dm['train']\nhistory = vae_model.fit(\n    xs, xs,\n    batch_size=64,\n    epochs=2,\n    verbose=0  # Set to 1 for training progress\n)\nassert history.history['loss'][0] &gt; history.history['loss'][-1]\n\n/home/birk/code/jax-relax/relax/data_module.py:234: UserWarning: Passing `config` will have no effect.\n  warnings.warn(\"Passing `config` will have no effect.\")\n\n\n\nx = xs[:1]\npred_fn = load_ml_module('dummy').pred_fn\ncf = _clue_generate(\n    x,\n    jrand.PRNGKey(get_config().global_seed),\n    y_target=1 - pred_fn(x),\n    pred_fn=pred_fn,\n    max_steps=100,\n    step_size=0.1,\n    vae_module=vae_model,\n    uncertainty_weight=1.,\n    aleatoric_weight=1.,\n    prior_weight=1.,\n    distance_weight=1.,\n    validity_weight=1.,\n    validity_fn=keras.losses.get({'class_name': 'KLDivergence', 'config': {'reduction': None}}),\n    apply_fn=lambda x, cf, hard: cf\n)\nassert cf.shape == x.shape\n\n\n\n\n\nrelax.methods.clue.CLUEConfig\n\n[source]\n\nclass relax.methods.clue.CLUEConfig (enc_sizes=[20, 16, 14, 12], dec_sizes=[12, 14, 16, 20], dropout_rate=0.1, encoded_size=5, lr=0.001, max_steps=500, step_size=0.01, vae_n_epochs=10, vae_batch_size=128, seed=0)\n\nBase class for all config classes.\n\nParameters:\n\nenc_sizes (List[int], default=[20, 16, 14, 12]) – Sequence of Encoder layer sizes.\ndec_sizes (List[int], default=[12, 14, 16, 20]) – Sequence of Decoder layer sizes.\ndropout_rate (float, default=0.1) – Dropout rate\nencoded_size (int, default=5) – Encoded size\nlr (float, default=0.001) – Learning rate\nmax_steps (int, default=500) – Max steps\nstep_size (float, default=0.01) – Step size\nvae_n_epochs (int, default=10) – Number of epochs for VAE\nvae_batch_size (int, default=128) – Batch size for VAE\nseed (int, default=0) – Seed for random number generator\n\n\n\nrelax.methods.clue.get_reconstruction_loss_fn\n\n[source]\n\nrelax.methods.clue.get_reconstruction_loss_fn (dm)\n\n\ndm = load_data('adult')\nreconstruction_loss = get_reconstruction_loss_fn(dm)\nxs, _ = dm['test']\ncfs = jrand.normal(jrand.PRNGKey(0), xs.shape)\nloss = reconstruction_loss(xs, cfs)\nassert loss.shape == (xs.shape[0], len(dm.features))\n\n/home/birk/code/jax-relax/relax/data_module.py:234: UserWarning: Passing `config` will have no effect.\n  warnings.warn(\"Passing `config` will have no effect.\")\n\n\n\nrelax.methods.clue.CLUE\n\n[source]\n\nclass relax.methods.clue.CLUE (config=None, vae=None, name=‘CLUE’)\n\nBase class for parametric counterfactual modules.\n\nMethods\n[source]\n\nset_apply_constraints_fn (apply_constraints_fn)\n\n[source]\n\nset_compute_reg_loss_fn (compute_reg_loss_fn)\n\n[source]\n\napply_constraints (*args, **kwargs)\n\n[source]\n\ncompute_reg_loss (*args, **kwargs)\n\n[source]\n\nsave (path)\n\n[source]\n\nload_from_path (path)\n\n[source]\n\nbefore_generate_cf (*args, **kwargs)\n\n\ngenerate_cf (*args, **kwargs)\n\n\n\ndata = load_data('adult')\npred_fn = load_ml_module('adult').pred_fn\nxs_train, ys_train = data['train']\nxs_test, ys_test = data['test']\n\n/home/birk/code/jax-relax/relax/data_module.py:234: UserWarning: Passing `config` will have no effect.\n  warnings.warn(\"Passing `config` will have no effect.\")\n\n\n\nclue = CLUE()\nclue.train(data, batch_size=128, epochs=5)\nclue.set_apply_constraints_fn(data.apply_constraints)\n\nEpoch 1/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - loss: 0.1202    \nEpoch 2/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 769us/step - loss: 0.0694     \nEpoch 3/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 748us/step - loss: 0.0639    \nEpoch 4/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 743us/step - loss: 0.0621    \nEpoch 5/5\n191/191 ━━━━━━━━━━━━━━━━━━━━ 0s 736us/step - loss: 0.0613    \n\n\n\ncf = clue.generate_cf(xs_train[0], pred_fn, rng_key=jrand.PRNGKey(0))\n\n\n\n\n\nn_tests = 100\npartial_gen = partial(clue.generate_cf, pred_fn=pred_fn)\ncfs = jax.vmap(partial_gen)(xs_test[:n_tests], rng_key=jrand.split(jrand.PRNGKey(0), n_tests))\n\nassert cfs.shape == xs_test[:100].shape\n\nprint(\"Validity: \", keras.metrics.binary_accuracy(\n    (1 - pred_fn(xs_test[:100])).round(),\n    pred_fn(cfs[:, :])\n).mean())\n\n\n\n\nValidity:  0.16",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Methods",
      "CLUE"
    ]
  },
  {
    "objectID": "tutorials/forktable.html",
    "href": "tutorials/forktable.html",
    "title": "Benchmarking to 10M Dataset",
    "section": "",
    "text": "from relax.import_essentials import *\nfrom relax.data import *\nfrom relax.module import *\nimport datasets as hfds\nfrom relax.trainer import train_model, TrainingConfigs\nfrom relax.utils import *\nfrom relax._ckpt_manager import load_checkpoint, save_checkpoint\n\n\nds = hfds.load_dataset(\"birkhoffg/folktables-acs-income\")\n\nFound cached dataset parquet (/home/birk/.cache/huggingface/datasets/birkhoffg___parquet/birkhoffg--folktables-acs-income-bc190711a423bf3e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n\n\n\n\n\n\ndef hfds_to_dm(\n    dataset: hfds.Dataset, \n    configs: TabularDataModuleConfigs\n) -&gt; TabularDataModule:\n    train_df = dataset[\"train\"].to_pandas()\n    test_df = dataset[\"test\"].to_pandas()\n    df = pd.concat([train_df, test_df])\n    if \"__index_level_0__\" in df.columns:\n        df = df.drop(columns=[\"__index_level_0__\"])\n    print('df is loaded')\n    dm = TabularDataModule(configs, df)\n    return dm\n\n\nconfigs = TabularDataModuleConfigs(\n    data_dir='',\n    data_name='forktable',\n    continous_cols=['AGEP', 'OCCP', 'POBP', 'RELP', 'WKHP'],\n    discret_cols=['COW', 'SCHL', 'MAR', 'SEX', 'RAC1P', 'STATE', 'YEAR'],\n    # sample_frac=0.1\n)\n\n\ndm = hfds_to_dm(ds, configs)\n\ndf is loaded\n\n\n/home/birk/mambaforge-pypy3/envs/nbdev2/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n\n\n\nmodule = PredictiveTrainingModule({\n    'lr': 1e-3,\n    'sizes': [110, 110, 50, 10],\n    'dropout': 0.3,\n})\n\n\nparams, _ = train_model(\n    module, dm, TrainingConfigs(\n        n_epochs=10, batch_size=256, monitor_metrics='val/val_accuracy',\n        max_n_checkpoints=1\n    )\n)\n\nEpoch 9: 100%|██████████| 28694/28694 [02:57&lt;00:00, 161.59batch/s, train/train_loss_1=0.0623]\n\n\n\ntrain_X, train_y = dm.train_dataset[:]\ntest_X, test_y = dm.test_dataset[:]\n\n\ny_pred = module.pred_fn(test_X, params, jrand.PRNGKey(0)).round()\n(y_pred == test_y).mean()"
  },
  {
    "objectID": "tutorials/contribution.html",
    "href": "tutorials/contribution.html",
    "title": "Contribute",
    "section": "",
    "text": "This library uses nbdev for development. We love great flexibility offered by jupyter notebook, and nbdev in addressing limitations of using Notebook in developing large-scale projects (e.g., sync between notebooks and python modules, documentations).\nHere, we only cover basis of our development procedure. For an in-depth use of nbdev, please refer to the nbdev tutorial. Following links are particularly useful:",
    "crumbs": [
      "Overview",
      "Tutorials",
      "Contribute"
    ]
  },
  {
    "objectID": "tutorials/contribution.html#set-up-the-working-environment",
    "href": "tutorials/contribution.html#set-up-the-working-environment",
    "title": "Contribute",
    "section": "Set up the working environment",
    "text": "Set up the working environment\nRefer to installation guidance for installing ReLax. For running ReLax in CPU, you should\npip install \"jax-relax[dev]\"\nNext, install Quarto for the documentation system. See nbdev docs for more details.\nnbdev_install_quarto\nNext, install hooks for cleaning Jupyter Notebooks.\nnbdev_install_hooks",
    "crumbs": [
      "Overview",
      "Tutorials",
      "Contribute"
    ]
  },
  {
    "objectID": "tutorials/contribution.html#write-code-in-jupyter-notebook",
    "href": "tutorials/contribution.html#write-code-in-jupyter-notebook",
    "title": "Contribute",
    "section": "Write Code in Jupyter Notebook",
    "text": "Write Code in Jupyter Notebook\nNote that nbdev provides a best practice guidline to writing code in Jupyter Notebooks. Here, we present some of the most important steps.\n\nExport Cell to Python Module\n#| export marks code cells (in Notebook; .ipynb) to be exported to Python Module (.py). By default, this cell will be exported to the file defined in #| default_exp file_name (usually presented upfront).\nFor example, the below function will be exported to the Python module.\n#| export\ndef func(args):\n    ...\nWe can also specify files to be exported.\n#| export file_name.py\ndef func(args):\n    ...\nFor private functions/objects, we can use #| exporti. In this way, the code will still be exported to the file, but not included in __all__.\nMore about directives.\n\n\nTwo-way Sync between Notebooks (.ipynb) and Python Code (.py)\nTo update code written in Jupyter Notebook to Python Module (i.e., .ipynb -&gt; .py)\nnbdev_export\nTo sync code updated in Python Module back to Jupyter Notebook (i.e., .py -&gt; .ipynb)\nnbdev_update\n\n\n\n\n\n\nWarning\n\n\n\nIf you write a new function/object in .py, nbdev_update will not include this function in __all__. The best practice is to write functions/objects in Jupyter Notebook, and debug in Python Module (via IDE).\n\n\n\n\nCode Style\nReLax follows the black code style. See black’s code style document.",
    "crumbs": [
      "Overview",
      "Tutorials",
      "Contribute"
    ]
  },
  {
    "objectID": "tutorials/contribution.html#write-test-cases-in-jupyter-notebook",
    "href": "tutorials/contribution.html#write-test-cases-in-jupyter-notebook",
    "title": "Contribute",
    "section": "Write Test Cases in Jupyter Notebook",
    "text": "Write Test Cases in Jupyter Notebook\nIt is desirable to write some unit tests for each function and object. nbdev recommends to write test cases after implementing a feature. A normal cell is considered for testing.\nFor example, let’s consider a function which adds up all the inputs:\n\ndef add_numbers(*args):\n    return sum(args)\n\nTo test this function, we write unit tests via assert.\n\n# check correctness\nassert add_numbers(1, 2, 3) == 6\n# check types\nassert type(add_numbers(1, 2, 3)) == int\nassert type(add_numbers(1., 2, 3)) == float\n\n\n\n\n\n\n\nNote\n\n\n\nNote that all the test cases should be quickly run. If a cell takes a long time to run (e.g., model training), mark the cell as #| eval: false to skip this cell.",
    "crumbs": [
      "Overview",
      "Tutorials",
      "Contribute"
    ]
  },
  {
    "objectID": "tutorials/contribution.html#write-documentations-in-jupyter-notebook",
    "href": "tutorials/contribution.html#write-documentations-in-jupyter-notebook",
    "title": "Contribute",
    "section": "Write Documentations in Jupyter Notebook",
    "text": "Write Documentations in Jupyter Notebook\n\nDoc string\nTo write documentations in nbdev, it is recommended to\n\nuse simple type annotations\ndescribe each arguments with short comments\nprovide code examples and explanations in separate cells\n\n\n\n\n\n\n\nTip\n\n\n\nUnion typing is introduced after Python 3.10. For Python 3.7 - 3.9 users, you should\nfrom __future__ import annotations\n\n\n\ndef validate_configs(\n    configs: dict|BaseParser, # A configuration of the model/data.\n    config_cls: BaseParser # The desired configuration class.\n) -&gt; BaseParser:\n    \"\"\"return a valid configuration object.\"\"\"\n    ...\n\nnbdev will automatically render the documentation:\n\n\nvalidate_configs\n\n\nvalidate_configs (configs, config_cls)\n\nreturn a valid configuration object.\n\nParameters:\n\nconfigs (dict | BaseParser) – A configuration of the model/data.\nconfig_cls (BaseParser) – The desired configuration class.\n\n\n\nReturns:\n    (BaseParser)\n\n\nNext, we elaborate the use of this function with more descriptions and code examples.\n\nWe define a configuration object (which inherent BaseParser) to manage training/model/data configurations. validate_configs ensures to return the designated configuration object.\nFor example, we define a configuration object:\n\nclass LearningConfigs(BaseParser):\n    lr: float\n\nA configuration can be LearningConfigs, or the raw data in dictionary.\n\nconfigs = dict(lr=0.01)\n\nvalidate_configs will return a designated configuration object.\n\nvalidate_configs(configs, LearningConfigs)\n\nLearningConfigs(lr=0.01)\n\n\n\n\n\nCallout\nWe can also use callout for clear documentations.\n:::{.callout-note}\nNote that there are five types of callouts, including:\n`note`, `warning`, `important`, `tip`, and `caution`.\n:::\nwhich renders:\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.",
    "crumbs": [
      "Overview",
      "Tutorials",
      "Contribute"
    ]
  },
  {
    "objectID": "tutorials/contribution.html#preparing-a-code-commit",
    "href": "tutorials/contribution.html#preparing-a-code-commit",
    "title": "Contribute",
    "section": "Preparing a Code Commit",
    "text": "Preparing a Code Commit\nPreview the documentation system\nnbdev_preview\nIf everything is in your satisfaction, prepare code before commit to GitHub\nnbdev_prepare",
    "crumbs": [
      "Overview",
      "Tutorials",
      "Contribute"
    ]
  },
  {
    "objectID": "tutorials/contribution.html#summary",
    "href": "tutorials/contribution.html#summary",
    "title": "Contribute",
    "section": "Summary",
    "text": "Summary\n\nInstall all required packages based on installation guidance\nInstall the git hook nbdev_install_hooks\nWrite code in Jupyter Notebooks; add approprate directives, e.g., #| export\nWrite tests after the code in the Notebooks; test the code via nbdev_test\nWrite documents directly in the Notebooks; preview the docs nbdev_preview\nPrepare changes with nbdev_prepare\nCreate pull requests and push changes to GitHub",
    "crumbs": [
      "Overview",
      "Tutorials",
      "Contribute"
    ]
  },
  {
    "objectID": "evaluate.html",
    "href": "evaluate.html",
    "title": "Evaluate",
    "section": "",
    "text": "[source]\n\nclass relax.evaluate.BaseEvalMetrics (name=None)\n\nBase evaluation metrics class.\n\n[source]\n\nclass relax.evaluate.PredictiveAccuracy (name=‘accuracy’)\n\nCompute the accuracy of the predict function.\n\nacc = PredictiveAccuracy()\nexp = fake_explanation(3)\nacc(exp)\n\nArray(0.98300004, dtype=float32)\n\n\n\n[source]\n\nrelax.evaluate.compute_validity (xs, cfs, pred_fn)\n\n\nParameters:\n\nxs (&lt;class 'jax.Array'&gt;) – (n, d)\ncfs (&lt;class 'jax.Array'&gt;) – (n, d) or (n, b, d)\npred_fn (typing.Callable[[jax.Array], jax.Array])\n\n\n\nReturns:\n    (&lt;class 'float'&gt;)\n\n\n[source]\n\nrelax.evaluate.compute_single_validity (xs, cfs, pred_fn)\n\n\nParameters:\n\nxs (&lt;class 'jax.Array'&gt;) – (n, d)\ncfs (&lt;class 'jax.Array'&gt;) – (n, d)\npred_fn (typing.Callable[[jax.Array], jax.Array])\n\n\n\n[source]\n\nclass relax.evaluate.Validity (name=‘validity’)\n\nCompute fraction of input instances on which CF explanation methods output valid CF examples. Support binary case only.\n\nval = Validity()\nassert val(exp) == 0.\n\n\n[source]\n\nrelax.evaluate.compute_proximity (xs, cfs)\n\n\n[source]\n\nrelax.evaluate.compute_single_proximity (xs, cfs)\n\n\nassert jnp.isclose(\n    compute_proximity(xs, cfs, ), 0.\n)\nassert jnp.isclose(\n    compute_proximity(xs, cfs[:, 0, :], ), 0.\n)\n\n\n[source]\n\nclass relax.evaluate.Proximity (name=‘proximity’)\n\nCompute L1 norm distance between input datasets and CF examples divided by the number of features.\n\nprox = Proximity()\nassert prox(exp) == 0.\n\n\n[source]\n\nrelax.evaluate.compute_sparsity (xs, cfs, feature_indices)\n\n\n[source]\n\nrelax.evaluate.compute_single_sparsity (xs, cfs, feature_indices)\n\n\n[source]\n\nclass relax.evaluate.Sparsity (name=‘sparsity’)\n\nCompute the number of feature changes between input datasets and CF examples.\n\nspar = Sparsity()\nassert spar(exp) == 0.\n\n\n[source]\n\nclass relax.evaluate.ManifoldDist (n_neighbors=1, name=‘manifold_dist’)\n\nCompute the L1 distance to the n-nearest neighbor for all CF examples.\n\nman = ManifoldDist()\nman(exp)\n\nArray(6.905339e-07, dtype=float32)\n\n\n\n[source]\n\nclass relax.evaluate.Runtime (name=‘runtime’)\n\nCompute the runtime of the CF explanation method.\n\nrun = Runtime()\nrun(exp)\n\n0.0",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Evaluate"
    ]
  },
  {
    "objectID": "evaluate.html#metrics",
    "href": "evaluate.html#metrics",
    "title": "Evaluate",
    "section": "",
    "text": "[source]\n\nclass relax.evaluate.BaseEvalMetrics (name=None)\n\nBase evaluation metrics class.\n\n[source]\n\nclass relax.evaluate.PredictiveAccuracy (name=‘accuracy’)\n\nCompute the accuracy of the predict function.\n\nacc = PredictiveAccuracy()\nexp = fake_explanation(3)\nacc(exp)\n\nArray(0.98300004, dtype=float32)\n\n\n\n[source]\n\nrelax.evaluate.compute_validity (xs, cfs, pred_fn)\n\n\nParameters:\n\nxs (&lt;class 'jax.Array'&gt;) – (n, d)\ncfs (&lt;class 'jax.Array'&gt;) – (n, d) or (n, b, d)\npred_fn (typing.Callable[[jax.Array], jax.Array])\n\n\n\nReturns:\n    (&lt;class 'float'&gt;)\n\n\n[source]\n\nrelax.evaluate.compute_single_validity (xs, cfs, pred_fn)\n\n\nParameters:\n\nxs (&lt;class 'jax.Array'&gt;) – (n, d)\ncfs (&lt;class 'jax.Array'&gt;) – (n, d)\npred_fn (typing.Callable[[jax.Array], jax.Array])\n\n\n\n[source]\n\nclass relax.evaluate.Validity (name=‘validity’)\n\nCompute fraction of input instances on which CF explanation methods output valid CF examples. Support binary case only.\n\nval = Validity()\nassert val(exp) == 0.\n\n\n[source]\n\nrelax.evaluate.compute_proximity (xs, cfs)\n\n\n[source]\n\nrelax.evaluate.compute_single_proximity (xs, cfs)\n\n\nassert jnp.isclose(\n    compute_proximity(xs, cfs, ), 0.\n)\nassert jnp.isclose(\n    compute_proximity(xs, cfs[:, 0, :], ), 0.\n)\n\n\n[source]\n\nclass relax.evaluate.Proximity (name=‘proximity’)\n\nCompute L1 norm distance between input datasets and CF examples divided by the number of features.\n\nprox = Proximity()\nassert prox(exp) == 0.\n\n\n[source]\n\nrelax.evaluate.compute_sparsity (xs, cfs, feature_indices)\n\n\n[source]\n\nrelax.evaluate.compute_single_sparsity (xs, cfs, feature_indices)\n\n\n[source]\n\nclass relax.evaluate.Sparsity (name=‘sparsity’)\n\nCompute the number of feature changes between input datasets and CF examples.\n\nspar = Sparsity()\nassert spar(exp) == 0.\n\n\n[source]\n\nclass relax.evaluate.ManifoldDist (n_neighbors=1, name=‘manifold_dist’)\n\nCompute the L1 distance to the n-nearest neighbor for all CF examples.\n\nman = ManifoldDist()\nman(exp)\n\nArray(6.905339e-07, dtype=float32)\n\n\n\n[source]\n\nclass relax.evaluate.Runtime (name=‘runtime’)\n\nCompute the runtime of the CF explanation method.\n\nrun = Runtime()\nrun(exp)\n\n0.0",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Evaluate"
    ]
  },
  {
    "objectID": "evaluate.html#benchmarking",
    "href": "evaluate.html#benchmarking",
    "title": "Evaluate",
    "section": "Benchmarking",
    "text": "Benchmarking\n\nrelax.evaluate.evaluate_cfs\n\n[source]\n\nrelax.evaluate.evaluate_cfs (cf_exp, metrics=None, return_dict=True, return_df=False)\n\n\nParameters:\n\ncf_exp (&lt;class 'relax.explain.Explanation'&gt;) – CF Explanations\nmetrics (typing.Iterable[typing.Union[str, __main__.BaseEvalMetrics]], default=None) – A list of Metrics. Can be str or a subclass of BaseEvalMetrics\nreturn_dict (&lt;class 'bool'&gt;, default=True) – return a dictionary or not (default: True)\nreturn_df (&lt;class 'bool'&gt;, default=False) – return a pandas Dataframe or not (default: False)\n\n\n\nrelax.evaluate.benchmark_cfs\n\n[source]\n\nrelax.evaluate.benchmark_cfs (cf_results_list, metrics=None)",
    "crumbs": [
      "Overview",
      "API Documentations",
      "Evaluate"
    ]
  }
]